{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(6)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../prepare_data.py\n",
    "%run ../architectures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, valid_dataset = mura_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 250, 200]), torch.Size([32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNet(width_mult=1.0, depth_mult=1.0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225153"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 5.37 s, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lrs, losses = LR_range_finder(model, train_loader, \n",
    "                              loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                              binary=True, lr_high=0.05)\n",
    "plot_lr(lrs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [1.0, 0.75, 0.5, 0.25]\n",
    "depths = [1.0, 0.7, 0.6, 0.5, 0.3, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width multiplier - 1.000 depth multiplier - 1.000\n",
      "train_loss 0.677 val_loss 0.699 val_auc_score 0.589\n",
      "----End of step 0:01:30.664166\n",
      "train_loss 0.663 val_loss 0.769 val_auc_score 0.583\n",
      "----End of step 0:01:36.549204\n",
      "train_loss 0.667 val_loss 0.690 val_auc_score 0.603\n",
      "----End of step 0:01:29.527985\n",
      "train_loss 0.666 val_loss 0.696 val_auc_score 0.601\n",
      "----End of step 0:01:27.533567\n",
      "train_loss 0.657 val_loss 0.694 val_auc_score 0.593\n",
      "----End of step 0:01:27.671294\n",
      "train_loss 0.648 val_loss 0.680 val_auc_score 0.667\n",
      "----End of step 0:01:31.697636\n",
      "train_loss 0.631 val_loss 0.829 val_auc_score 0.703\n",
      "----End of step 0:01:33.549522\n",
      "train_loss 0.605 val_loss 0.634 val_auc_score 0.755\n",
      "----End of step 0:01:31.047360\n",
      "train_loss 0.581 val_loss 0.597 val_auc_score 0.791\n",
      "----End of step 0:01:31.144480\n",
      "train_loss 0.564 val_loss 0.565 val_auc_score 0.812\n",
      "----End of step 0:01:28.976562\n",
      "train_loss 0.549 val_loss 0.567 val_auc_score 0.811\n",
      "----End of step 0:01:28.466531\n",
      "train_loss 0.536 val_loss 0.571 val_auc_score 0.821\n",
      "----End of step 0:01:27.685841\n",
      "train_loss 0.525 val_loss 0.536 val_auc_score 0.832\n",
      "----End of step 0:01:28.443851\n",
      "train_loss 0.518 val_loss 0.534 val_auc_score 0.835\n",
      "----End of step 0:01:29.273181\n",
      "train_loss 0.516 val_loss 0.538 val_auc_score 0.836\n",
      "----End of step 0:01:28.468944\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.700\n",
      "train_loss 0.673 val_loss 0.730 val_auc_score 0.592\n",
      "----End of step 0:01:24.466885\n",
      "train_loss 0.661 val_loss 0.676 val_auc_score 0.658\n",
      "----End of step 0:01:23.326857\n",
      "train_loss 0.644 val_loss 0.866 val_auc_score 0.653\n",
      "----End of step 0:01:26.381354\n",
      "train_loss 0.614 val_loss 0.647 val_auc_score 0.716\n",
      "----End of step 0:01:23.913783\n",
      "train_loss 0.595 val_loss 0.653 val_auc_score 0.736\n",
      "----End of step 0:01:25.663815\n",
      "train_loss 0.580 val_loss 0.592 val_auc_score 0.766\n",
      "----End of step 0:01:25.046689\n",
      "train_loss 0.568 val_loss 0.587 val_auc_score 0.781\n",
      "----End of step 0:01:27.265734\n",
      "train_loss 0.556 val_loss 0.563 val_auc_score 0.797\n",
      "----End of step 0:01:27.967488\n",
      "train_loss 0.546 val_loss 0.568 val_auc_score 0.802\n",
      "----End of step 0:01:26.109792\n",
      "train_loss 0.535 val_loss 0.541 val_auc_score 0.816\n",
      "----End of step 0:01:26.601536\n",
      "train_loss 0.522 val_loss 0.544 val_auc_score 0.822\n",
      "----End of step 0:01:26.299291\n",
      "train_loss 0.513 val_loss 0.529 val_auc_score 0.824\n",
      "----End of step 0:01:29.107881\n",
      "train_loss 0.503 val_loss 0.532 val_auc_score 0.827\n",
      "----End of step 0:01:28.572602\n",
      "train_loss 0.495 val_loss 0.516 val_auc_score 0.835\n",
      "----End of step 0:01:27.695687\n",
      "train_loss 0.492 val_loss 0.520 val_auc_score 0.834\n",
      "----End of step 0:01:28.917706\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.600\n",
      "train_loss 0.675 val_loss 0.722 val_auc_score 0.601\n",
      "----End of step 0:01:16.872774\n",
      "train_loss 0.657 val_loss 0.757 val_auc_score 0.504\n",
      "----End of step 0:01:16.532571\n",
      "train_loss 0.662 val_loss 0.692 val_auc_score 0.551\n",
      "----End of step 0:01:16.492262\n",
      "train_loss 0.651 val_loss 0.632 val_auc_score 0.713\n",
      "----End of step 0:01:16.680322\n",
      "train_loss 0.624 val_loss 0.675 val_auc_score 0.705\n",
      "----End of step 0:01:15.408870\n",
      "train_loss 0.596 val_loss 0.659 val_auc_score 0.728\n",
      "----End of step 0:01:15.781712\n",
      "train_loss 0.577 val_loss 0.590 val_auc_score 0.761\n",
      "----End of step 0:01:16.125657\n",
      "train_loss 0.562 val_loss 0.583 val_auc_score 0.776\n",
      "----End of step 0:01:16.087361\n",
      "train_loss 0.551 val_loss 0.553 val_auc_score 0.810\n",
      "----End of step 0:01:16.155810\n",
      "train_loss 0.538 val_loss 0.591 val_auc_score 0.807\n",
      "----End of step 0:01:15.573385\n",
      "train_loss 0.526 val_loss 0.559 val_auc_score 0.821\n",
      "----End of step 0:01:10.796786\n",
      "train_loss 0.513 val_loss 0.545 val_auc_score 0.821\n",
      "----End of step 0:01:12.697369\n",
      "train_loss 0.503 val_loss 0.528 val_auc_score 0.829\n",
      "----End of step 0:01:13.356731\n",
      "train_loss 0.496 val_loss 0.520 val_auc_score 0.832\n",
      "----End of step 0:01:14.522723\n",
      "train_loss 0.494 val_loss 0.528 val_auc_score 0.836\n",
      "----End of step 0:01:12.341598\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.500\n",
      "train_loss 0.674 val_loss 0.763 val_auc_score 0.626\n",
      "----End of step 0:01:05.881993\n",
      "train_loss 0.661 val_loss 0.723 val_auc_score 0.604\n",
      "----End of step 0:01:07.055451\n",
      "train_loss 0.665 val_loss 0.698 val_auc_score 0.604\n",
      "----End of step 0:01:03.671172\n",
      "train_loss 0.658 val_loss 0.701 val_auc_score 0.665\n",
      "----End of step 0:01:03.586155\n",
      "train_loss 0.641 val_loss 0.678 val_auc_score 0.677\n",
      "----End of step 0:01:03.863030\n",
      "train_loss 0.618 val_loss 0.650 val_auc_score 0.742\n",
      "----End of step 0:01:05.195200\n",
      "train_loss 0.593 val_loss 0.614 val_auc_score 0.748\n",
      "----End of step 0:01:04.164296\n",
      "train_loss 0.573 val_loss 0.620 val_auc_score 0.795\n",
      "----End of step 0:01:04.339803\n",
      "train_loss 0.560 val_loss 0.609 val_auc_score 0.784\n",
      "----End of step 0:01:09.379635\n",
      "train_loss 0.548 val_loss 0.586 val_auc_score 0.800\n",
      "----End of step 0:01:08.266696\n",
      "train_loss 0.537 val_loss 0.578 val_auc_score 0.821\n",
      "----End of step 0:01:03.704244\n",
      "train_loss 0.528 val_loss 0.548 val_auc_score 0.824\n",
      "----End of step 0:01:05.655239\n",
      "train_loss 0.518 val_loss 0.549 val_auc_score 0.826\n",
      "----End of step 0:01:04.619711\n",
      "train_loss 0.511 val_loss 0.534 val_auc_score 0.831\n",
      "----End of step 0:01:05.016337\n",
      "train_loss 0.506 val_loss 0.532 val_auc_score 0.832\n",
      "----End of step 0:01:04.151405\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.300\n",
      "train_loss 0.672 val_loss 0.747 val_auc_score 0.516\n",
      "----End of step 0:00:49.178265\n",
      "train_loss 0.652 val_loss 0.699 val_auc_score 0.662\n",
      "----End of step 0:00:50.611324\n",
      "train_loss 0.634 val_loss 0.647 val_auc_score 0.704\n",
      "----End of step 0:00:51.644486\n",
      "train_loss 0.616 val_loss 0.625 val_auc_score 0.721\n",
      "----End of step 0:00:52.886757\n",
      "train_loss 0.597 val_loss 0.616 val_auc_score 0.742\n",
      "----End of step 0:00:51.846356\n",
      "train_loss 0.590 val_loss 0.617 val_auc_score 0.758\n",
      "----End of step 0:00:53.191506\n",
      "train_loss 0.576 val_loss 0.593 val_auc_score 0.770\n",
      "----End of step 0:00:54.932069\n",
      "train_loss 0.563 val_loss 0.604 val_auc_score 0.792\n",
      "----End of step 0:00:53.156491\n",
      "train_loss 0.553 val_loss 0.583 val_auc_score 0.798\n",
      "----End of step 0:00:52.108344\n",
      "train_loss 0.544 val_loss 0.565 val_auc_score 0.809\n",
      "----End of step 0:00:51.363432\n",
      "train_loss 0.533 val_loss 0.575 val_auc_score 0.817\n",
      "----End of step 0:00:51.826687\n",
      "train_loss 0.525 val_loss 0.544 val_auc_score 0.826\n",
      "----End of step 0:00:51.872471\n",
      "train_loss 0.516 val_loss 0.548 val_auc_score 0.826\n",
      "----End of step 0:00:52.288564\n",
      "train_loss 0.509 val_loss 0.533 val_auc_score 0.831\n",
      "----End of step 0:00:51.911140\n",
      "train_loss 0.507 val_loss 0.533 val_auc_score 0.832\n",
      "----End of step 0:00:51.670429\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.200\n",
      "train_loss 0.675 val_loss 0.687 val_auc_score 0.640\n",
      "----End of step 0:00:47.587377\n",
      "train_loss 0.658 val_loss 0.664 val_auc_score 0.682\n",
      "----End of step 0:00:47.884454\n",
      "train_loss 0.637 val_loss 0.734 val_auc_score 0.715\n",
      "----End of step 0:00:47.385316\n",
      "train_loss 0.611 val_loss 0.628 val_auc_score 0.709\n",
      "----End of step 0:00:48.128501\n",
      "train_loss 0.595 val_loss 0.655 val_auc_score 0.733\n",
      "----End of step 0:00:49.726887\n",
      "train_loss 0.585 val_loss 0.625 val_auc_score 0.740\n",
      "----End of step 0:00:48.634172\n",
      "train_loss 0.575 val_loss 0.597 val_auc_score 0.769\n",
      "----End of step 0:00:47.734061\n",
      "train_loss 0.565 val_loss 0.594 val_auc_score 0.785\n",
      "----End of step 0:00:49.312322\n",
      "train_loss 0.556 val_loss 0.612 val_auc_score 0.799\n",
      "----End of step 0:00:50.787388\n",
      "train_loss 0.545 val_loss 0.563 val_auc_score 0.807\n",
      "----End of step 0:00:49.227485\n",
      "train_loss 0.538 val_loss 0.547 val_auc_score 0.816\n",
      "----End of step 0:00:47.784215\n",
      "train_loss 0.527 val_loss 0.554 val_auc_score 0.817\n",
      "----End of step 0:00:51.065721\n",
      "train_loss 0.521 val_loss 0.544 val_auc_score 0.824\n",
      "----End of step 0:00:49.843936\n",
      "train_loss 0.516 val_loss 0.532 val_auc_score 0.832\n",
      "----End of step 0:00:47.746294\n",
      "train_loss 0.512 val_loss 0.532 val_auc_score 0.830\n",
      "----End of step 0:00:47.104223\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 1.000\n",
      "train_loss 0.675 val_loss 0.678 val_auc_score 0.644\n",
      "----End of step 0:01:30.951468\n",
      "train_loss 0.667 val_loss 0.694 val_auc_score 0.583\n",
      "----End of step 0:01:33.349445\n",
      "train_loss 0.667 val_loss 0.914 val_auc_score 0.581\n",
      "----End of step 0:01:31.879790\n",
      "train_loss 0.662 val_loss 0.700 val_auc_score 0.623\n",
      "----End of step 0:01:31.163166\n",
      "train_loss 0.652 val_loss 0.684 val_auc_score 0.618\n",
      "----End of step 0:01:30.782199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.637 val_loss 0.668 val_auc_score 0.702\n",
      "----End of step 0:01:31.222018\n",
      "train_loss 0.611 val_loss 0.625 val_auc_score 0.747\n",
      "----End of step 0:01:31.525589\n",
      "train_loss 0.590 val_loss 0.607 val_auc_score 0.758\n",
      "----End of step 0:01:32.528014\n",
      "train_loss 0.575 val_loss 0.589 val_auc_score 0.775\n",
      "----End of step 0:01:32.330189\n",
      "train_loss 0.559 val_loss 0.568 val_auc_score 0.788\n",
      "----End of step 0:01:31.908835\n",
      "train_loss 0.546 val_loss 0.570 val_auc_score 0.811\n",
      "----End of step 0:01:31.668413\n",
      "train_loss 0.533 val_loss 0.553 val_auc_score 0.819\n",
      "----End of step 0:01:30.938144\n",
      "train_loss 0.526 val_loss 0.542 val_auc_score 0.824\n",
      "----End of step 0:01:32.150734\n",
      "train_loss 0.519 val_loss 0.537 val_auc_score 0.829\n",
      "----End of step 0:01:31.342265\n",
      "train_loss 0.512 val_loss 0.535 val_auc_score 0.826\n",
      "----End of step 0:01:30.905742\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.700\n",
      "train_loss 0.677 val_loss 0.688 val_auc_score 0.615\n",
      "----End of step 0:01:27.478927\n",
      "train_loss 0.664 val_loss 0.691 val_auc_score 0.568\n",
      "----End of step 0:01:26.870857\n",
      "train_loss 0.660 val_loss 0.687 val_auc_score 0.663\n",
      "----End of step 0:01:25.815047\n",
      "train_loss 0.632 val_loss 0.651 val_auc_score 0.713\n",
      "----End of step 0:01:26.195620\n",
      "train_loss 0.615 val_loss 0.640 val_auc_score 0.684\n",
      "----End of step 0:01:26.436588\n",
      "train_loss 0.598 val_loss 0.616 val_auc_score 0.741\n",
      "----End of step 0:01:25.833352\n",
      "train_loss 0.587 val_loss 0.617 val_auc_score 0.743\n",
      "----End of step 0:01:25.497771\n",
      "train_loss 0.574 val_loss 0.621 val_auc_score 0.759\n",
      "----End of step 0:01:26.007311\n",
      "train_loss 0.562 val_loss 0.580 val_auc_score 0.791\n",
      "----End of step 0:01:26.139998\n",
      "train_loss 0.550 val_loss 0.566 val_auc_score 0.805\n",
      "----End of step 0:01:25.893343\n",
      "train_loss 0.536 val_loss 0.568 val_auc_score 0.813\n",
      "----End of step 0:01:30.137250\n",
      "train_loss 0.526 val_loss 0.561 val_auc_score 0.817\n",
      "----End of step 0:01:29.002304\n",
      "train_loss 0.518 val_loss 0.541 val_auc_score 0.829\n",
      "----End of step 0:01:24.959553\n",
      "train_loss 0.512 val_loss 0.535 val_auc_score 0.831\n",
      "----End of step 0:01:27.967019\n",
      "train_loss 0.505 val_loss 0.533 val_auc_score 0.832\n",
      "----End of step 0:01:26.262974\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.600\n",
      "train_loss 0.677 val_loss 0.842 val_auc_score 0.575\n",
      "----End of step 0:01:13.685223\n",
      "train_loss 0.664 val_loss 0.704 val_auc_score 0.602\n",
      "----End of step 0:01:14.881956\n",
      "train_loss 0.663 val_loss 0.725 val_auc_score 0.590\n",
      "----End of step 0:01:12.486892\n",
      "train_loss 0.660 val_loss 0.670 val_auc_score 0.639\n",
      "----End of step 0:01:13.012892\n",
      "train_loss 0.652 val_loss 0.681 val_auc_score 0.646\n",
      "----End of step 0:01:12.923901\n",
      "train_loss 0.629 val_loss 0.637 val_auc_score 0.699\n",
      "----End of step 0:01:15.350194\n",
      "train_loss 0.599 val_loss 0.613 val_auc_score 0.754\n",
      "----End of step 0:01:14.961616\n",
      "train_loss 0.581 val_loss 0.608 val_auc_score 0.758\n",
      "----End of step 0:01:15.773200\n",
      "train_loss 0.567 val_loss 0.588 val_auc_score 0.767\n",
      "----End of step 0:01:15.083897\n",
      "train_loss 0.555 val_loss 0.604 val_auc_score 0.778\n",
      "----End of step 0:01:13.861943\n",
      "train_loss 0.543 val_loss 0.563 val_auc_score 0.810\n",
      "----End of step 0:01:13.139077\n",
      "train_loss 0.532 val_loss 0.570 val_auc_score 0.815\n",
      "----End of step 0:01:15.901588\n",
      "train_loss 0.525 val_loss 0.553 val_auc_score 0.823\n",
      "----End of step 0:01:15.159106\n",
      "train_loss 0.515 val_loss 0.541 val_auc_score 0.827\n",
      "----End of step 0:01:16.215777\n",
      "train_loss 0.515 val_loss 0.536 val_auc_score 0.831\n",
      "----End of step 0:01:13.247604\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.500\n",
      "train_loss 0.677 val_loss 0.736 val_auc_score 0.626\n",
      "----End of step 0:01:05.800001\n",
      "train_loss 0.665 val_loss 0.708 val_auc_score 0.606\n",
      "----End of step 0:01:05.509829\n",
      "train_loss 0.660 val_loss 0.645 val_auc_score 0.694\n",
      "----End of step 0:01:05.645482\n",
      "train_loss 0.645 val_loss 0.658 val_auc_score 0.695\n",
      "----End of step 0:01:04.197930\n",
      "train_loss 0.620 val_loss 0.622 val_auc_score 0.731\n",
      "----End of step 0:01:04.148045\n",
      "train_loss 0.600 val_loss 0.609 val_auc_score 0.754\n",
      "----End of step 0:01:04.462953\n",
      "train_loss 0.583 val_loss 0.643 val_auc_score 0.763\n",
      "----End of step 0:01:07.734502\n",
      "train_loss 0.571 val_loss 0.637 val_auc_score 0.788\n",
      "----End of step 0:01:05.607617\n",
      "train_loss 0.558 val_loss 0.583 val_auc_score 0.795\n",
      "----End of step 0:01:05.374995\n",
      "train_loss 0.546 val_loss 0.573 val_auc_score 0.817\n",
      "----End of step 0:01:04.322601\n",
      "train_loss 0.535 val_loss 0.573 val_auc_score 0.820\n",
      "----End of step 0:01:06.179652\n",
      "train_loss 0.527 val_loss 0.549 val_auc_score 0.822\n",
      "----End of step 0:01:04.691461\n",
      "train_loss 0.517 val_loss 0.542 val_auc_score 0.822\n",
      "----End of step 0:01:05.703001\n",
      "train_loss 0.512 val_loss 0.540 val_auc_score 0.825\n",
      "----End of step 0:01:05.717834\n",
      "train_loss 0.509 val_loss 0.540 val_auc_score 0.823\n",
      "----End of step 0:01:06.711079\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.300\n",
      "train_loss 0.678 val_loss 0.744 val_auc_score 0.612\n",
      "----End of step 0:00:53.097190\n",
      "train_loss 0.659 val_loss 0.808 val_auc_score 0.612\n",
      "----End of step 0:00:52.762351\n",
      "train_loss 0.652 val_loss 0.733 val_auc_score 0.678\n",
      "----End of step 0:00:54.077477\n",
      "train_loss 0.626 val_loss 0.703 val_auc_score 0.689\n",
      "----End of step 0:00:53.580537\n",
      "train_loss 0.596 val_loss 0.626 val_auc_score 0.767\n",
      "----End of step 0:00:52.973325\n",
      "train_loss 0.584 val_loss 0.595 val_auc_score 0.758\n",
      "----End of step 0:00:51.820377\n",
      "train_loss 0.568 val_loss 0.576 val_auc_score 0.783\n",
      "----End of step 0:00:52.648464\n",
      "train_loss 0.557 val_loss 0.639 val_auc_score 0.784\n",
      "----End of step 0:00:52.273767\n",
      "train_loss 0.547 val_loss 0.550 val_auc_score 0.816\n",
      "----End of step 0:00:52.007456\n",
      "train_loss 0.539 val_loss 0.539 val_auc_score 0.815\n",
      "----End of step 0:00:50.901277\n",
      "train_loss 0.530 val_loss 0.536 val_auc_score 0.819\n",
      "----End of step 0:00:50.917419\n",
      "train_loss 0.519 val_loss 0.546 val_auc_score 0.826\n",
      "----End of step 0:00:52.609674\n",
      "train_loss 0.512 val_loss 0.536 val_auc_score 0.832\n",
      "----End of step 0:00:52.484155\n",
      "train_loss 0.505 val_loss 0.530 val_auc_score 0.835\n",
      "----End of step 0:00:50.531643\n",
      "train_loss 0.508 val_loss 0.529 val_auc_score 0.834\n",
      "----End of step 0:00:50.714128\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.200\n",
      "train_loss 0.681 val_loss 0.696 val_auc_score 0.653\n",
      "----End of step 0:00:48.332692\n",
      "train_loss 0.653 val_loss 0.678 val_auc_score 0.687\n",
      "----End of step 0:00:47.517733\n",
      "train_loss 0.637 val_loss 0.702 val_auc_score 0.676\n",
      "----End of step 0:00:47.787863\n",
      "train_loss 0.616 val_loss 0.688 val_auc_score 0.687\n",
      "----End of step 0:00:49.975925\n",
      "train_loss 0.600 val_loss 0.654 val_auc_score 0.749\n",
      "----End of step 0:00:46.399916\n",
      "train_loss 0.594 val_loss 0.623 val_auc_score 0.777\n",
      "----End of step 0:00:50.708101\n",
      "train_loss 0.582 val_loss 0.615 val_auc_score 0.773\n",
      "----End of step 0:00:47.056503\n",
      "train_loss 0.571 val_loss 0.602 val_auc_score 0.786\n",
      "----End of step 0:00:47.019149\n",
      "train_loss 0.557 val_loss 0.593 val_auc_score 0.778\n",
      "----End of step 0:00:47.977881\n",
      "train_loss 0.546 val_loss 0.586 val_auc_score 0.801\n",
      "----End of step 0:00:48.085784\n",
      "train_loss 0.539 val_loss 0.552 val_auc_score 0.813\n",
      "----End of step 0:00:47.377729\n",
      "train_loss 0.533 val_loss 0.554 val_auc_score 0.820\n",
      "----End of step 0:00:47.600164\n",
      "train_loss 0.523 val_loss 0.550 val_auc_score 0.820\n",
      "----End of step 0:00:46.504060\n",
      "train_loss 0.517 val_loss 0.550 val_auc_score 0.823\n",
      "----End of step 0:00:48.292626\n",
      "train_loss 0.517 val_loss 0.538 val_auc_score 0.825\n",
      "----End of step 0:00:47.167354\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 1.000\n",
      "train_loss 0.683 val_loss 0.711 val_auc_score 0.556\n",
      "----End of step 0:01:27.938055\n",
      "train_loss 0.667 val_loss 0.743 val_auc_score 0.586\n",
      "----End of step 0:01:27.577494\n",
      "train_loss 0.656 val_loss 0.717 val_auc_score 0.634\n",
      "----End of step 0:01:28.518319\n",
      "train_loss 0.624 val_loss 0.655 val_auc_score 0.716\n",
      "----End of step 0:01:30.491396\n",
      "train_loss 0.600 val_loss 0.605 val_auc_score 0.750\n",
      "----End of step 0:01:31.616972\n",
      "train_loss 0.588 val_loss 0.614 val_auc_score 0.753\n",
      "----End of step 0:01:32.159490\n",
      "train_loss 0.576 val_loss 0.649 val_auc_score 0.743\n",
      "----End of step 0:01:31.947635\n",
      "train_loss 0.567 val_loss 0.586 val_auc_score 0.769\n",
      "----End of step 0:01:30.716424\n",
      "train_loss 0.553 val_loss 0.591 val_auc_score 0.786\n",
      "----End of step 0:01:28.482467\n",
      "train_loss 0.543 val_loss 0.566 val_auc_score 0.809\n",
      "----End of step 0:01:32.076498\n",
      "train_loss 0.532 val_loss 0.548 val_auc_score 0.818\n",
      "----End of step 0:01:30.139217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.523 val_loss 0.545 val_auc_score 0.831\n",
      "----End of step 0:01:30.823470\n",
      "train_loss 0.513 val_loss 0.542 val_auc_score 0.832\n",
      "----End of step 0:01:29.840887\n",
      "train_loss 0.508 val_loss 0.527 val_auc_score 0.835\n",
      "----End of step 0:01:33.499153\n",
      "train_loss 0.504 val_loss 0.527 val_auc_score 0.835\n",
      "----End of step 0:01:33.001289\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.700\n",
      "train_loss 0.682 val_loss 0.800 val_auc_score 0.561\n",
      "----End of step 0:01:27.556160\n",
      "train_loss 0.663 val_loss 0.700 val_auc_score 0.648\n",
      "----End of step 0:01:28.773070\n",
      "train_loss 0.653 val_loss 0.680 val_auc_score 0.663\n",
      "----End of step 0:01:29.591199\n",
      "train_loss 0.633 val_loss 0.676 val_auc_score 0.690\n",
      "----End of step 0:01:28.715999\n",
      "train_loss 0.609 val_loss 0.614 val_auc_score 0.746\n",
      "----End of step 0:01:28.424957\n",
      "train_loss 0.588 val_loss 0.619 val_auc_score 0.744\n",
      "----End of step 0:01:29.086712\n",
      "train_loss 0.576 val_loss 0.600 val_auc_score 0.785\n",
      "----End of step 0:01:26.910277\n",
      "train_loss 0.566 val_loss 0.603 val_auc_score 0.799\n",
      "----End of step 0:01:26.698211\n",
      "train_loss 0.552 val_loss 0.606 val_auc_score 0.787\n",
      "----End of step 0:01:27.173132\n",
      "train_loss 0.537 val_loss 0.567 val_auc_score 0.810\n",
      "----End of step 0:01:28.629010\n",
      "train_loss 0.526 val_loss 0.550 val_auc_score 0.812\n",
      "----End of step 0:01:26.552998\n",
      "train_loss 0.517 val_loss 0.524 val_auc_score 0.832\n",
      "----End of step 0:01:25.326348\n",
      "train_loss 0.508 val_loss 0.526 val_auc_score 0.829\n",
      "----End of step 0:01:26.113832\n",
      "train_loss 0.503 val_loss 0.533 val_auc_score 0.835\n",
      "----End of step 0:01:24.876030\n",
      "train_loss 0.498 val_loss 0.526 val_auc_score 0.837\n",
      "----End of step 0:01:23.937490\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.600\n",
      "train_loss 0.684 val_loss 0.693 val_auc_score 0.602\n",
      "----End of step 0:01:11.515004\n",
      "train_loss 0.663 val_loss 0.703 val_auc_score 0.622\n",
      "----End of step 0:01:12.697961\n",
      "train_loss 0.660 val_loss 0.673 val_auc_score 0.636\n",
      "----End of step 0:01:12.378777\n",
      "train_loss 0.638 val_loss 0.667 val_auc_score 0.671\n",
      "----End of step 0:01:13.234278\n",
      "train_loss 0.611 val_loss 0.775 val_auc_score 0.700\n",
      "----End of step 0:01:12.477836\n",
      "train_loss 0.591 val_loss 0.619 val_auc_score 0.758\n",
      "----End of step 0:01:13.773142\n",
      "train_loss 0.578 val_loss 0.600 val_auc_score 0.771\n",
      "----End of step 0:01:13.759200\n",
      "train_loss 0.567 val_loss 0.595 val_auc_score 0.781\n",
      "----End of step 0:01:16.271064\n",
      "train_loss 0.559 val_loss 0.587 val_auc_score 0.778\n",
      "----End of step 0:01:14.237735\n",
      "train_loss 0.550 val_loss 0.598 val_auc_score 0.792\n",
      "----End of step 0:01:10.941657\n",
      "train_loss 0.541 val_loss 0.552 val_auc_score 0.809\n",
      "----End of step 0:01:11.606014\n",
      "train_loss 0.530 val_loss 0.552 val_auc_score 0.815\n",
      "----End of step 0:01:15.007120\n",
      "train_loss 0.523 val_loss 0.546 val_auc_score 0.818\n",
      "----End of step 0:01:12.718866\n",
      "train_loss 0.518 val_loss 0.535 val_auc_score 0.824\n",
      "----End of step 0:01:13.467582\n",
      "train_loss 0.514 val_loss 0.536 val_auc_score 0.824\n",
      "----End of step 0:01:12.360465\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.500\n",
      "train_loss 0.680 val_loss 0.739 val_auc_score 0.590\n",
      "----End of step 0:01:08.271815\n",
      "train_loss 0.664 val_loss 0.674 val_auc_score 0.649\n",
      "----End of step 0:01:05.547355\n",
      "train_loss 0.659 val_loss 0.758 val_auc_score 0.630\n",
      "----End of step 0:01:04.650995\n",
      "train_loss 0.631 val_loss 0.637 val_auc_score 0.714\n",
      "----End of step 0:01:04.460810\n",
      "train_loss 0.612 val_loss 0.658 val_auc_score 0.714\n",
      "----End of step 0:01:05.186234\n",
      "train_loss 0.598 val_loss 0.642 val_auc_score 0.747\n",
      "----End of step 0:01:05.243631\n",
      "train_loss 0.584 val_loss 0.631 val_auc_score 0.735\n",
      "----End of step 0:01:04.170919\n",
      "train_loss 0.574 val_loss 0.610 val_auc_score 0.773\n",
      "----End of step 0:01:05.510107\n",
      "train_loss 0.561 val_loss 0.594 val_auc_score 0.787\n",
      "----End of step 0:01:04.293248\n",
      "train_loss 0.551 val_loss 0.606 val_auc_score 0.795\n",
      "----End of step 0:01:04.708359\n",
      "train_loss 0.539 val_loss 0.557 val_auc_score 0.807\n",
      "----End of step 0:01:05.074374\n",
      "train_loss 0.534 val_loss 0.548 val_auc_score 0.816\n",
      "----End of step 0:01:03.862028\n",
      "train_loss 0.527 val_loss 0.545 val_auc_score 0.825\n",
      "----End of step 0:01:05.063368\n",
      "train_loss 0.518 val_loss 0.547 val_auc_score 0.823\n",
      "----End of step 0:01:04.238334\n",
      "train_loss 0.516 val_loss 0.543 val_auc_score 0.826\n",
      "----End of step 0:01:05.902394\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.300\n",
      "train_loss 0.681 val_loss 0.681 val_auc_score 0.643\n",
      "----End of step 0:00:52.985818\n",
      "train_loss 0.661 val_loss 0.685 val_auc_score 0.675\n",
      "----End of step 0:00:51.845316\n",
      "train_loss 0.647 val_loss 0.661 val_auc_score 0.679\n",
      "----End of step 0:00:52.756746\n",
      "train_loss 0.629 val_loss 0.624 val_auc_score 0.725\n",
      "----End of step 0:00:52.740320\n",
      "train_loss 0.610 val_loss 0.640 val_auc_score 0.686\n",
      "----End of step 0:00:52.887174\n",
      "train_loss 0.594 val_loss 0.665 val_auc_score 0.719\n",
      "----End of step 0:00:54.186862\n",
      "train_loss 0.582 val_loss 0.605 val_auc_score 0.745\n",
      "----End of step 0:00:51.571621\n",
      "train_loss 0.574 val_loss 0.582 val_auc_score 0.778\n",
      "----End of step 0:00:52.780898\n",
      "train_loss 0.564 val_loss 0.579 val_auc_score 0.778\n",
      "----End of step 0:00:52.117013\n",
      "train_loss 0.556 val_loss 0.565 val_auc_score 0.793\n",
      "----End of step 0:00:53.583429\n",
      "train_loss 0.547 val_loss 0.569 val_auc_score 0.793\n",
      "----End of step 0:00:52.239770\n",
      "train_loss 0.538 val_loss 0.559 val_auc_score 0.800\n",
      "----End of step 0:00:53.456408\n",
      "train_loss 0.532 val_loss 0.554 val_auc_score 0.805\n",
      "----End of step 0:00:52.948644\n",
      "train_loss 0.525 val_loss 0.563 val_auc_score 0.806\n",
      "----End of step 0:00:52.879780\n",
      "train_loss 0.524 val_loss 0.555 val_auc_score 0.807\n",
      "----End of step 0:00:52.433599\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.200\n",
      "train_loss 0.683 val_loss 0.687 val_auc_score 0.624\n",
      "----End of step 0:00:50.151644\n",
      "train_loss 0.658 val_loss 0.665 val_auc_score 0.666\n",
      "----End of step 0:00:49.302792\n",
      "train_loss 0.650 val_loss 0.788 val_auc_score 0.640\n",
      "----End of step 0:00:50.170640\n",
      "train_loss 0.624 val_loss 0.643 val_auc_score 0.691\n",
      "----End of step 0:00:49.887550\n",
      "train_loss 0.605 val_loss 0.658 val_auc_score 0.678\n",
      "----End of step 0:00:47.830131\n",
      "train_loss 0.595 val_loss 0.610 val_auc_score 0.747\n",
      "----End of step 0:00:47.513918\n",
      "train_loss 0.583 val_loss 0.611 val_auc_score 0.765\n",
      "----End of step 0:00:49.486647\n",
      "train_loss 0.574 val_loss 0.604 val_auc_score 0.753\n",
      "----End of step 0:00:48.829801\n",
      "train_loss 0.563 val_loss 0.598 val_auc_score 0.784\n",
      "----End of step 0:00:48.583068\n",
      "train_loss 0.558 val_loss 0.581 val_auc_score 0.793\n",
      "----End of step 0:00:49.614640\n",
      "train_loss 0.547 val_loss 0.570 val_auc_score 0.801\n",
      "----End of step 0:00:48.099477\n",
      "train_loss 0.539 val_loss 0.580 val_auc_score 0.800\n",
      "----End of step 0:00:48.216316\n",
      "train_loss 0.532 val_loss 0.563 val_auc_score 0.812\n",
      "----End of step 0:00:48.919651\n",
      "train_loss 0.528 val_loss 0.556 val_auc_score 0.814\n",
      "----End of step 0:00:48.795610\n",
      "train_loss 0.525 val_loss 0.566 val_auc_score 0.814\n",
      "----End of step 0:00:48.119714\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 1.000\n",
      "train_loss 0.685 val_loss 0.706 val_auc_score 0.613\n",
      "----End of step 0:01:30.019348\n",
      "train_loss 0.667 val_loss 0.690 val_auc_score 0.609\n",
      "----End of step 0:01:29.323219\n",
      "train_loss 0.661 val_loss 0.689 val_auc_score 0.609\n",
      "----End of step 0:01:27.895551\n",
      "train_loss 0.656 val_loss 0.710 val_auc_score 0.621\n",
      "----End of step 0:01:30.402303\n",
      "train_loss 0.642 val_loss 0.682 val_auc_score 0.680\n",
      "----End of step 0:01:29.560260\n",
      "train_loss 0.623 val_loss 0.637 val_auc_score 0.710\n",
      "----End of step 0:01:29.748891\n",
      "train_loss 0.602 val_loss 0.676 val_auc_score 0.717\n",
      "----End of step 0:01:31.582943\n",
      "train_loss 0.584 val_loss 0.674 val_auc_score 0.746\n",
      "----End of step 0:01:34.835951\n",
      "train_loss 0.569 val_loss 0.613 val_auc_score 0.755\n",
      "----End of step 0:01:32.527702\n",
      "train_loss 0.556 val_loss 0.594 val_auc_score 0.800\n",
      "----End of step 0:01:29.661346\n",
      "train_loss 0.547 val_loss 0.577 val_auc_score 0.803\n",
      "----End of step 0:01:28.882469\n",
      "train_loss 0.538 val_loss 0.563 val_auc_score 0.809\n",
      "----End of step 0:01:32.402966\n",
      "train_loss 0.529 val_loss 0.542 val_auc_score 0.819\n",
      "----End of step 0:01:30.392593\n",
      "train_loss 0.524 val_loss 0.546 val_auc_score 0.822\n",
      "----End of step 0:01:28.781658\n",
      "train_loss 0.522 val_loss 0.546 val_auc_score 0.821\n",
      "----End of step 0:01:30.135809\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 0.700\n",
      "train_loss 0.690 val_loss 0.685 val_auc_score 0.575\n",
      "----End of step 0:01:26.018243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.665 val_loss 0.697 val_auc_score 0.607\n",
      "----End of step 0:01:27.782081\n",
      "train_loss 0.673 val_loss 0.719 val_auc_score 0.594\n",
      "----End of step 0:01:28.186904\n",
      "train_loss 0.665 val_loss 0.685 val_auc_score 0.610\n",
      "----End of step 0:01:27.425488\n",
      "train_loss 0.661 val_loss 0.685 val_auc_score 0.635\n",
      "----End of step 0:01:26.112184\n",
      "train_loss 0.646 val_loss 0.695 val_auc_score 0.650\n",
      "----End of step 0:01:26.074170\n",
      "train_loss 0.627 val_loss 0.665 val_auc_score 0.703\n",
      "----End of step 0:01:26.660654\n",
      "train_loss 0.615 val_loss 0.644 val_auc_score 0.697\n",
      "----End of step 0:01:25.576472\n",
      "train_loss 0.603 val_loss 0.645 val_auc_score 0.730\n",
      "----End of step 0:01:25.648382\n",
      "train_loss 0.593 val_loss 0.606 val_auc_score 0.752\n",
      "----End of step 0:01:26.624226\n",
      "train_loss 0.582 val_loss 0.623 val_auc_score 0.754\n",
      "----End of step 0:01:27.907236\n",
      "train_loss 0.572 val_loss 0.595 val_auc_score 0.774\n",
      "----End of step 0:01:27.892983\n",
      "train_loss 0.565 val_loss 0.592 val_auc_score 0.771\n",
      "----End of step 0:01:24.909558\n",
      "train_loss 0.560 val_loss 0.590 val_auc_score 0.777\n",
      "----End of step 0:01:31.002373\n",
      "train_loss 0.558 val_loss 0.591 val_auc_score 0.782\n",
      "----End of step 0:01:28.682810\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 0.600\n",
      "train_loss 0.685 val_loss 0.684 val_auc_score 0.590\n",
      "----End of step 0:01:14.141763\n",
      "train_loss 0.665 val_loss 0.691 val_auc_score 0.635\n",
      "----End of step 0:01:13.211720\n",
      "train_loss 0.658 val_loss 0.715 val_auc_score 0.646\n",
      "----End of step 0:01:16.965493\n",
      "train_loss 0.644 val_loss 0.660 val_auc_score 0.681\n",
      "----End of step 0:01:16.190695\n",
      "train_loss 0.620 val_loss 0.704 val_auc_score 0.685\n",
      "----End of step 0:01:12.480852\n",
      "train_loss 0.605 val_loss 0.618 val_auc_score 0.751\n",
      "----End of step 0:01:19.063635\n",
      "train_loss 0.592 val_loss 0.620 val_auc_score 0.756\n",
      "----End of step 0:01:17.605847\n",
      "train_loss 0.574 val_loss 0.621 val_auc_score 0.777\n",
      "----End of step 0:01:14.147875\n",
      "train_loss 0.564 val_loss 0.590 val_auc_score 0.778\n",
      "----End of step 0:01:12.628373\n",
      "train_loss 0.553 val_loss 0.575 val_auc_score 0.806\n",
      "----End of step 0:01:14.602189\n",
      "train_loss 0.543 val_loss 0.562 val_auc_score 0.811\n",
      "----End of step 0:01:13.403078\n",
      "train_loss 0.537 val_loss 0.568 val_auc_score 0.815\n",
      "----End of step 0:01:12.739076\n",
      "train_loss 0.529 val_loss 0.557 val_auc_score 0.815\n",
      "----End of step 0:01:12.267864\n",
      "train_loss 0.525 val_loss 0.558 val_auc_score 0.816\n",
      "----End of step 0:01:14.627334\n",
      "train_loss 0.523 val_loss 0.557 val_auc_score 0.816\n",
      "----End of step 0:01:17.008257\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 0.500\n",
      "train_loss 0.693 val_loss 0.747 val_auc_score 0.521\n",
      "----End of step 0:01:07.107411\n",
      "train_loss 0.672 val_loss 0.698 val_auc_score 0.601\n",
      "----End of step 0:01:07.304217\n",
      "train_loss 0.665 val_loss 0.672 val_auc_score 0.659\n",
      "----End of step 0:01:05.420678\n",
      "train_loss 0.646 val_loss 0.682 val_auc_score 0.672\n",
      "----End of step 0:01:07.137638\n",
      "train_loss 0.631 val_loss 0.632 val_auc_score 0.718\n",
      "----End of step 0:01:07.184433\n",
      "train_loss 0.614 val_loss 0.626 val_auc_score 0.723\n",
      "----End of step 0:01:07.819564\n",
      "train_loss 0.606 val_loss 0.618 val_auc_score 0.731\n",
      "----End of step 0:01:06.911160\n",
      "train_loss 0.596 val_loss 0.616 val_auc_score 0.745\n",
      "----End of step 0:01:06.370186\n",
      "train_loss 0.583 val_loss 0.603 val_auc_score 0.756\n",
      "----End of step 0:01:06.817691\n",
      "train_loss 0.573 val_loss 0.593 val_auc_score 0.767\n",
      "----End of step 0:01:07.142504\n",
      "train_loss 0.562 val_loss 0.584 val_auc_score 0.792\n",
      "----End of step 0:01:07.772269\n",
      "train_loss 0.554 val_loss 0.577 val_auc_score 0.801\n",
      "----End of step 0:01:07.049894\n",
      "train_loss 0.546 val_loss 0.564 val_auc_score 0.804\n",
      "----End of step 0:01:05.642735\n",
      "train_loss 0.542 val_loss 0.566 val_auc_score 0.801\n",
      "----End of step 0:01:07.056417\n",
      "train_loss 0.539 val_loss 0.570 val_auc_score 0.803\n",
      "----End of step 0:01:03.191967\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 0.300\n",
      "train_loss 0.686 val_loss 0.720 val_auc_score 0.624\n",
      "----End of step 0:00:51.785837\n",
      "train_loss 0.664 val_loss 0.692 val_auc_score 0.619\n",
      "----End of step 0:00:54.316868\n",
      "train_loss 0.660 val_loss 0.672 val_auc_score 0.646\n",
      "----End of step 0:00:54.589580\n",
      "train_loss 0.649 val_loss 0.659 val_auc_score 0.674\n",
      "----End of step 0:00:54.039727\n",
      "train_loss 0.641 val_loss 0.656 val_auc_score 0.683\n",
      "----End of step 0:00:53.855010\n",
      "train_loss 0.623 val_loss 0.657 val_auc_score 0.703\n",
      "----End of step 0:00:54.995706\n",
      "train_loss 0.606 val_loss 0.630 val_auc_score 0.742\n",
      "----End of step 0:00:55.545489\n",
      "train_loss 0.593 val_loss 0.621 val_auc_score 0.751\n",
      "----End of step 0:00:55.068765\n",
      "train_loss 0.581 val_loss 0.615 val_auc_score 0.755\n",
      "----End of step 0:00:54.079873\n",
      "train_loss 0.571 val_loss 0.600 val_auc_score 0.756\n",
      "----End of step 0:00:55.529814\n",
      "train_loss 0.564 val_loss 0.596 val_auc_score 0.774\n",
      "----End of step 0:00:55.874787\n",
      "train_loss 0.557 val_loss 0.591 val_auc_score 0.777\n",
      "----End of step 0:00:56.709919\n",
      "train_loss 0.549 val_loss 0.577 val_auc_score 0.786\n",
      "----End of step 0:00:54.167743\n",
      "train_loss 0.547 val_loss 0.575 val_auc_score 0.785\n",
      "----End of step 0:00:55.343518\n",
      "train_loss 0.545 val_loss 0.573 val_auc_score 0.786\n",
      "----End of step 0:00:55.381113\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 0.200\n",
      "train_loss 0.683 val_loss 0.689 val_auc_score 0.587\n",
      "----End of step 0:00:51.363978\n",
      "train_loss 0.668 val_loss 0.700 val_auc_score 0.623\n",
      "----End of step 0:00:52.051823\n",
      "train_loss 0.654 val_loss 0.677 val_auc_score 0.674\n",
      "----End of step 0:00:53.240062\n",
      "train_loss 0.640 val_loss 0.674 val_auc_score 0.668\n",
      "----End of step 0:00:50.657623\n",
      "train_loss 0.626 val_loss 0.643 val_auc_score 0.716\n",
      "----End of step 0:00:53.029000\n",
      "train_loss 0.613 val_loss 0.646 val_auc_score 0.723\n",
      "----End of step 0:00:51.950526\n",
      "train_loss 0.602 val_loss 0.635 val_auc_score 0.724\n",
      "----End of step 0:00:51.206314\n",
      "train_loss 0.596 val_loss 0.687 val_auc_score 0.743\n",
      "----End of step 0:00:53.087649\n",
      "train_loss 0.587 val_loss 0.671 val_auc_score 0.749\n",
      "----End of step 0:00:52.512762\n",
      "train_loss 0.578 val_loss 0.612 val_auc_score 0.770\n",
      "----End of step 0:00:52.724048\n",
      "train_loss 0.568 val_loss 0.607 val_auc_score 0.767\n",
      "----End of step 0:00:53.226836\n",
      "train_loss 0.558 val_loss 0.582 val_auc_score 0.790\n",
      "----End of step 0:00:51.173861\n",
      "train_loss 0.554 val_loss 0.588 val_auc_score 0.787\n",
      "----End of step 0:00:51.635739\n",
      "train_loss 0.551 val_loss 0.582 val_auc_score 0.788\n",
      "----End of step 0:00:51.512579\n",
      "train_loss 0.549 val_loss 0.583 val_auc_score 0.793\n",
      "----End of step 0:00:53.089415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for w in widths:\n",
    "    for d in depths:\n",
    "        print('width multiplier - %.3f depth multiplier - %.3f' % (w, d))\n",
    "        model = MobileNet(width_mult=w, depth_mult=d).cuda()\n",
    "        p = sum(p.numel() for p in model.parameters())\n",
    "        optimizer = create_optimizer(model, 0.02)\n",
    "        score, t = train_triangular_policy(model, optimizer, train_loader, valid_loader, valid_dataset,\n",
    "                                           loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                                           dataset='mura', binary=True, max_lr=0.02, epochs=15)\n",
    "        data.append([w, d, score, p, t])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['width_x', 'depth_x', 'val_score', 'params', 'time_per_epoch']\n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mura_mobilenet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.read_csv('mura_mobilenet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width_x</th>\n",
       "      <th>depth_x</th>\n",
       "      <th>val_score</th>\n",
       "      <th>params</th>\n",
       "      <th>time_per_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.835978</td>\n",
       "      <td>2225153</td>\n",
       "      <td>0 days 00:01:28.468944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.834497</td>\n",
       "      <td>2170881</td>\n",
       "      <td>0 days 00:01:28.917706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.835956</td>\n",
       "      <td>1717761</td>\n",
       "      <td>0 days 00:01:12.341598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.831918</td>\n",
       "      <td>1654657</td>\n",
       "      <td>0 days 00:01:04.151405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.831631</td>\n",
       "      <td>1201537</td>\n",
       "      <td>0 days 00:00:51.670429000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   width_x  depth_x  val_score   params             time_per_epoch\n",
       "0      1.0      1.0   0.835978  2225153  0 days 00:01:28.468944000\n",
       "1      1.0      0.7   0.834497  2170881  0 days 00:01:28.917706000\n",
       "2      1.0      0.6   0.835956  1717761  0 days 00:01:12.341598000\n",
       "3      1.0      0.5   0.831918  1654657  0 days 00:01:04.151405000\n",
       "4      1.0      0.3   0.831631  1201537  0 days 00:00:51.670429000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
