{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(4)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../prepare_data.py\n",
    "%run ../architectures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, valid_dataset = mura_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 250, 200]), torch.Size([32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11177025"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 s, sys: 8.66 s, total: 58.3 s\n",
      "Wall time: 58.6 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lrs, losses = LR_range_finder(model, train_loader, \n",
    "                              loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                              binary=True, lr_high=0.05)\n",
    "plot_lr(lrs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [1.0, 0.75, 0.5, 0.25]\n",
    "depths = [[[[64, 2], [128, 2]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 2], [128, 2]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 1], [512, 1]]],\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width multiplier - 1.000 depth multiplier - 7.000\n",
      "train_loss 0.681 val_loss 0.694 val_auc_score 0.541\n",
      "----End of step 0:01:01.986017\n",
      "train_loss 0.668 val_loss 0.689 val_auc_score 0.624\n",
      "----End of step 0:01:02.000551\n",
      "train_loss 0.669 val_loss 0.686 val_auc_score 0.611\n",
      "----End of step 0:01:02.029979\n",
      "train_loss 0.661 val_loss 0.702 val_auc_score 0.602\n",
      "----End of step 0:01:02.063659\n",
      "train_loss 0.655 val_loss 0.743 val_auc_score 0.654\n",
      "----End of step 0:01:02.633722\n",
      "train_loss 0.641 val_loss 0.655 val_auc_score 0.697\n",
      "----End of step 0:01:02.231589\n",
      "train_loss 0.625 val_loss 0.748 val_auc_score 0.693\n",
      "----End of step 0:01:02.342450\n",
      "train_loss 0.602 val_loss 0.733 val_auc_score 0.724\n",
      "----End of step 0:01:02.244441\n",
      "train_loss 0.585 val_loss 0.758 val_auc_score 0.657\n",
      "----End of step 0:01:03.147179\n",
      "train_loss 0.571 val_loss 0.590 val_auc_score 0.770\n",
      "----End of step 0:01:02.961666\n",
      "train_loss 0.558 val_loss 0.629 val_auc_score 0.789\n",
      "----End of step 0:01:02.527494\n",
      "train_loss 0.545 val_loss 0.572 val_auc_score 0.785\n",
      "----End of step 0:01:02.049179\n",
      "train_loss 0.536 val_loss 0.549 val_auc_score 0.819\n",
      "----End of step 0:01:02.646643\n",
      "train_loss 0.529 val_loss 0.562 val_auc_score 0.813\n",
      "----End of step 0:01:03.175855\n",
      "train_loss 0.525 val_loss 0.547 val_auc_score 0.818\n",
      "----End of step 0:01:03.022018\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.681 val_loss 0.715 val_auc_score 0.559\n",
      "----End of step 0:01:00.477330\n",
      "train_loss 0.670 val_loss 0.700 val_auc_score 0.585\n",
      "----End of step 0:01:00.082320\n",
      "train_loss 0.670 val_loss 0.687 val_auc_score 0.607\n",
      "----End of step 0:01:00.365186\n",
      "train_loss 0.661 val_loss 0.675 val_auc_score 0.616\n",
      "----End of step 0:01:00.475711\n",
      "train_loss 0.654 val_loss 0.680 val_auc_score 0.662\n",
      "----End of step 0:01:00.053935\n",
      "train_loss 0.648 val_loss 0.895 val_auc_score 0.685\n",
      "----End of step 0:01:00.674493\n",
      "train_loss 0.634 val_loss 0.654 val_auc_score 0.720\n",
      "----End of step 0:01:00.391586\n",
      "train_loss 0.620 val_loss 0.617 val_auc_score 0.727\n",
      "----End of step 0:01:00.570248\n",
      "train_loss 0.597 val_loss 0.628 val_auc_score 0.748\n",
      "----End of step 0:01:00.547982\n",
      "train_loss 0.580 val_loss 0.630 val_auc_score 0.767\n",
      "----End of step 0:01:00.618576\n",
      "train_loss 0.564 val_loss 0.609 val_auc_score 0.789\n",
      "----End of step 0:01:00.233033\n",
      "train_loss 0.554 val_loss 0.590 val_auc_score 0.791\n",
      "----End of step 0:01:00.454502\n",
      "train_loss 0.545 val_loss 0.559 val_auc_score 0.806\n",
      "----End of step 0:00:59.673251\n",
      "train_loss 0.533 val_loss 0.566 val_auc_score 0.814\n",
      "----End of step 0:00:59.898165\n",
      "train_loss 0.530 val_loss 0.554 val_auc_score 0.809\n",
      "----End of step 0:01:00.259887\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.681 val_loss 0.690 val_auc_score 0.533\n",
      "----End of step 0:00:56.577023\n",
      "train_loss 0.668 val_loss 0.710 val_auc_score 0.595\n",
      "----End of step 0:00:56.724590\n",
      "train_loss 0.667 val_loss 0.693 val_auc_score 0.588\n",
      "----End of step 0:00:55.898333\n",
      "train_loss 0.660 val_loss 0.678 val_auc_score 0.619\n",
      "----End of step 0:00:55.998099\n",
      "train_loss 0.652 val_loss 0.719 val_auc_score 0.640\n",
      "----End of step 0:00:56.053877\n",
      "train_loss 0.641 val_loss 0.665 val_auc_score 0.654\n",
      "----End of step 0:00:56.906916\n",
      "train_loss 0.618 val_loss 0.625 val_auc_score 0.713\n",
      "----End of step 0:00:55.899691\n",
      "train_loss 0.596 val_loss 0.671 val_auc_score 0.705\n",
      "----End of step 0:00:56.146263\n",
      "train_loss 0.582 val_loss 0.597 val_auc_score 0.752\n",
      "----End of step 0:00:55.823969\n",
      "train_loss 0.565 val_loss 0.631 val_auc_score 0.714\n",
      "----End of step 0:00:55.658675\n",
      "train_loss 0.557 val_loss 0.604 val_auc_score 0.793\n",
      "----End of step 0:00:55.711614\n",
      "train_loss 0.544 val_loss 0.563 val_auc_score 0.801\n",
      "----End of step 0:00:55.958132\n",
      "train_loss 0.534 val_loss 0.566 val_auc_score 0.801\n",
      "----End of step 0:00:55.972059\n",
      "train_loss 0.531 val_loss 0.557 val_auc_score 0.813\n",
      "----End of step 0:00:56.105013\n",
      "train_loss 0.526 val_loss 0.552 val_auc_score 0.817\n",
      "----End of step 0:00:56.047289\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.682 val_loss 0.691 val_auc_score 0.573\n",
      "----End of step 0:00:58.932943\n",
      "train_loss 0.671 val_loss 0.716 val_auc_score 0.570\n",
      "----End of step 0:00:58.894997\n",
      "train_loss 0.666 val_loss 0.677 val_auc_score 0.623\n",
      "----End of step 0:00:58.725044\n",
      "train_loss 0.669 val_loss 0.705 val_auc_score 0.607\n",
      "----End of step 0:00:58.557476\n",
      "train_loss 0.659 val_loss 0.683 val_auc_score 0.611\n",
      "----End of step 0:00:59.209441\n",
      "train_loss 0.653 val_loss 0.673 val_auc_score 0.658\n",
      "----End of step 0:00:58.776228\n",
      "train_loss 0.641 val_loss 0.669 val_auc_score 0.673\n",
      "----End of step 0:00:58.918654\n",
      "train_loss 0.624 val_loss 0.660 val_auc_score 0.704\n",
      "----End of step 0:00:58.897164\n",
      "train_loss 0.604 val_loss 0.639 val_auc_score 0.734\n",
      "----End of step 0:00:58.737194\n",
      "train_loss 0.585 val_loss 0.591 val_auc_score 0.771\n",
      "----End of step 0:00:58.810345\n",
      "train_loss 0.572 val_loss 0.584 val_auc_score 0.770\n",
      "----End of step 0:00:59.226759\n",
      "train_loss 0.558 val_loss 0.571 val_auc_score 0.784\n",
      "----End of step 0:00:58.633313\n",
      "train_loss 0.548 val_loss 0.567 val_auc_score 0.791\n",
      "----End of step 0:00:58.919474\n",
      "train_loss 0.544 val_loss 0.563 val_auc_score 0.812\n",
      "----End of step 0:00:59.166022\n",
      "train_loss 0.538 val_loss 0.557 val_auc_score 0.802\n",
      "----End of step 0:00:58.746922\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.677 val_loss 0.757 val_auc_score 0.590\n",
      "----End of step 0:00:50.905709\n",
      "train_loss 0.666 val_loss 0.704 val_auc_score 0.627\n",
      "----End of step 0:00:51.718701\n",
      "train_loss 0.663 val_loss 0.685 val_auc_score 0.585\n",
      "----End of step 0:00:51.454376\n",
      "train_loss 0.660 val_loss 0.691 val_auc_score 0.595\n",
      "----End of step 0:00:51.284632\n",
      "train_loss 0.653 val_loss 0.679 val_auc_score 0.647\n",
      "----End of step 0:00:51.015647\n",
      "train_loss 0.647 val_loss 0.662 val_auc_score 0.673\n",
      "----End of step 0:00:51.453000\n",
      "train_loss 0.625 val_loss 0.650 val_auc_score 0.720\n",
      "----End of step 0:00:50.692601\n",
      "train_loss 0.608 val_loss 0.644 val_auc_score 0.720\n",
      "----End of step 0:00:50.959472\n",
      "train_loss 0.595 val_loss 0.679 val_auc_score 0.719\n",
      "----End of step 0:00:51.157308\n",
      "train_loss 0.581 val_loss 0.666 val_auc_score 0.710\n",
      "----End of step 0:00:51.478833\n",
      "train_loss 0.567 val_loss 0.752 val_auc_score 0.750\n",
      "----End of step 0:00:50.899166\n",
      "train_loss 0.558 val_loss 0.573 val_auc_score 0.790\n",
      "----End of step 0:00:52.088311\n",
      "train_loss 0.548 val_loss 0.593 val_auc_score 0.804\n",
      "----End of step 0:00:51.326930\n",
      "train_loss 0.541 val_loss 0.574 val_auc_score 0.797\n",
      "----End of step 0:00:51.156157\n",
      "train_loss 0.540 val_loss 0.568 val_auc_score 0.804\n",
      "----End of step 0:00:51.095131\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 4.000\n",
      "train_loss 0.676 val_loss 0.687 val_auc_score 0.622\n",
      "----End of step 0:00:47.698428\n",
      "train_loss 0.664 val_loss 0.702 val_auc_score 0.596\n",
      "----End of step 0:00:48.428008\n",
      "train_loss 0.661 val_loss 0.679 val_auc_score 0.651\n",
      "----End of step 0:00:48.563185\n",
      "train_loss 0.654 val_loss 0.671 val_auc_score 0.647\n",
      "----End of step 0:00:48.026580\n",
      "train_loss 0.645 val_loss 0.738 val_auc_score 0.637\n",
      "----End of step 0:00:47.981370\n",
      "train_loss 0.629 val_loss 0.656 val_auc_score 0.706\n",
      "----End of step 0:00:48.141081\n",
      "train_loss 0.602 val_loss 0.668 val_auc_score 0.725\n",
      "----End of step 0:00:47.895014\n",
      "train_loss 0.584 val_loss 0.594 val_auc_score 0.758\n",
      "----End of step 0:00:48.167460\n",
      "train_loss 0.568 val_loss 0.584 val_auc_score 0.768\n",
      "----End of step 0:00:48.084694\n",
      "train_loss 0.558 val_loss 0.587 val_auc_score 0.789\n",
      "----End of step 0:00:48.282244\n",
      "train_loss 0.545 val_loss 0.563 val_auc_score 0.818\n",
      "----End of step 0:00:47.612296\n",
      "train_loss 0.537 val_loss 0.575 val_auc_score 0.825\n",
      "----End of step 0:00:48.476094\n",
      "train_loss 0.529 val_loss 0.547 val_auc_score 0.830\n",
      "----End of step 0:00:48.092597\n",
      "train_loss 0.521 val_loss 0.540 val_auc_score 0.831\n",
      "----End of step 0:00:47.520653\n",
      "train_loss 0.516 val_loss 0.542 val_auc_score 0.825\n",
      "----End of step 0:00:47.982466\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 7.000\n",
      "train_loss 0.674 val_loss 0.726 val_auc_score 0.521\n",
      "----End of step 0:00:53.949204\n",
      "train_loss 0.669 val_loss 0.693 val_auc_score 0.576\n",
      "----End of step 0:00:54.289106\n",
      "train_loss 0.669 val_loss 0.704 val_auc_score 0.540\n",
      "----End of step 0:00:54.350334\n",
      "train_loss 0.663 val_loss 0.698 val_auc_score 0.569\n",
      "----End of step 0:00:54.654953\n",
      "train_loss 0.659 val_loss 0.692 val_auc_score 0.619\n",
      "----End of step 0:00:53.981682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.656 val_loss 0.681 val_auc_score 0.591\n",
      "----End of step 0:00:53.529710\n",
      "train_loss 0.650 val_loss 0.677 val_auc_score 0.610\n",
      "----End of step 0:00:53.643713\n",
      "train_loss 0.646 val_loss 0.711 val_auc_score 0.645\n",
      "----End of step 0:00:54.310628\n",
      "train_loss 0.639 val_loss 0.738 val_auc_score 0.649\n",
      "----End of step 0:00:53.476185\n",
      "train_loss 0.629 val_loss 0.638 val_auc_score 0.720\n",
      "----End of step 0:00:54.028083\n",
      "train_loss 0.608 val_loss 0.654 val_auc_score 0.749\n",
      "----End of step 0:00:53.259107\n",
      "train_loss 0.585 val_loss 0.625 val_auc_score 0.770\n",
      "----End of step 0:00:53.542186\n",
      "train_loss 0.568 val_loss 0.672 val_auc_score 0.786\n",
      "----End of step 0:00:53.591490\n",
      "train_loss 0.558 val_loss 0.582 val_auc_score 0.778\n",
      "----End of step 0:00:53.463936\n",
      "train_loss 0.550 val_loss 0.573 val_auc_score 0.794\n",
      "----End of step 0:00:53.611313\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.674 val_loss 0.742 val_auc_score 0.557\n",
      "----End of step 0:00:50.814428\n",
      "train_loss 0.669 val_loss 0.726 val_auc_score 0.589\n",
      "----End of step 0:00:51.116181\n",
      "train_loss 0.664 val_loss 0.714 val_auc_score 0.569\n",
      "----End of step 0:00:51.148519\n",
      "train_loss 0.662 val_loss 0.705 val_auc_score 0.601\n",
      "----End of step 0:00:51.928366\n",
      "train_loss 0.657 val_loss 0.683 val_auc_score 0.618\n",
      "----End of step 0:00:51.170403\n",
      "train_loss 0.655 val_loss 0.694 val_auc_score 0.600\n",
      "----End of step 0:00:51.237072\n",
      "train_loss 0.654 val_loss 0.691 val_auc_score 0.603\n",
      "----End of step 0:00:50.677881\n",
      "train_loss 0.648 val_loss 0.676 val_auc_score 0.636\n",
      "----End of step 0:00:50.747657\n",
      "train_loss 0.637 val_loss 0.775 val_auc_score 0.688\n",
      "----End of step 0:00:51.479959\n",
      "train_loss 0.622 val_loss 0.639 val_auc_score 0.733\n",
      "----End of step 0:00:50.403159\n",
      "train_loss 0.591 val_loss 0.617 val_auc_score 0.745\n",
      "----End of step 0:00:51.492908\n",
      "train_loss 0.569 val_loss 0.613 val_auc_score 0.779\n",
      "----End of step 0:00:50.725854\n",
      "train_loss 0.556 val_loss 0.588 val_auc_score 0.782\n",
      "----End of step 0:00:50.685556\n",
      "train_loss 0.545 val_loss 0.571 val_auc_score 0.793\n",
      "----End of step 0:00:50.724373\n",
      "train_loss 0.539 val_loss 0.569 val_auc_score 0.798\n",
      "----End of step 0:00:50.438097\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.673 val_loss 0.928 val_auc_score 0.581\n",
      "----End of step 0:00:47.784128\n",
      "train_loss 0.664 val_loss 0.679 val_auc_score 0.620\n",
      "----End of step 0:00:48.131762\n",
      "train_loss 0.661 val_loss 0.696 val_auc_score 0.539\n",
      "----End of step 0:00:47.490104\n",
      "train_loss 0.663 val_loss 0.727 val_auc_score 0.614\n",
      "----End of step 0:00:47.487724\n",
      "train_loss 0.656 val_loss 0.668 val_auc_score 0.645\n",
      "----End of step 0:00:48.155311\n",
      "train_loss 0.650 val_loss 0.691 val_auc_score 0.635\n",
      "----End of step 0:00:48.112134\n",
      "train_loss 0.638 val_loss 0.654 val_auc_score 0.703\n",
      "----End of step 0:00:49.210699\n",
      "train_loss 0.618 val_loss 1.008 val_auc_score 0.680\n",
      "----End of step 0:00:48.386351\n",
      "train_loss 0.596 val_loss 0.611 val_auc_score 0.741\n",
      "----End of step 0:00:47.852978\n",
      "train_loss 0.578 val_loss 0.682 val_auc_score 0.771\n",
      "----End of step 0:00:48.106050\n",
      "train_loss 0.565 val_loss 0.585 val_auc_score 0.786\n",
      "----End of step 0:00:47.889954\n",
      "train_loss 0.552 val_loss 0.566 val_auc_score 0.789\n",
      "----End of step 0:00:47.772476\n",
      "train_loss 0.541 val_loss 0.559 val_auc_score 0.809\n",
      "----End of step 0:00:48.022966\n",
      "train_loss 0.535 val_loss 0.558 val_auc_score 0.812\n",
      "----End of step 0:00:47.878658\n",
      "train_loss 0.530 val_loss 0.560 val_auc_score 0.815\n",
      "----End of step 0:00:48.619656\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.676 val_loss 0.717 val_auc_score 0.549\n",
      "----End of step 0:00:50.910684\n",
      "train_loss 0.670 val_loss 0.688 val_auc_score 0.606\n",
      "----End of step 0:00:50.824966\n",
      "train_loss 0.667 val_loss 0.684 val_auc_score 0.614\n",
      "----End of step 0:00:50.376220\n",
      "train_loss 0.663 val_loss 0.695 val_auc_score 0.615\n",
      "----End of step 0:00:51.339224\n",
      "train_loss 0.659 val_loss 0.693 val_auc_score 0.623\n",
      "----End of step 0:00:50.833279\n",
      "train_loss 0.654 val_loss 0.706 val_auc_score 0.625\n",
      "----End of step 0:00:50.648418\n",
      "train_loss 0.645 val_loss 0.670 val_auc_score 0.663\n",
      "----End of step 0:00:50.696294\n",
      "train_loss 0.633 val_loss 0.687 val_auc_score 0.685\n",
      "----End of step 0:00:50.164781\n",
      "train_loss 0.613 val_loss 0.740 val_auc_score 0.687\n",
      "----End of step 0:00:50.687991\n",
      "train_loss 0.593 val_loss 0.633 val_auc_score 0.762\n",
      "----End of step 0:00:50.204962\n",
      "train_loss 0.575 val_loss 0.594 val_auc_score 0.774\n",
      "----End of step 0:00:50.392315\n",
      "train_loss 0.560 val_loss 0.594 val_auc_score 0.769\n",
      "----End of step 0:00:51.941098\n",
      "train_loss 0.549 val_loss 0.583 val_auc_score 0.796\n",
      "----End of step 0:00:50.945706\n",
      "train_loss 0.540 val_loss 0.562 val_auc_score 0.800\n",
      "----End of step 0:00:50.563763\n",
      "train_loss 0.537 val_loss 0.563 val_auc_score 0.804\n",
      "----End of step 0:00:50.365001\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.674 val_loss 0.848 val_auc_score 0.596\n",
      "----End of step 0:00:45.942160\n",
      "train_loss 0.669 val_loss 0.717 val_auc_score 0.549\n",
      "----End of step 0:00:44.758817\n",
      "train_loss 0.668 val_loss 0.701 val_auc_score 0.581\n",
      "----End of step 0:00:46.881048\n",
      "train_loss 0.666 val_loss 0.689 val_auc_score 0.601\n",
      "----End of step 0:00:46.365515\n",
      "train_loss 0.664 val_loss 0.723 val_auc_score 0.613\n",
      "----End of step 0:00:45.044930\n",
      "train_loss 0.653 val_loss 0.747 val_auc_score 0.574\n",
      "----End of step 0:00:46.096962\n",
      "train_loss 0.639 val_loss 0.656 val_auc_score 0.688\n",
      "----End of step 0:00:45.964154\n",
      "train_loss 0.613 val_loss 0.652 val_auc_score 0.689\n",
      "----End of step 0:00:46.014459\n",
      "train_loss 0.589 val_loss 0.698 val_auc_score 0.724\n",
      "----End of step 0:00:46.291878\n",
      "train_loss 0.573 val_loss 0.596 val_auc_score 0.771\n",
      "----End of step 0:00:45.245528\n",
      "train_loss 0.563 val_loss 0.591 val_auc_score 0.772\n",
      "----End of step 0:00:45.268584\n",
      "train_loss 0.552 val_loss 0.583 val_auc_score 0.778\n",
      "----End of step 0:00:45.412804\n",
      "train_loss 0.545 val_loss 0.559 val_auc_score 0.803\n",
      "----End of step 0:00:45.948835\n",
      "train_loss 0.537 val_loss 0.574 val_auc_score 0.812\n",
      "----End of step 0:00:46.119111\n",
      "train_loss 0.534 val_loss 0.557 val_auc_score 0.808\n",
      "----End of step 0:00:45.943403\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 4.000\n",
      "train_loss 0.673 val_loss 0.694 val_auc_score 0.586\n",
      "----End of step 0:00:43.013715\n",
      "train_loss 0.665 val_loss 0.682 val_auc_score 0.609\n",
      "----End of step 0:00:43.342095\n",
      "train_loss 0.659 val_loss 0.712 val_auc_score 0.618\n",
      "----End of step 0:00:43.228198\n",
      "train_loss 0.657 val_loss 0.691 val_auc_score 0.594\n",
      "----End of step 0:00:42.762347\n",
      "train_loss 0.651 val_loss 0.730 val_auc_score 0.635\n",
      "----End of step 0:00:43.437206\n",
      "train_loss 0.638 val_loss 0.702 val_auc_score 0.668\n",
      "----End of step 0:00:43.421207\n",
      "train_loss 0.616 val_loss 0.734 val_auc_score 0.709\n",
      "----End of step 0:00:43.598016\n",
      "train_loss 0.596 val_loss 0.643 val_auc_score 0.767\n",
      "----End of step 0:00:43.666528\n",
      "train_loss 0.580 val_loss 0.599 val_auc_score 0.765\n",
      "----End of step 0:00:42.106539\n",
      "train_loss 0.568 val_loss 0.678 val_auc_score 0.778\n",
      "----End of step 0:00:43.438715\n",
      "train_loss 0.557 val_loss 0.583 val_auc_score 0.767\n",
      "----End of step 0:00:43.527027\n",
      "train_loss 0.547 val_loss 0.577 val_auc_score 0.795\n",
      "----End of step 0:00:43.633535\n",
      "train_loss 0.541 val_loss 0.562 val_auc_score 0.806\n",
      "----End of step 0:00:43.984308\n",
      "train_loss 0.532 val_loss 0.562 val_auc_score 0.807\n",
      "----End of step 0:00:43.680712\n",
      "train_loss 0.532 val_loss 0.557 val_auc_score 0.813\n",
      "----End of step 0:00:44.620635\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 7.000\n",
      "train_loss 0.677 val_loss 0.720 val_auc_score 0.578\n",
      "----End of step 0:00:52.718179\n",
      "train_loss 0.669 val_loss 0.697 val_auc_score 0.561\n",
      "----End of step 0:00:53.236318\n",
      "train_loss 0.667 val_loss 0.693 val_auc_score 0.603\n",
      "----End of step 0:00:53.457736\n",
      "train_loss 0.667 val_loss 0.699 val_auc_score 0.580\n",
      "----End of step 0:00:52.415509\n",
      "train_loss 0.668 val_loss 0.698 val_auc_score 0.583\n",
      "----End of step 0:00:53.503890\n",
      "train_loss 0.662 val_loss 0.730 val_auc_score 0.539\n",
      "----End of step 0:00:52.790920\n",
      "train_loss 0.662 val_loss 0.710 val_auc_score 0.598\n",
      "----End of step 0:00:51.760634\n",
      "train_loss 0.656 val_loss 0.708 val_auc_score 0.617\n",
      "----End of step 0:00:51.570586\n",
      "train_loss 0.652 val_loss 0.715 val_auc_score 0.646\n",
      "----End of step 0:00:49.904144\n",
      "train_loss 0.643 val_loss 0.672 val_auc_score 0.650\n",
      "----End of step 0:00:52.323684\n",
      "train_loss 0.628 val_loss 0.745 val_auc_score 0.696\n",
      "----End of step 0:00:51.689832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.601 val_loss 0.616 val_auc_score 0.738\n",
      "----End of step 0:00:51.628177\n",
      "train_loss 0.580 val_loss 0.598 val_auc_score 0.764\n",
      "----End of step 0:00:50.578230\n",
      "train_loss 0.572 val_loss 0.595 val_auc_score 0.779\n",
      "----End of step 0:00:51.949031\n",
      "train_loss 0.565 val_loss 0.593 val_auc_score 0.785\n",
      "----End of step 0:00:51.239120\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 6.000\n",
      "train_loss 0.670 val_loss 0.735 val_auc_score 0.568\n",
      "----End of step 0:00:48.070799\n",
      "train_loss 0.668 val_loss 0.717 val_auc_score 0.504\n",
      "----End of step 0:00:48.074411\n",
      "train_loss 0.670 val_loss 0.704 val_auc_score 0.539\n",
      "----End of step 0:00:46.023876\n",
      "train_loss 0.664 val_loss 0.685 val_auc_score 0.617\n",
      "----End of step 0:00:49.908097\n",
      "train_loss 0.656 val_loss 0.690 val_auc_score 0.627\n",
      "----End of step 0:00:51.174647\n",
      "train_loss 0.649 val_loss 0.728 val_auc_score 0.631\n",
      "----End of step 0:00:48.524884\n",
      "train_loss 0.635 val_loss 0.732 val_auc_score 0.662\n",
      "----End of step 0:00:48.019088\n",
      "train_loss 0.617 val_loss 0.631 val_auc_score 0.723\n",
      "----End of step 0:00:48.733144\n",
      "train_loss 0.596 val_loss 0.623 val_auc_score 0.750\n",
      "----End of step 0:00:49.689371\n",
      "train_loss 0.578 val_loss 0.580 val_auc_score 0.791\n",
      "----End of step 0:00:48.581506\n",
      "train_loss 0.564 val_loss 0.603 val_auc_score 0.786\n",
      "----End of step 0:00:49.432597\n",
      "train_loss 0.550 val_loss 0.571 val_auc_score 0.786\n",
      "----End of step 0:00:49.475671\n",
      "train_loss 0.542 val_loss 0.584 val_auc_score 0.813\n",
      "----End of step 0:00:48.905321\n",
      "train_loss 0.538 val_loss 0.558 val_auc_score 0.807\n",
      "----End of step 0:00:49.534942\n",
      "train_loss 0.533 val_loss 0.561 val_auc_score 0.809\n",
      "----End of step 0:00:50.545197\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 5.000\n",
      "train_loss 0.667 val_loss 0.688 val_auc_score 0.657\n",
      "----End of step 0:00:45.726808\n",
      "train_loss 0.663 val_loss 0.701 val_auc_score 0.544\n",
      "----End of step 0:00:49.003514\n",
      "train_loss 0.660 val_loss 0.704 val_auc_score 0.578\n",
      "----End of step 0:00:46.370154\n",
      "train_loss 0.655 val_loss 0.694 val_auc_score 0.615\n",
      "----End of step 0:00:48.834214\n",
      "train_loss 0.655 val_loss 0.748 val_auc_score 0.595\n",
      "----End of step 0:00:46.067819\n",
      "train_loss 0.650 val_loss 0.675 val_auc_score 0.651\n",
      "----End of step 0:00:47.783185\n",
      "train_loss 0.640 val_loss 0.664 val_auc_score 0.660\n",
      "----End of step 0:00:46.488355\n",
      "train_loss 0.621 val_loss 0.636 val_auc_score 0.712\n",
      "----End of step 0:00:46.707593\n",
      "train_loss 0.598 val_loss 0.638 val_auc_score 0.734\n",
      "----End of step 0:00:46.663400\n",
      "train_loss 0.582 val_loss 0.603 val_auc_score 0.761\n",
      "----End of step 0:00:47.764108\n",
      "train_loss 0.565 val_loss 0.618 val_auc_score 0.779\n",
      "----End of step 0:00:47.696140\n",
      "train_loss 0.552 val_loss 0.583 val_auc_score 0.793\n",
      "----End of step 0:00:48.335380\n",
      "train_loss 0.542 val_loss 0.560 val_auc_score 0.807\n",
      "----End of step 0:00:47.198349\n",
      "train_loss 0.536 val_loss 0.564 val_auc_score 0.809\n",
      "----End of step 0:00:48.772956\n",
      "train_loss 0.531 val_loss 0.567 val_auc_score 0.811\n",
      "----End of step 0:00:46.960827\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 6.000\n",
      "train_loss 0.673 val_loss 0.714 val_auc_score 0.596\n",
      "----End of step 0:00:49.964000\n",
      "train_loss 0.666 val_loss 0.689 val_auc_score 0.602\n",
      "----End of step 0:00:49.477340\n",
      "train_loss 0.662 val_loss 0.692 val_auc_score 0.625\n",
      "----End of step 0:00:51.472218\n",
      "train_loss 0.658 val_loss 0.715 val_auc_score 0.575\n",
      "----End of step 0:00:50.591709\n",
      "train_loss 0.657 val_loss 0.692 val_auc_score 0.608\n",
      "----End of step 0:00:49.793442\n",
      "train_loss 0.651 val_loss 0.677 val_auc_score 0.628\n",
      "----End of step 0:00:47.626281\n",
      "train_loss 0.643 val_loss 0.682 val_auc_score 0.636\n",
      "----End of step 0:00:46.708470\n",
      "train_loss 0.635 val_loss 0.842 val_auc_score 0.681\n",
      "----End of step 0:00:45.819407\n",
      "train_loss 0.623 val_loss 0.654 val_auc_score 0.705\n",
      "----End of step 0:00:45.777120\n",
      "train_loss 0.601 val_loss 0.626 val_auc_score 0.742\n",
      "----End of step 0:00:46.750695\n",
      "train_loss 0.580 val_loss 0.612 val_auc_score 0.769\n",
      "----End of step 0:00:47.078491\n",
      "train_loss 0.567 val_loss 0.584 val_auc_score 0.778\n",
      "----End of step 0:00:46.707850\n",
      "train_loss 0.557 val_loss 0.582 val_auc_score 0.791\n",
      "----End of step 0:00:46.860278\n",
      "train_loss 0.550 val_loss 0.587 val_auc_score 0.794\n",
      "----End of step 0:00:45.457097\n",
      "train_loss 0.548 val_loss 0.580 val_auc_score 0.793\n",
      "----End of step 0:00:46.896465\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 5.000\n",
      "train_loss 0.674 val_loss 0.706 val_auc_score 0.600\n",
      "----End of step 0:00:43.864390\n",
      "train_loss 0.665 val_loss 0.685 val_auc_score 0.611\n",
      "----End of step 0:00:43.287958\n",
      "train_loss 0.662 val_loss 0.712 val_auc_score 0.580\n",
      "----End of step 0:00:43.688828\n",
      "train_loss 0.658 val_loss 0.692 val_auc_score 0.629\n",
      "----End of step 0:00:44.019844\n",
      "train_loss 0.654 val_loss 0.735 val_auc_score 0.631\n",
      "----End of step 0:00:42.042407\n",
      "train_loss 0.648 val_loss 0.780 val_auc_score 0.611\n",
      "----End of step 0:00:42.800063\n",
      "train_loss 0.633 val_loss 0.641 val_auc_score 0.702\n",
      "----End of step 0:00:43.888750\n",
      "train_loss 0.613 val_loss 0.662 val_auc_score 0.699\n",
      "----End of step 0:00:43.843565\n",
      "train_loss 0.589 val_loss 0.602 val_auc_score 0.767\n",
      "----End of step 0:00:43.457333\n",
      "train_loss 0.574 val_loss 0.576 val_auc_score 0.784\n",
      "----End of step 0:00:43.814675\n",
      "train_loss 0.565 val_loss 0.579 val_auc_score 0.780\n",
      "----End of step 0:00:43.047553\n",
      "train_loss 0.553 val_loss 0.562 val_auc_score 0.802\n",
      "----End of step 0:00:43.885517\n",
      "train_loss 0.546 val_loss 0.570 val_auc_score 0.807\n",
      "----End of step 0:00:44.064658\n",
      "train_loss 0.538 val_loss 0.562 val_auc_score 0.805\n",
      "----End of step 0:00:43.889587\n",
      "train_loss 0.536 val_loss 0.555 val_auc_score 0.809\n",
      "----End of step 0:00:43.125913\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 4.000\n",
      "train_loss 0.672 val_loss 0.703 val_auc_score 0.626\n",
      "----End of step 0:00:41.293368\n",
      "train_loss 0.663 val_loss 0.683 val_auc_score 0.587\n",
      "----End of step 0:00:40.653258\n",
      "train_loss 0.660 val_loss 0.703 val_auc_score 0.617\n",
      "----End of step 0:00:40.689343\n",
      "train_loss 0.658 val_loss 0.702 val_auc_score 0.578\n",
      "----End of step 0:00:39.969741\n",
      "train_loss 0.656 val_loss 0.720 val_auc_score 0.607\n",
      "----End of step 0:00:39.473222\n",
      "train_loss 0.650 val_loss 0.683 val_auc_score 0.655\n",
      "----End of step 0:00:40.552283\n",
      "train_loss 0.633 val_loss 0.661 val_auc_score 0.684\n",
      "----End of step 0:00:40.765483\n",
      "train_loss 0.607 val_loss 0.617 val_auc_score 0.729\n",
      "----End of step 0:00:40.393966\n",
      "train_loss 0.587 val_loss 0.625 val_auc_score 0.737\n",
      "----End of step 0:00:40.508507\n",
      "train_loss 0.571 val_loss 0.607 val_auc_score 0.752\n",
      "----End of step 0:00:40.949847\n",
      "train_loss 0.560 val_loss 0.583 val_auc_score 0.770\n",
      "----End of step 0:00:40.917928\n",
      "train_loss 0.550 val_loss 0.580 val_auc_score 0.794\n",
      "----End of step 0:00:40.228438\n",
      "train_loss 0.541 val_loss 0.569 val_auc_score 0.788\n",
      "----End of step 0:00:40.225057\n",
      "train_loss 0.534 val_loss 0.566 val_auc_score 0.797\n",
      "----End of step 0:00:40.025267\n",
      "train_loss 0.531 val_loss 0.570 val_auc_score 0.795\n",
      "----End of step 0:00:40.885923\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 7.000\n",
      "train_loss 0.672 val_loss 0.714 val_auc_score 0.618\n",
      "----End of step 0:00:48.779757\n",
      "train_loss 0.667 val_loss 0.694 val_auc_score 0.586\n",
      "----End of step 0:00:49.763407\n",
      "train_loss 0.663 val_loss 0.684 val_auc_score 0.594\n",
      "----End of step 0:00:49.929592\n",
      "train_loss 0.660 val_loss 0.684 val_auc_score 0.639\n",
      "----End of step 0:00:49.988532\n",
      "train_loss 0.655 val_loss 0.675 val_auc_score 0.651\n",
      "----End of step 0:00:48.211374\n",
      "train_loss 0.648 val_loss 0.729 val_auc_score 0.644\n",
      "----End of step 0:00:49.359830\n",
      "train_loss 0.634 val_loss 0.694 val_auc_score 0.676\n",
      "----End of step 0:00:49.189303\n",
      "train_loss 0.623 val_loss 1.248 val_auc_score 0.635\n",
      "----End of step 0:00:48.130297\n",
      "train_loss 0.606 val_loss 0.839 val_auc_score 0.700\n",
      "----End of step 0:00:49.521631\n",
      "train_loss 0.582 val_loss 0.599 val_auc_score 0.762\n",
      "----End of step 0:00:50.272316\n",
      "train_loss 0.564 val_loss 0.578 val_auc_score 0.782\n",
      "----End of step 0:00:49.391768\n",
      "train_loss 0.552 val_loss 0.592 val_auc_score 0.776\n",
      "----End of step 0:00:49.103972\n",
      "train_loss 0.543 val_loss 0.584 val_auc_score 0.815\n",
      "----End of step 0:00:49.627198\n",
      "train_loss 0.538 val_loss 0.542 val_auc_score 0.822\n",
      "----End of step 0:00:49.702600\n",
      "train_loss 0.532 val_loss 0.548 val_auc_score 0.822\n",
      "----End of step 0:00:48.716595\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 6.000\n",
      "train_loss 0.667 val_loss 0.785 val_auc_score 0.584\n",
      "----End of step 0:00:45.696540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.669 val_loss 0.743 val_auc_score 0.544\n",
      "----End of step 0:00:45.202504\n",
      "train_loss 0.663 val_loss 0.677 val_auc_score 0.613\n",
      "----End of step 0:00:46.150795\n",
      "train_loss 0.660 val_loss 0.700 val_auc_score 0.637\n",
      "----End of step 0:00:47.370070\n",
      "train_loss 0.650 val_loss 0.675 val_auc_score 0.640\n",
      "----End of step 0:00:45.596044\n",
      "train_loss 0.642 val_loss 0.684 val_auc_score 0.647\n",
      "----End of step 0:00:45.617560\n",
      "train_loss 0.625 val_loss 0.926 val_auc_score 0.664\n",
      "----End of step 0:00:46.557781\n",
      "train_loss 0.596 val_loss 0.620 val_auc_score 0.751\n",
      "----End of step 0:00:47.149272\n",
      "train_loss 0.578 val_loss 0.599 val_auc_score 0.765\n",
      "----End of step 0:00:46.869487\n",
      "train_loss 0.566 val_loss 0.592 val_auc_score 0.787\n",
      "----End of step 0:00:45.970214\n",
      "train_loss 0.553 val_loss 0.621 val_auc_score 0.792\n",
      "----End of step 0:00:45.962802\n",
      "train_loss 0.544 val_loss 0.580 val_auc_score 0.801\n",
      "----End of step 0:00:46.905632\n",
      "train_loss 0.535 val_loss 0.553 val_auc_score 0.803\n",
      "----End of step 0:00:46.196317\n",
      "train_loss 0.530 val_loss 0.548 val_auc_score 0.811\n",
      "----End of step 0:00:45.987275\n",
      "train_loss 0.527 val_loss 0.547 val_auc_score 0.813\n",
      "----End of step 0:00:46.418045\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 5.000\n",
      "train_loss 0.669 val_loss 0.712 val_auc_score 0.620\n",
      "----End of step 0:00:43.280026\n",
      "train_loss 0.665 val_loss 0.688 val_auc_score 0.605\n",
      "----End of step 0:00:43.568570\n",
      "train_loss 0.659 val_loss 0.703 val_auc_score 0.615\n",
      "----End of step 0:00:42.677213\n",
      "train_loss 0.652 val_loss 0.653 val_auc_score 0.679\n",
      "----End of step 0:00:43.405033\n",
      "train_loss 0.639 val_loss 0.702 val_auc_score 0.629\n",
      "----End of step 0:00:42.628419\n",
      "train_loss 0.626 val_loss 0.661 val_auc_score 0.725\n",
      "----End of step 0:00:44.367178\n",
      "train_loss 0.609 val_loss 0.693 val_auc_score 0.696\n",
      "----End of step 0:00:43.985882\n",
      "train_loss 0.593 val_loss 0.634 val_auc_score 0.719\n",
      "----End of step 0:00:45.102594\n",
      "train_loss 0.579 val_loss 0.611 val_auc_score 0.770\n",
      "----End of step 0:00:44.722887\n",
      "train_loss 0.565 val_loss 0.639 val_auc_score 0.777\n",
      "----End of step 0:00:44.409578\n",
      "train_loss 0.556 val_loss 0.646 val_auc_score 0.776\n",
      "----End of step 0:00:45.029056\n",
      "train_loss 0.546 val_loss 0.568 val_auc_score 0.788\n",
      "----End of step 0:00:45.558111\n",
      "train_loss 0.539 val_loss 0.574 val_auc_score 0.780\n",
      "----End of step 0:00:45.106036\n",
      "train_loss 0.533 val_loss 0.555 val_auc_score 0.803\n",
      "----End of step 0:00:43.429326\n",
      "train_loss 0.531 val_loss 0.554 val_auc_score 0.806\n",
      "----End of step 0:00:45.391294\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 6.000\n",
      "train_loss 0.669 val_loss 0.690 val_auc_score 0.598\n",
      "----End of step 0:00:47.407589\n",
      "train_loss 0.665 val_loss 0.680 val_auc_score 0.608\n",
      "----End of step 0:00:47.429668\n",
      "train_loss 0.662 val_loss 0.674 val_auc_score 0.630\n",
      "----End of step 0:00:45.974418\n",
      "train_loss 0.656 val_loss 0.677 val_auc_score 0.620\n",
      "----End of step 0:00:47.146542\n",
      "train_loss 0.647 val_loss 0.730 val_auc_score 0.611\n",
      "----End of step 0:00:46.273841\n",
      "train_loss 0.630 val_loss 0.689 val_auc_score 0.698\n",
      "----End of step 0:00:47.695828\n",
      "train_loss 0.613 val_loss 0.645 val_auc_score 0.721\n",
      "----End of step 0:00:47.185134\n",
      "train_loss 0.595 val_loss 0.636 val_auc_score 0.740\n",
      "----End of step 0:00:46.790268\n",
      "train_loss 0.579 val_loss 0.616 val_auc_score 0.726\n",
      "----End of step 0:00:46.398416\n",
      "train_loss 0.566 val_loss 0.617 val_auc_score 0.764\n",
      "----End of step 0:00:48.044664\n",
      "train_loss 0.554 val_loss 0.624 val_auc_score 0.747\n",
      "----End of step 0:00:47.773457\n",
      "train_loss 0.545 val_loss 0.565 val_auc_score 0.797\n",
      "----End of step 0:00:48.218172\n",
      "train_loss 0.536 val_loss 0.554 val_auc_score 0.804\n",
      "----End of step 0:00:47.340994\n",
      "train_loss 0.530 val_loss 0.552 val_auc_score 0.807\n",
      "----End of step 0:00:46.616537\n",
      "train_loss 0.527 val_loss 0.552 val_auc_score 0.808\n",
      "----End of step 0:00:47.976586\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 5.000\n",
      "train_loss 0.671 val_loss 0.738 val_auc_score 0.551\n",
      "----End of step 0:00:44.684237\n",
      "train_loss 0.666 val_loss 0.695 val_auc_score 0.586\n",
      "----End of step 0:00:44.108010\n",
      "train_loss 0.660 val_loss 0.718 val_auc_score 0.620\n",
      "----End of step 0:00:44.093269\n",
      "train_loss 0.655 val_loss 0.687 val_auc_score 0.596\n",
      "----End of step 0:00:43.059726\n",
      "train_loss 0.647 val_loss 0.671 val_auc_score 0.664\n",
      "----End of step 0:00:43.548553\n",
      "train_loss 0.630 val_loss 1.045 val_auc_score 0.641\n",
      "----End of step 0:00:43.155727\n",
      "train_loss 0.607 val_loss 0.619 val_auc_score 0.729\n",
      "----End of step 0:00:42.555078\n",
      "train_loss 0.587 val_loss 0.619 val_auc_score 0.734\n",
      "----End of step 0:00:42.826314\n",
      "train_loss 0.572 val_loss 0.623 val_auc_score 0.737\n",
      "----End of step 0:00:42.127757\n",
      "train_loss 0.564 val_loss 0.585 val_auc_score 0.771\n",
      "----End of step 0:00:42.274924\n",
      "train_loss 0.553 val_loss 0.680 val_auc_score 0.799\n",
      "----End of step 0:00:42.933516\n",
      "train_loss 0.543 val_loss 0.561 val_auc_score 0.811\n",
      "----End of step 0:00:42.472310\n",
      "train_loss 0.536 val_loss 0.550 val_auc_score 0.817\n",
      "----End of step 0:00:42.839933\n",
      "train_loss 0.531 val_loss 0.552 val_auc_score 0.819\n",
      "----End of step 0:00:42.755004\n",
      "train_loss 0.526 val_loss 0.551 val_auc_score 0.817\n",
      "----End of step 0:00:43.445783\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 4.000\n",
      "train_loss 0.666 val_loss 0.670 val_auc_score 0.645\n",
      "----End of step 0:00:40.676118\n",
      "train_loss 0.662 val_loss 0.733 val_auc_score 0.533\n",
      "----End of step 0:00:39.550905\n",
      "train_loss 0.659 val_loss 0.679 val_auc_score 0.627\n",
      "----End of step 0:00:39.027563\n",
      "train_loss 0.653 val_loss 0.672 val_auc_score 0.637\n",
      "----End of step 0:00:38.493430\n",
      "train_loss 0.649 val_loss 0.676 val_auc_score 0.633\n",
      "----End of step 0:00:39.131363\n",
      "train_loss 0.639 val_loss 0.679 val_auc_score 0.678\n",
      "----End of step 0:00:38.701558\n",
      "train_loss 0.620 val_loss 0.857 val_auc_score 0.649\n",
      "----End of step 0:00:39.092293\n",
      "train_loss 0.594 val_loss 0.608 val_auc_score 0.757\n",
      "----End of step 0:00:39.213606\n",
      "train_loss 0.576 val_loss 0.601 val_auc_score 0.761\n",
      "----End of step 0:00:38.705348\n",
      "train_loss 0.561 val_loss 0.586 val_auc_score 0.790\n",
      "----End of step 0:00:39.003016\n",
      "train_loss 0.551 val_loss 0.597 val_auc_score 0.764\n",
      "----End of step 0:00:39.167101\n",
      "train_loss 0.541 val_loss 0.562 val_auc_score 0.795\n",
      "----End of step 0:00:38.974720\n",
      "train_loss 0.534 val_loss 0.552 val_auc_score 0.805\n",
      "----End of step 0:00:39.567373\n",
      "train_loss 0.525 val_loss 0.559 val_auc_score 0.812\n",
      "----End of step 0:00:38.886783\n",
      "train_loss 0.522 val_loss 0.552 val_auc_score 0.811\n",
      "----End of step 0:00:39.418699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for w in widths:\n",
    "    for d in depths:\n",
    "        d_s = sum(j[1] for i in d for j in i)\n",
    "        print('width multiplier - %.3f depth multiplier - %.3f' % (w, d_s))\n",
    "        model = resnet18(width_mult=w, \n",
    "                         inverted_residual_setting1=d[0], \n",
    "                         inverted_residual_setting2=d[1]).cuda()\n",
    "        \n",
    "        p = sum(p.numel() for p in model.parameters())\n",
    "        optimizer = create_optimizer(model, 0.02)\n",
    "        score, t = train_triangular_policy(model, optimizer, train_loader, valid_loader, valid_dataset,\n",
    "                                           loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                                           dataset='mura', binary=True, max_lr=0.02, epochs=15)\n",
    "        data.append([w, d_s, score, p, t])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['width_x', 'depth_x', 'val_score', 'params', 'time_per_epoch']\n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mura_resnet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.read_csv('mura_resnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width_x</th>\n",
       "      <th>depth_x</th>\n",
       "      <th>val_score</th>\n",
       "      <th>params</th>\n",
       "      <th>time_per_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.818418</td>\n",
       "      <td>11177025</td>\n",
       "      <td>0 days 00:01:03.022018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.809076</td>\n",
       "      <td>9996353</td>\n",
       "      <td>0 days 00:01:00.259887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.817026</td>\n",
       "      <td>9700929</td>\n",
       "      <td>0 days 00:00:56.047289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.802111</td>\n",
       "      <td>10881601</td>\n",
       "      <td>0 days 00:00:58.746922000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>10807617</td>\n",
       "      <td>0 days 00:00:51.095131000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   width_x  depth_x  val_score    params             time_per_epoch\n",
       "0      1.0        7   0.818418  11177025  0 days 00:01:03.022018000\n",
       "1      1.0        6   0.809076   9996353  0 days 00:01:00.259887000\n",
       "2      1.0        5   0.817026   9700929  0 days 00:00:56.047289000\n",
       "3      1.0        6   0.802111  10881601  0 days 00:00:58.746922000\n",
       "4      1.0        5   0.803922  10807617  0 days 00:00:51.095131000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
