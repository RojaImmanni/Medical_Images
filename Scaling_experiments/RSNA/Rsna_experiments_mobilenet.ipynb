{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../prepare_data.py\n",
    "%run ../../architectures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, valid_dataset = rsna_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 256, 256]), torch.Size([32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNet(width_mult=1.0, depth_mult=1.0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225153"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 6.25 s, total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lrs, losses = LR_range_finder(model, train_loader, \n",
    "                              loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                              binary=True, lr_high=0.5)\n",
    "plot_lr(lrs, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.023 val_loss 101.072 val_auc_score 0.525\n",
      "----End of step 0:01:37.198823\n",
      "train_loss 0.275 val_loss 35.152 val_auc_score 0.419\n",
      "----End of step 0:01:37.102364\n",
      "train_loss 0.390 val_loss 8.248 val_auc_score 0.531\n",
      "----End of step 0:01:36.998774\n",
      "train_loss 0.088 val_loss 17.455 val_auc_score 0.527\n",
      "----End of step 0:01:36.439648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d285d6279d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_triangular_policy(model, optimizer, train_loader, valid_loader, valid_dataset,\n\u001b[1;32m      3\u001b[0m                                            \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                            dataset='rsna', binary=True, max_lr=0.07, epochs=15)\n\u001b[0m",
      "\u001b[0;32m~/Medical_Images/training.py\u001b[0m in \u001b[0;36mtrain_triangular_policy\u001b[0;34m(model, optimizer, train_dl, valid_dl, valid_dataset, loss_fn, dataset, binary, max_lr, epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_metrics_mura\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rsna'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_metrics_rsna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"chexpert\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_metrics_chexpert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Medical_Images/training.py\u001b[0m in \u001b[0;36mval_metrics_rsna\u001b[0;34m(model, valid_dl)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = create_optimizer(model, 0.07)\n",
    "train_triangular_policy(model, optimizer, train_loader, valid_loader, valid_dataset,\n",
    "                                           loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                                           dataset='rsna', binary=True, max_lr=0.07, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [1.0, 0.75, 0.5, 0.25]\n",
    "depths = [1.0, 0.7, 0.6, 0.5, 0.3, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width multiplier - 1.000 depth multiplier - 1.000\n",
      "train_loss 0.029 val_loss 19.674 val_auc_score 0.549\n",
      "----End of step 0:01:41.731872\n",
      "train_loss 0.163 val_loss 27.570 val_auc_score 0.389\n",
      "----End of step 0:01:42.033404\n",
      "train_loss 0.121 val_loss 145.906 val_auc_score 0.450\n",
      "----End of step 0:01:41.486886\n",
      "train_loss 1.139 val_loss 399.562 val_auc_score 0.511\n",
      "----End of step 0:01:41.224449\n",
      "train_loss 3.791 val_loss 189.413 val_auc_score 0.542\n",
      "----End of step 0:01:41.078969\n",
      "train_loss 3.450 val_loss 170.412 val_auc_score 0.584\n",
      "----End of step 0:01:42.265765\n",
      "train_loss 3.245 val_loss 318.524 val_auc_score 0.413\n",
      "----End of step 0:01:41.405434\n",
      "train_loss 4.110 val_loss 471.268 val_auc_score 0.428\n",
      "----End of step 0:01:42.035988\n",
      "train_loss 5.233 val_loss 141.611 val_auc_score 0.421\n",
      "----End of step 0:01:41.985906\n",
      "train_loss 2.429 val_loss 73.253 val_auc_score 0.397\n",
      "----End of step 0:01:41.928575\n",
      "train_loss 1.273 val_loss 59.724 val_auc_score 0.589\n",
      "----End of step 0:01:43.313878\n",
      "train_loss 1.195 val_loss 37.683 val_auc_score 0.520\n",
      "----End of step 0:01:42.914740\n",
      "train_loss 0.659 val_loss 16.732 val_auc_score 0.607\n",
      "----End of step 0:01:41.490072\n",
      "train_loss 0.394 val_loss 8.689 val_auc_score 0.663\n",
      "----End of step 0:01:42.057463\n",
      "train_loss 0.494 val_loss 4.407 val_auc_score 0.703\n",
      "----End of step 0:01:41.957913\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.700\n",
      "train_loss 0.025 val_loss 26.827 val_auc_score 0.411\n",
      "----End of step 0:01:39.675424\n",
      "train_loss 0.276 val_loss 29.022 val_auc_score 0.583\n",
      "----End of step 0:01:38.104051\n",
      "train_loss 0.294 val_loss 100.077 val_auc_score 0.515\n",
      "----End of step 0:01:38.427182\n",
      "train_loss 3.429 val_loss 409.066 val_auc_score 0.367\n",
      "----End of step 0:01:37.639053\n",
      "train_loss 2.834 val_loss 390.399 val_auc_score 0.395\n",
      "----End of step 0:01:38.797813\n",
      "train_loss 1.943 val_loss 128.176 val_auc_score 0.520\n",
      "----End of step 0:01:39.746107\n",
      "train_loss 1.323 val_loss 228.534 val_auc_score 0.449\n",
      "----End of step 0:01:37.499054\n",
      "train_loss 1.757 val_loss 49.794 val_auc_score 0.381\n",
      "----End of step 0:01:37.974609\n",
      "train_loss 0.759 val_loss 75.635 val_auc_score 0.530\n",
      "----End of step 0:01:38.050408\n",
      "train_loss 0.707 val_loss 39.388 val_auc_score 0.390\n",
      "----End of step 0:01:39.012568\n",
      "train_loss 0.524 val_loss 30.494 val_auc_score 0.427\n",
      "----End of step 0:01:39.171910\n",
      "train_loss 0.384 val_loss 14.617 val_auc_score 0.439\n",
      "----End of step 0:01:38.142002\n",
      "train_loss 0.237 val_loss 10.239 val_auc_score 0.445\n",
      "----End of step 0:01:38.388555\n",
      "train_loss 0.312 val_loss 6.215 val_auc_score 0.438\n",
      "----End of step 0:01:39.201975\n",
      "train_loss 0.355 val_loss 3.504 val_auc_score 0.444\n",
      "----End of step 0:01:39.544917\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.600\n",
      "train_loss 0.028 val_loss 36.505 val_auc_score 0.480\n",
      "----End of step 0:01:28.133688\n",
      "train_loss 0.233 val_loss 21.617 val_auc_score 0.565\n",
      "----End of step 0:01:28.343879\n",
      "train_loss 0.172 val_loss 100.239 val_auc_score 0.553\n",
      "----End of step 0:01:26.790387\n",
      "train_loss 1.099 val_loss 64.010 val_auc_score 0.412\n",
      "----End of step 0:01:26.870446\n",
      "train_loss 1.091 val_loss 140.782 val_auc_score 0.499\n",
      "----End of step 0:01:27.407233\n",
      "train_loss 1.807 val_loss 124.475 val_auc_score 0.494\n",
      "----End of step 0:01:27.686849\n",
      "train_loss 1.774 val_loss 682.920 val_auc_score 0.508\n",
      "----End of step 0:01:26.531129\n",
      "train_loss 5.544 val_loss 133.035 val_auc_score 0.479\n",
      "----End of step 0:01:28.216170\n",
      "train_loss 1.973 val_loss 63.544 val_auc_score 0.479\n",
      "----End of step 0:01:25.756753\n",
      "train_loss 0.741 val_loss 54.774 val_auc_score 0.646\n",
      "----End of step 0:01:28.820051\n",
      "train_loss 0.689 val_loss 93.056 val_auc_score 0.718\n",
      "----End of step 0:01:28.051157\n",
      "train_loss 2.141 val_loss 81.164 val_auc_score 0.404\n",
      "----End of step 0:01:27.237846\n",
      "train_loss 1.487 val_loss 47.104 val_auc_score 0.638\n",
      "----End of step 0:01:26.797536\n",
      "train_loss 1.376 val_loss 18.302 val_auc_score 0.591\n",
      "----End of step 0:01:26.397078\n",
      "train_loss 1.939 val_loss 11.573 val_auc_score 0.492\n",
      "----End of step 0:01:26.106067\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.500\n",
      "train_loss 0.031 val_loss 45.495 val_auc_score 0.493\n",
      "----End of step 0:01:19.774766\n",
      "train_loss 0.246 val_loss 20.946 val_auc_score 0.450\n",
      "----End of step 0:01:20.807076\n",
      "train_loss 0.204 val_loss 160.315 val_auc_score 0.429\n",
      "----End of step 0:01:19.304858\n",
      "train_loss 1.899 val_loss 596.135 val_auc_score 0.467\n",
      "----End of step 0:01:20.555523\n",
      "train_loss 5.977 val_loss 466.123 val_auc_score 0.476\n",
      "----End of step 0:01:20.890449\n",
      "train_loss 5.356 val_loss 448.206 val_auc_score 0.534\n",
      "----End of step 0:01:21.056035\n",
      "train_loss 3.858 val_loss 496.815 val_auc_score 0.651\n",
      "----End of step 0:01:17.882689\n",
      "train_loss 3.566 val_loss 261.871 val_auc_score 0.599\n",
      "----End of step 0:01:19.926800\n",
      "train_loss 4.865 val_loss 306.157 val_auc_score 0.373\n",
      "----End of step 0:01:20.811158\n",
      "train_loss 3.731 val_loss 69.245 val_auc_score 0.455\n",
      "----End of step 0:01:21.544714\n",
      "train_loss 1.118 val_loss 37.242 val_auc_score 0.658\n",
      "----End of step 0:01:22.064535\n",
      "train_loss 0.663 val_loss 20.297 val_auc_score 0.520\n",
      "----End of step 0:01:21.351573\n",
      "train_loss 0.529 val_loss 11.150 val_auc_score 0.412\n",
      "----End of step 0:01:21.597640\n",
      "train_loss 0.348 val_loss 5.169 val_auc_score 0.466\n",
      "----End of step 0:01:20.167452\n",
      "train_loss 0.350 val_loss 3.212 val_auc_score 0.404\n",
      "----End of step 0:01:20.035026\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.300\n",
      "train_loss 0.030 val_loss 26.986 val_auc_score 0.401\n",
      "----End of step 0:01:12.339940\n",
      "train_loss 0.215 val_loss 32.195 val_auc_score 0.478\n",
      "----End of step 0:01:14.661139\n",
      "train_loss 0.353 val_loss 275.592 val_auc_score 0.557\n",
      "----End of step 0:01:16.848855\n",
      "train_loss 3.512 val_loss 203.947 val_auc_score 0.508\n",
      "----End of step 0:01:15.579679\n",
      "train_loss 4.218 val_loss 571.086 val_auc_score 0.409\n",
      "----End of step 0:01:17.082415\n",
      "train_loss 5.621 val_loss 323.909 val_auc_score 0.453\n",
      "----End of step 0:01:14.995788\n",
      "train_loss 4.037 val_loss 204.991 val_auc_score 0.442\n",
      "----End of step 0:01:15.999116\n",
      "train_loss 1.835 val_loss 103.028 val_auc_score 0.580\n",
      "----End of step 0:01:17.099801\n",
      "train_loss 1.052 val_loss 21.916 val_auc_score 0.491\n",
      "----End of step 0:01:16.218963\n",
      "train_loss 0.784 val_loss 37.136 val_auc_score 0.475\n",
      "----End of step 0:01:15.361271\n",
      "train_loss 0.365 val_loss 102.632 val_auc_score 0.534\n",
      "----End of step 0:01:17.335012\n",
      "train_loss 0.726 val_loss 7.480 val_auc_score 0.482\n",
      "----End of step 0:01:16.325271\n",
      "train_loss 0.268 val_loss 7.859 val_auc_score 0.583\n",
      "----End of step 0:01:14.777889\n",
      "train_loss 0.253 val_loss 6.597 val_auc_score 0.448\n",
      "----End of step 0:01:16.142645\n",
      "train_loss 0.525 val_loss 4.252 val_auc_score 0.451\n",
      "----End of step 0:01:13.983822\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 0.200\n",
      "train_loss 0.030 val_loss 25.990 val_auc_score 0.536\n",
      "----End of step 0:01:15.669191\n",
      "train_loss 0.160 val_loss 50.478 val_auc_score 0.573\n",
      "----End of step 0:01:15.366344\n",
      "train_loss 0.269 val_loss 41.565 val_auc_score 0.514\n",
      "----End of step 0:01:12.682133\n",
      "train_loss 0.691 val_loss 93.910 val_auc_score 0.478\n",
      "----End of step 0:01:15.815964\n",
      "train_loss 1.566 val_loss 214.681 val_auc_score 0.505\n",
      "----End of step 0:01:12.029195\n",
      "train_loss 3.457 val_loss 408.485 val_auc_score 0.365\n",
      "----End of step 0:01:10.043574\n",
      "train_loss 4.269 val_loss 220.023 val_auc_score 0.472\n",
      "----End of step 0:01:09.053133\n",
      "train_loss 2.807 val_loss 351.162 val_auc_score 0.540\n",
      "----End of step 0:01:08.791731\n",
      "train_loss 3.174 val_loss 71.229 val_auc_score 0.602\n",
      "----End of step 0:01:08.498074\n",
      "train_loss 1.476 val_loss 45.083 val_auc_score 0.499\n",
      "----End of step 0:01:08.584721\n",
      "train_loss 0.950 val_loss 21.958 val_auc_score 0.372\n",
      "----End of step 0:01:07.723660\n",
      "train_loss 1.005 val_loss 60.266 val_auc_score 0.551\n",
      "----End of step 0:01:08.239888\n",
      "train_loss 1.325 val_loss 27.889 val_auc_score 0.548\n",
      "----End of step 0:01:10.148661\n",
      "train_loss 0.766 val_loss 11.804 val_auc_score 0.565\n",
      "----End of step 0:01:15.665981\n",
      "train_loss 0.840 val_loss 7.273 val_auc_score 0.557\n",
      "----End of step 0:01:13.843748\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 1.000\n",
      "train_loss 0.028 val_loss 22.956 val_auc_score 0.542\n",
      "----End of step 0:01:47.029488\n",
      "train_loss 0.202 val_loss 11.351 val_auc_score 0.456\n",
      "----End of step 0:01:48.191722\n",
      "train_loss 0.129 val_loss 102.739 val_auc_score 0.630\n",
      "----End of step 0:01:49.090242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 1.053 val_loss 639.074 val_auc_score 0.478\n",
      "----End of step 0:01:49.567580\n",
      "train_loss 9.376 val_loss 655.319 val_auc_score 0.423\n",
      "----End of step 0:01:48.031294\n",
      "train_loss 7.332 val_loss 292.624 val_auc_score 0.625\n",
      "----End of step 0:01:47.186915\n",
      "train_loss 4.332 val_loss 299.651 val_auc_score 0.465\n",
      "----End of step 0:01:47.670195\n",
      "train_loss 2.645 val_loss 359.130 val_auc_score 0.526\n",
      "----End of step 0:01:43.098541\n",
      "train_loss 3.799 val_loss 157.159 val_auc_score 0.433\n",
      "----End of step 0:01:43.140610\n",
      "train_loss 1.468 val_loss 218.022 val_auc_score 0.415\n",
      "----End of step 0:01:40.268905\n",
      "train_loss 3.122 val_loss 74.101 val_auc_score 0.580\n",
      "----End of step 0:01:43.180731\n",
      "train_loss 1.347 val_loss 57.686 val_auc_score 0.449\n",
      "----End of step 0:01:40.294678\n",
      "train_loss 1.263 val_loss 29.172 val_auc_score 0.588\n",
      "----End of step 0:01:40.365134\n",
      "train_loss 0.897 val_loss 15.073 val_auc_score 0.584\n",
      "----End of step 0:01:40.936362\n",
      "train_loss 1.286 val_loss 7.771 val_auc_score 0.541\n",
      "----End of step 0:01:41.540278\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.700\n",
      "train_loss 0.029 val_loss 15.793 val_auc_score 0.519\n",
      "----End of step 0:01:38.084268\n",
      "train_loss 0.172 val_loss 53.139 val_auc_score 0.398\n",
      "----End of step 0:01:37.497854\n",
      "train_loss 0.395 val_loss 154.343 val_auc_score 0.423\n",
      "----End of step 0:01:38.170698\n",
      "train_loss 0.898 val_loss 231.280 val_auc_score 0.493\n",
      "----End of step 0:01:36.458750\n",
      "train_loss 3.564 val_loss 534.995 val_auc_score 0.474\n",
      "----End of step 0:01:37.376888\n",
      "train_loss 4.688 val_loss 331.381 val_auc_score 0.479\n",
      "----End of step 0:01:39.034106\n",
      "train_loss 3.249 val_loss 253.592 val_auc_score 0.702\n",
      "----End of step 0:01:35.437364\n",
      "train_loss 4.627 val_loss 302.279 val_auc_score 0.501\n",
      "----End of step 0:01:37.509605\n",
      "train_loss 3.257 val_loss 83.875 val_auc_score 0.465\n",
      "----End of step 0:01:37.406832\n",
      "train_loss 1.053 val_loss 80.569 val_auc_score 0.521\n",
      "----End of step 0:01:36.853788\n",
      "train_loss 1.139 val_loss 60.463 val_auc_score 0.587\n",
      "----End of step 0:01:39.521812\n",
      "train_loss 0.842 val_loss 27.934 val_auc_score 0.529\n",
      "----End of step 0:01:44.756108\n",
      "train_loss 0.496 val_loss 15.340 val_auc_score 0.527\n",
      "----End of step 0:01:46.653651\n",
      "train_loss 0.441 val_loss 8.106 val_auc_score 0.607\n",
      "----End of step 0:01:46.047049\n",
      "train_loss 0.446 val_loss 5.027 val_auc_score 0.638\n",
      "----End of step 0:01:45.833327\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.600\n",
      "train_loss 0.030 val_loss 13.585 val_auc_score 0.580\n",
      "----End of step 0:01:31.241150\n",
      "train_loss 0.169 val_loss 28.062 val_auc_score 0.435\n",
      "----End of step 0:01:34.534032\n",
      "train_loss 0.547 val_loss 465.862 val_auc_score 0.473\n",
      "----End of step 0:01:34.640129\n",
      "train_loss 4.083 val_loss 257.524 val_auc_score 0.364\n",
      "----End of step 0:01:34.126529\n",
      "train_loss 3.066 val_loss 201.509 val_auc_score 0.388\n",
      "----End of step 0:01:30.770175\n",
      "train_loss 2.626 val_loss 267.986 val_auc_score 0.444\n",
      "----End of step 0:01:27.432816\n",
      "train_loss 2.907 val_loss 224.164 val_auc_score 0.414\n",
      "----End of step 0:01:28.363092\n",
      "train_loss 2.384 val_loss 156.379 val_auc_score 0.578\n",
      "----End of step 0:01:26.907587\n",
      "train_loss 2.173 val_loss 129.057 val_auc_score 0.576\n",
      "----End of step 0:01:28.550889\n",
      "train_loss 2.424 val_loss 74.307 val_auc_score 0.541\n",
      "----End of step 0:01:27.524662\n",
      "train_loss 1.271 val_loss 22.166 val_auc_score 0.369\n",
      "----End of step 0:01:27.805154\n",
      "train_loss 0.539 val_loss 11.158 val_auc_score 0.504\n",
      "----End of step 0:01:28.304589\n",
      "train_loss 0.292 val_loss 5.169 val_auc_score 0.441\n",
      "----End of step 0:01:28.815336\n",
      "train_loss 0.273 val_loss 7.600 val_auc_score 0.691\n",
      "----End of step 0:01:27.054778\n",
      "train_loss 0.422 val_loss 2.916 val_auc_score 0.707\n",
      "----End of step 0:01:28.645701\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.500\n",
      "train_loss 0.029 val_loss 53.138 val_auc_score 0.340\n",
      "----End of step 0:01:19.953099\n",
      "train_loss 0.260 val_loss 25.982 val_auc_score 0.576\n",
      "----End of step 0:01:19.332978\n",
      "train_loss 0.215 val_loss 124.321 val_auc_score 0.513\n",
      "----End of step 0:01:20.094318\n",
      "train_loss 0.976 val_loss 196.939 val_auc_score 0.437\n",
      "----End of step 0:01:19.710337\n",
      "train_loss 3.080 val_loss 1085.795 val_auc_score 0.419\n",
      "----End of step 0:01:20.257002\n",
      "train_loss 9.097 val_loss 275.344 val_auc_score 0.473\n",
      "----End of step 0:01:20.616463\n",
      "train_loss 3.409 val_loss 305.671 val_auc_score 0.333\n",
      "----End of step 0:01:20.697424\n",
      "train_loss 3.635 val_loss 326.168 val_auc_score 0.419\n",
      "----End of step 0:01:21.031986\n",
      "train_loss 5.152 val_loss 111.405 val_auc_score 0.492\n",
      "----End of step 0:01:20.975181\n",
      "train_loss 2.012 val_loss 116.171 val_auc_score 0.688\n",
      "----End of step 0:01:21.052140\n",
      "train_loss 2.093 val_loss 88.956 val_auc_score 0.434\n",
      "----End of step 0:01:20.515966\n",
      "train_loss 1.455 val_loss 62.254 val_auc_score 0.573\n",
      "----End of step 0:01:21.598832\n",
      "train_loss 1.224 val_loss 43.055 val_auc_score 0.533\n",
      "----End of step 0:01:21.334531\n",
      "train_loss 2.208 val_loss 32.286 val_auc_score 0.477\n",
      "----End of step 0:01:20.340470\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.300\n",
      "train_loss 0.028 val_loss 52.982 val_auc_score 0.492\n",
      "----End of step 0:01:09.976118\n",
      "train_loss 0.272 val_loss 80.074 val_auc_score 0.671\n",
      "----End of step 0:01:10.792325\n",
      "train_loss 0.692 val_loss 198.919 val_auc_score 0.461\n",
      "----End of step 0:01:10.808431\n",
      "train_loss 2.317 val_loss 413.675 val_auc_score 0.441\n",
      "----End of step 0:01:10.943594\n",
      "train_loss 3.511 val_loss 283.180 val_auc_score 0.461\n",
      "----End of step 0:01:09.810454\n",
      "train_loss 3.709 val_loss 245.829 val_auc_score 0.475\n",
      "----End of step 0:01:11.083594\n",
      "train_loss 2.451 val_loss 361.877 val_auc_score 0.342\n",
      "----End of step 0:01:09.649282\n",
      "train_loss 3.675 val_loss 143.036 val_auc_score 0.452\n",
      "----End of step 0:01:09.931279\n",
      "train_loss 2.588 val_loss 139.311 val_auc_score 0.454\n",
      "----End of step 0:01:10.386439\n",
      "train_loss 1.927 val_loss 41.891 val_auc_score 0.530\n",
      "----End of step 0:01:08.333395\n",
      "train_loss 0.750 val_loss 37.479 val_auc_score 0.415\n",
      "----End of step 0:01:10.666993\n",
      "train_loss 0.905 val_loss 27.221 val_auc_score 0.661\n",
      "----End of step 0:01:10.808247\n",
      "train_loss 0.573 val_loss 12.413 val_auc_score 0.516\n",
      "----End of step 0:01:10.710497\n",
      "train_loss 0.321 val_loss 7.398 val_auc_score 0.619\n",
      "----End of step 0:01:09.668252\n",
      "train_loss 0.453 val_loss 4.195 val_auc_score 0.508\n",
      "----End of step 0:01:10.746390\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 0.200\n",
      "train_loss 0.026 val_loss 40.360 val_auc_score 0.462\n",
      "----End of step 0:01:07.425588\n",
      "train_loss 0.235 val_loss 72.763 val_auc_score 0.529\n",
      "----End of step 0:01:06.606291\n",
      "train_loss 0.708 val_loss 203.772 val_auc_score 0.511\n",
      "----End of step 0:01:07.358185\n",
      "train_loss 2.298 val_loss 339.438 val_auc_score 0.469\n",
      "----End of step 0:01:06.506762\n",
      "train_loss 3.917 val_loss 221.701 val_auc_score 0.523\n",
      "----End of step 0:01:05.730909\n",
      "train_loss 4.821 val_loss 243.644 val_auc_score 0.629\n",
      "----End of step 0:01:05.684626\n",
      "train_loss 4.328 val_loss 251.684 val_auc_score 0.655\n",
      "----End of step 0:01:05.498229\n",
      "train_loss 4.166 val_loss 277.620 val_auc_score 0.340\n",
      "----End of step 0:01:07.070632\n",
      "train_loss 3.698 val_loss 105.555 val_auc_score 0.560\n",
      "----End of step 0:01:06.854233\n",
      "train_loss 1.843 val_loss 117.692 val_auc_score 0.488\n",
      "----End of step 0:01:06.521631\n",
      "train_loss 1.399 val_loss 23.207 val_auc_score 0.478\n",
      "----End of step 0:01:05.766840\n",
      "train_loss 0.428 val_loss 9.713 val_auc_score 0.502\n",
      "----End of step 0:01:04.687368\n",
      "train_loss 0.274 val_loss 25.047 val_auc_score 0.604\n",
      "----End of step 0:01:05.558669\n",
      "train_loss 0.563 val_loss 5.436 val_auc_score 0.474\n",
      "----End of step 0:01:07.621734\n",
      "train_loss 0.494 val_loss 3.147 val_auc_score 0.479\n",
      "----End of step 0:01:07.240004\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 1.000\n",
      "train_loss 0.029 val_loss 27.485 val_auc_score 0.401\n",
      "----End of step 0:01:39.949613\n",
      "train_loss 0.166 val_loss 47.630 val_auc_score 0.522\n",
      "----End of step 0:01:45.048053\n",
      "train_loss 0.167 val_loss 81.517 val_auc_score 0.416\n",
      "----End of step 0:01:45.424511\n",
      "train_loss 0.829 val_loss 238.191 val_auc_score 0.461\n",
      "----End of step 0:01:46.857439\n",
      "train_loss 6.371 val_loss 420.080 val_auc_score 0.432\n",
      "----End of step 0:01:45.129434\n",
      "train_loss 9.212 val_loss 959.077 val_auc_score 0.474\n",
      "----End of step 0:01:46.669791\n",
      "train_loss 10.611 val_loss 613.651 val_auc_score 0.498\n",
      "----End of step 0:01:46.283717\n",
      "train_loss 7.028 val_loss 572.361 val_auc_score 0.372\n",
      "----End of step 0:01:45.958953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 5.737 val_loss 343.018 val_auc_score 0.510\n",
      "----End of step 0:01:47.142261\n",
      "train_loss 4.428 val_loss 108.002 val_auc_score 0.422\n",
      "----End of step 0:01:49.708538\n",
      "train_loss 1.650 val_loss 36.747 val_auc_score 0.591\n",
      "----End of step 0:01:45.705275\n",
      "train_loss 0.462 val_loss 36.206 val_auc_score 0.576\n",
      "----End of step 0:01:47.101655\n",
      "train_loss 0.703 val_loss 73.276 val_auc_score 0.552\n",
      "----End of step 0:01:46.404443\n",
      "train_loss 1.793 val_loss 35.096 val_auc_score 0.458\n",
      "----End of step 0:01:45.779233\n",
      "train_loss 2.130 val_loss 20.593 val_auc_score 0.561\n",
      "----End of step 0:01:47.679679\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.700\n",
      "train_loss 0.029 val_loss 46.415 val_auc_score 0.435\n",
      "----End of step 0:01:43.959100\n",
      "train_loss 0.249 val_loss 13.199 val_auc_score 0.410\n",
      "----End of step 0:01:40.283061\n",
      "train_loss 0.146 val_loss 191.140 val_auc_score 0.346\n",
      "----End of step 0:01:46.531262\n",
      "train_loss 1.687 val_loss 148.454 val_auc_score 0.457\n",
      "----End of step 0:01:42.617331\n",
      "train_loss 1.009 val_loss 429.076 val_auc_score 0.499\n",
      "----End of step 0:01:39.221613\n",
      "train_loss 4.253 val_loss 311.761 val_auc_score 0.480\n",
      "----End of step 0:01:41.273954\n",
      "train_loss 2.559 val_loss 501.562 val_auc_score 0.450\n",
      "----End of step 0:01:41.202989\n",
      "train_loss 3.381 val_loss 441.310 val_auc_score 0.491\n",
      "----End of step 0:01:40.811542\n",
      "train_loss 3.162 val_loss 158.078 val_auc_score 0.530\n",
      "----End of step 0:01:44.371699\n",
      "train_loss 1.772 val_loss 143.776 val_auc_score 0.442\n",
      "----End of step 0:01:41.404458\n",
      "train_loss 1.748 val_loss 19.768 val_auc_score 0.656\n",
      "----End of step 0:01:38.510154\n",
      "train_loss 0.491 val_loss 20.020 val_auc_score 0.391\n",
      "----End of step 0:01:35.084811\n",
      "train_loss 0.525 val_loss 24.341 val_auc_score 0.525\n",
      "----End of step 0:01:35.324033\n",
      "train_loss 0.591 val_loss 12.225 val_auc_score 0.680\n",
      "----End of step 0:01:38.115965\n",
      "train_loss 0.801 val_loss 6.840 val_auc_score 0.478\n",
      "----End of step 0:01:36.825841\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.600\n",
      "train_loss 0.027 val_loss 24.317 val_auc_score 0.506\n",
      "----End of step 0:01:29.787970\n",
      "train_loss 0.200 val_loss 25.434 val_auc_score 0.429\n",
      "----End of step 0:01:27.074497\n",
      "train_loss 0.470 val_loss 296.727 val_auc_score 0.437\n",
      "----End of step 0:01:27.284932\n",
      "train_loss 2.914 val_loss 1127.898 val_auc_score 0.431\n",
      "----End of step 0:01:28.421647\n",
      "train_loss 9.249 val_loss 536.464 val_auc_score 0.495\n",
      "----End of step 0:01:27.688448\n",
      "train_loss 5.490 val_loss 470.187 val_auc_score 0.588\n",
      "----End of step 0:01:28.557758\n",
      "train_loss 6.581 val_loss 237.169 val_auc_score 0.570\n",
      "----End of step 0:01:27.627067\n",
      "train_loss 3.485 val_loss 229.385 val_auc_score 0.643\n",
      "----End of step 0:01:27.059780\n",
      "train_loss 3.461 val_loss 161.026 val_auc_score 0.580\n",
      "----End of step 0:01:28.395702\n",
      "train_loss 1.858 val_loss 90.912 val_auc_score 0.585\n",
      "----End of step 0:01:26.912410\n",
      "train_loss 1.062 val_loss 72.635 val_auc_score 0.418\n",
      "----End of step 0:01:27.772679\n",
      "train_loss 1.426 val_loss 40.904 val_auc_score 0.570\n",
      "----End of step 0:01:26.329130\n",
      "train_loss 0.905 val_loss 36.280 val_auc_score 0.362\n",
      "----End of step 0:01:26.599975\n",
      "train_loss 0.811 val_loss 15.094 val_auc_score 0.614\n",
      "----End of step 0:01:27.577724\n",
      "train_loss 0.691 val_loss 7.312 val_auc_score 0.383\n",
      "----End of step 0:01:27.368697\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.500\n",
      "train_loss 0.029 val_loss 49.598 val_auc_score 0.441\n",
      "----End of step 0:01:21.370680\n",
      "train_loss 0.241 val_loss 23.753 val_auc_score 0.568\n",
      "----End of step 0:01:20.506719\n",
      "train_loss 0.258 val_loss 41.592 val_auc_score 0.539\n",
      "----End of step 0:01:20.407315\n",
      "train_loss 0.740 val_loss 126.762 val_auc_score 0.562\n",
      "----End of step 0:01:18.992266\n",
      "train_loss 3.131 val_loss 357.684 val_auc_score 0.387\n",
      "----End of step 0:01:20.422643\n",
      "train_loss 4.413 val_loss 463.039 val_auc_score 0.482\n",
      "----End of step 0:01:19.476120\n",
      "train_loss 4.608 val_loss 427.232 val_auc_score 0.488\n",
      "----End of step 0:01:19.396812\n",
      "train_loss 4.005 val_loss 140.803 val_auc_score 0.537\n",
      "----End of step 0:01:18.508297\n",
      "train_loss 1.911 val_loss 54.966 val_auc_score 0.547\n",
      "----End of step 0:01:19.321908\n",
      "train_loss 1.760 val_loss 55.420 val_auc_score 0.366\n",
      "----End of step 0:01:19.008278\n",
      "train_loss 0.544 val_loss 54.032 val_auc_score 0.457\n",
      "----End of step 0:01:19.836161\n",
      "train_loss 0.644 val_loss 39.076 val_auc_score 0.523\n",
      "----End of step 0:01:20.672670\n",
      "train_loss 0.516 val_loss 19.381 val_auc_score 0.363\n",
      "----End of step 0:01:20.636004\n",
      "train_loss 0.596 val_loss 8.207 val_auc_score 0.488\n",
      "----End of step 0:01:19.586617\n",
      "train_loss 0.715 val_loss 6.583 val_auc_score 0.413\n",
      "----End of step 0:01:20.219898\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.300\n",
      "train_loss 0.028 val_loss 21.032 val_auc_score 0.537\n",
      "----End of step 0:01:11.163766\n",
      "train_loss 0.269 val_loss 10.301 val_auc_score 0.588\n",
      "----End of step 0:01:10.451534\n",
      "train_loss 0.333 val_loss 431.056 val_auc_score 0.462\n",
      "----End of step 0:01:09.158157\n",
      "train_loss 3.704 val_loss 525.581 val_auc_score 0.395\n",
      "----End of step 0:01:10.090659\n",
      "train_loss 3.626 val_loss 745.200 val_auc_score 0.599\n",
      "----End of step 0:01:12.624187\n",
      "train_loss 8.726 val_loss 899.470 val_auc_score 0.583\n",
      "----End of step 0:01:10.793796\n",
      "train_loss 10.485 val_loss 234.426 val_auc_score 0.498\n",
      "----End of step 0:01:10.371474\n",
      "train_loss 3.022 val_loss 357.538 val_auc_score 0.483\n",
      "----End of step 0:01:10.349661\n",
      "train_loss 4.565 val_loss 98.439 val_auc_score 0.437\n",
      "----End of step 0:01:10.532169\n",
      "train_loss 0.965 val_loss 169.719 val_auc_score 0.615\n",
      "----End of step 0:01:10.646668\n",
      "train_loss 2.617 val_loss 63.586 val_auc_score 0.664\n",
      "----End of step 0:01:07.578165\n",
      "train_loss 1.222 val_loss 42.146 val_auc_score 0.448\n",
      "----End of step 0:01:07.136233\n",
      "train_loss 0.612 val_loss 21.417 val_auc_score 0.512\n",
      "----End of step 0:01:06.193192\n",
      "train_loss 0.563 val_loss 13.046 val_auc_score 0.577\n",
      "----End of step 0:01:07.214658\n",
      "train_loss 1.049 val_loss 8.281 val_auc_score 0.541\n",
      "----End of step 0:01:07.904731\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 0.200\n",
      "train_loss 0.031 val_loss 49.688 val_auc_score 0.419\n",
      "----End of step 0:01:04.206623\n",
      "train_loss 0.225 val_loss 37.326 val_auc_score 0.510\n",
      "----End of step 0:01:05.735343\n",
      "train_loss 0.308 val_loss 117.828 val_auc_score 0.549\n",
      "----End of step 0:01:03.473068\n",
      "train_loss 1.429 val_loss 541.381 val_auc_score 0.530\n",
      "----End of step 0:01:03.198366\n",
      "train_loss 6.165 val_loss 704.392 val_auc_score 0.525\n",
      "----End of step 0:01:03.051954\n",
      "train_loss 7.000 val_loss 359.770 val_auc_score 0.463\n",
      "----End of step 0:01:03.632476\n",
      "train_loss 3.086 val_loss 270.086 val_auc_score 0.483\n",
      "----End of step 0:01:05.085601\n",
      "train_loss 1.275 val_loss 142.655 val_auc_score 0.589\n",
      "----End of step 0:01:03.575804\n",
      "train_loss 1.133 val_loss 59.222 val_auc_score 0.478\n",
      "----End of step 0:01:04.304736\n",
      "train_loss 0.486 val_loss 35.819 val_auc_score 0.563\n",
      "----End of step 0:01:04.414750\n",
      "train_loss 0.506 val_loss 28.388 val_auc_score 0.502\n",
      "----End of step 0:01:05.519753\n",
      "train_loss 0.534 val_loss 29.593 val_auc_score 0.458\n",
      "----End of step 0:01:04.499010\n",
      "train_loss 0.470 val_loss 18.404 val_auc_score 0.461\n",
      "----End of step 0:01:04.232520\n",
      "train_loss 0.425 val_loss 8.731 val_auc_score 0.478\n",
      "----End of step 0:01:12.074267\n",
      "train_loss 0.604 val_loss 3.962 val_auc_score 0.448\n",
      "----End of step 0:01:12.051810\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 1.000\n",
      "train_loss 0.031 val_loss 50.824 val_auc_score 0.459\n",
      "----End of step 0:01:38.520222\n",
      "train_loss 0.254 val_loss 22.912 val_auc_score 0.562\n",
      "----End of step 0:01:37.223011\n",
      "train_loss 0.131 val_loss 63.036 val_auc_score 0.413\n",
      "----End of step 0:01:37.170734\n",
      "train_loss 1.012 val_loss 406.349 val_auc_score 0.519\n",
      "----End of step 0:01:36.523181\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for w in widths:\n",
    "    for d in depths:\n",
    "        print('width multiplier - %.3f depth multiplier - %.3f' % (w, d))\n",
    "        model = MobileNet(width_mult=w, depth_mult=d).cuda()\n",
    "        p = sum(p.numel() for p in model.parameters())\n",
    "        optimizer = create_optimizer(model, 0.1)\n",
    "        score, t = train_triangular_policy(model, optimizer, train_loader, valid_loader, valid_dataset,\n",
    "                                           loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                                           dataset='rsna', binary=True, max_lr=0.1, epochs=15)\n",
    "        data.append([w, d, score, p, t])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['width_x', 'depth_x', 'val_score', 'params', 'time_per_epoch']\n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"rsna_mobilenet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.read_csv('rsna_mobilenet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
