{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(4)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../prepare_data.py\n",
    "%run ../../architectures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, valid_dataset = rsna_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 256, 256]), torch.Size([32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11177025"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.3 s, sys: 10.9 s, total: 58.2 s\n",
      "Wall time: 58.6 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lrs, losses = LR_range_finder(model, train_loader, \n",
    "                              loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                              binary=True, lr_high=0.6)\n",
    "plot_lr(lrs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [1.0, 0.75, 0.5, 0.25]\n",
    "depths = [[[[64, 2], [128, 2]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 2], [128, 2]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 1], [512, 1]]],\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width multiplier - 1.000 depth multiplier - 7.000\n",
      "train_loss 0.027 val_loss 31.942 val_auc_score 0.435\n",
      "----End of step 0:01:24.823345\n",
      "train_loss 0.174 val_loss 41.236 val_auc_score 0.421\n",
      "----End of step 0:01:25.186160\n",
      "train_loss 0.457 val_loss 162.258 val_auc_score 0.421\n",
      "----End of step 0:01:26.109637\n",
      "train_loss 1.757 val_loss 592.000 val_auc_score 0.363\n",
      "----End of step 0:01:26.904425\n",
      "train_loss 2.868 val_loss 628.369 val_auc_score 0.396\n",
      "----End of step 0:01:25.992702\n",
      "train_loss 5.038 val_loss 515.223 val_auc_score 0.318\n",
      "----End of step 0:01:25.170020\n",
      "train_loss 3.163 val_loss 331.005 val_auc_score 0.315\n",
      "----End of step 0:01:25.606591\n",
      "train_loss 1.697 val_loss 172.171 val_auc_score 0.383\n",
      "----End of step 0:01:25.658898\n",
      "train_loss 1.308 val_loss 159.366 val_auc_score 0.399\n",
      "----End of step 0:01:25.694892\n",
      "train_loss 0.765 val_loss 2433.362 val_auc_score 0.424\n",
      "----End of step 0:01:26.092601\n",
      "train_loss 0.798 val_loss 179.396 val_auc_score 0.367\n",
      "----End of step 0:01:26.319826\n",
      "train_loss 0.371 val_loss 120.928 val_auc_score 0.395\n",
      "----End of step 0:01:26.253394\n",
      "train_loss 0.107 val_loss 159.547 val_auc_score 0.454\n",
      "----End of step 0:01:25.943759\n",
      "train_loss 0.311 val_loss 7.450 val_auc_score 0.589\n",
      "----End of step 0:01:26.199048\n",
      "train_loss 0.256 val_loss 40.418 val_auc_score 0.492\n",
      "----End of step 0:01:26.229161\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.025 val_loss 40.560 val_auc_score 0.411\n",
      "----End of step 0:01:23.636071\n",
      "train_loss 0.185 val_loss 69.704 val_auc_score 0.427\n",
      "----End of step 0:01:23.588668\n",
      "train_loss 0.524 val_loss 64.994 val_auc_score 0.452\n",
      "----End of step 0:01:23.433212\n",
      "train_loss 0.995 val_loss 592.369 val_auc_score 0.351\n",
      "----End of step 0:01:22.971333\n",
      "train_loss 3.805 val_loss 900.247 val_auc_score 0.345\n",
      "----End of step 0:01:22.887213\n",
      "train_loss 3.427 val_loss 381.992 val_auc_score 0.419\n",
      "----End of step 0:01:22.901298\n",
      "train_loss 3.486 val_loss 176.525 val_auc_score 0.442\n",
      "----End of step 0:01:23.946797\n",
      "train_loss 1.467 val_loss 267.562 val_auc_score 0.427\n",
      "----End of step 0:01:23.277643\n",
      "train_loss 1.421 val_loss 206.583 val_auc_score 0.429\n",
      "----End of step 0:01:22.678046\n",
      "train_loss 0.919 val_loss 124.341 val_auc_score 0.420\n",
      "----End of step 0:01:22.775961\n",
      "train_loss 0.729 val_loss 108.947 val_auc_score 0.401\n",
      "----End of step 0:01:22.806947\n",
      "train_loss 0.448 val_loss 91.063 val_auc_score 0.438\n",
      "----End of step 0:01:23.130453\n",
      "train_loss 0.307 val_loss 82.089 val_auc_score 0.518\n",
      "----End of step 0:01:23.157640\n",
      "train_loss 0.345 val_loss 42.997 val_auc_score 0.484\n",
      "----End of step 0:01:23.036641\n",
      "train_loss 0.344 val_loss 17.939 val_auc_score 0.539\n",
      "----End of step 0:01:22.934795\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.030 val_loss 47.184 val_auc_score 0.359\n",
      "----End of step 0:01:19.366220\n",
      "train_loss 0.126 val_loss 23.196 val_auc_score 0.400\n",
      "----End of step 0:01:19.303444\n",
      "train_loss 0.092 val_loss 87.846 val_auc_score 0.475\n",
      "----End of step 0:01:19.525918\n",
      "train_loss 2.294 val_loss 798.198 val_auc_score 0.382\n",
      "----End of step 0:01:20.081075\n",
      "train_loss 10.226 val_loss 770.086 val_auc_score 0.622\n",
      "----End of step 0:01:19.256598\n",
      "train_loss 11.801 val_loss 955.734 val_auc_score 0.346\n",
      "----End of step 0:01:19.312431\n",
      "train_loss 6.779 val_loss 368.161 val_auc_score 0.373\n",
      "----End of step 0:01:19.388059\n",
      "train_loss 3.085 val_loss 217.897 val_auc_score 0.360\n",
      "----End of step 0:01:19.640245\n",
      "train_loss 1.556 val_loss 334.318 val_auc_score 0.355\n",
      "----End of step 0:01:19.678178\n",
      "train_loss 0.983 val_loss 82.147 val_auc_score 0.580\n",
      "----End of step 0:01:19.009305\n",
      "train_loss 0.458 val_loss 53.248 val_auc_score 0.544\n",
      "----End of step 0:01:18.990064\n",
      "train_loss 0.279 val_loss 31.470 val_auc_score 0.462\n",
      "----End of step 0:01:19.186362\n",
      "train_loss 0.353 val_loss 18.994 val_auc_score 0.435\n",
      "----End of step 0:01:19.608628\n",
      "train_loss 0.298 val_loss 13.328 val_auc_score 0.484\n",
      "----End of step 0:01:19.806294\n",
      "train_loss 0.224 val_loss 3.533 val_auc_score 0.483\n",
      "----End of step 0:01:19.373429\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.026 val_loss 46.902 val_auc_score 0.315\n",
      "----End of step 0:01:22.331503\n",
      "train_loss 0.173 val_loss 36.711 val_auc_score 0.723\n",
      "----End of step 0:01:22.314854\n",
      "train_loss 0.368 val_loss 162.416 val_auc_score 0.344\n",
      "----End of step 0:01:21.963410\n",
      "train_loss 3.776 val_loss 418.345 val_auc_score 0.534\n",
      "----End of step 0:01:22.004174\n",
      "train_loss 8.210 val_loss 1102.670 val_auc_score 0.413\n",
      "----End of step 0:01:21.560020\n",
      "train_loss 5.311 val_loss 1002.862 val_auc_score 0.410\n",
      "----End of step 0:01:21.585263\n",
      "train_loss 2.020 val_loss 237.817 val_auc_score 0.457\n",
      "----End of step 0:01:21.632988\n",
      "train_loss 1.969 val_loss 126.453 val_auc_score 0.425\n",
      "----End of step 0:01:21.779616\n",
      "train_loss 0.979 val_loss 136.095 val_auc_score 0.492\n",
      "----End of step 0:01:22.411863\n",
      "train_loss 0.680 val_loss 90.712 val_auc_score 0.410\n",
      "----End of step 0:01:21.889655\n",
      "train_loss 0.398 val_loss 76.706 val_auc_score 0.385\n",
      "----End of step 0:01:21.935332\n",
      "train_loss 0.372 val_loss 70.735 val_auc_score 0.425\n",
      "----End of step 0:01:22.161650\n",
      "train_loss 0.241 val_loss 67.161 val_auc_score 0.464\n",
      "----End of step 0:01:21.860835\n",
      "train_loss 0.146 val_loss 74.361 val_auc_score 0.339\n",
      "----End of step 0:01:22.526602\n",
      "train_loss 0.356 val_loss 4.582 val_auc_score 0.471\n",
      "----End of step 0:01:23.739263\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.032 val_loss 32.126 val_auc_score 0.352\n",
      "----End of step 0:01:12.731589\n",
      "train_loss 0.150 val_loss 45.247 val_auc_score 0.482\n",
      "----End of step 0:01:12.646809\n",
      "train_loss 0.304 val_loss 107.682 val_auc_score 0.382\n",
      "----End of step 0:01:13.582234\n",
      "train_loss 2.090 val_loss 675.181 val_auc_score 0.375\n",
      "----End of step 0:01:13.745401\n",
      "train_loss 4.390 val_loss 272.261 val_auc_score 0.547\n",
      "----End of step 0:01:13.333866\n",
      "train_loss 3.789 val_loss 370.086 val_auc_score 0.340\n",
      "----End of step 0:01:13.564102\n",
      "train_loss 2.918 val_loss 502.586 val_auc_score 0.343\n",
      "----End of step 0:01:13.912366\n",
      "train_loss 2.390 val_loss 600.234 val_auc_score 0.397\n",
      "----End of step 0:01:13.584607\n",
      "train_loss 2.340 val_loss 140.899 val_auc_score 0.321\n",
      "----End of step 0:01:14.364687\n",
      "train_loss 0.911 val_loss 234.804 val_auc_score 0.428\n",
      "----End of step 0:01:14.307734\n",
      "train_loss 0.403 val_loss 32.899 val_auc_score 0.512\n",
      "----End of step 0:01:12.354056\n",
      "train_loss 0.355 val_loss 68.208 val_auc_score 0.399\n",
      "----End of step 0:01:13.090596\n",
      "train_loss 0.256 val_loss 56.774 val_auc_score 0.447\n",
      "----End of step 0:01:15.554091\n",
      "train_loss 0.265 val_loss 6.849 val_auc_score 0.457\n",
      "----End of step 0:01:14.531598\n",
      "train_loss 0.241 val_loss 3.105 val_auc_score 0.485\n",
      "----End of step 0:01:14.747143\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 4.000\n",
      "train_loss 0.028 val_loss 28.520 val_auc_score 0.545\n",
      "----End of step 0:01:12.508206\n",
      "train_loss 0.149 val_loss 80.474 val_auc_score 0.442\n",
      "----End of step 0:01:12.878613\n",
      "train_loss 0.447 val_loss 226.665 val_auc_score 0.574\n",
      "----End of step 0:01:13.674092\n",
      "train_loss 2.198 val_loss 422.455 val_auc_score 0.581\n",
      "----End of step 0:01:13.911514\n",
      "train_loss 6.546 val_loss 435.658 val_auc_score 0.559\n",
      "----End of step 0:01:14.057231\n",
      "train_loss 7.405 val_loss 282.671 val_auc_score 0.599\n",
      "----End of step 0:01:14.894740\n",
      "train_loss 3.255 val_loss 126.317 val_auc_score 0.640\n",
      "----End of step 0:01:14.278581\n",
      "train_loss 2.794 val_loss 85.316 val_auc_score 0.622\n",
      "----End of step 0:01:15.776444\n",
      "train_loss 1.798 val_loss 46.087 val_auc_score 0.684\n",
      "----End of step 0:01:16.660899\n",
      "train_loss 0.757 val_loss 33.829 val_auc_score 0.669\n",
      "----End of step 0:01:15.682789\n",
      "train_loss 0.757 val_loss 42.855 val_auc_score 0.617\n",
      "----End of step 0:01:16.532722\n",
      "train_loss 0.503 val_loss 6.969 val_auc_score 0.663\n",
      "----End of step 0:01:16.420332\n",
      "train_loss 0.280 val_loss 6.208 val_auc_score 0.551\n",
      "----End of step 0:01:15.287136\n",
      "train_loss 0.305 val_loss 3.170 val_auc_score 0.663\n",
      "----End of step 0:01:14.930572\n",
      "train_loss 0.308 val_loss 2.596 val_auc_score 0.633\n",
      "----End of step 0:01:16.257139\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 7.000\n",
      "train_loss 0.024 val_loss 15.370 val_auc_score 0.556\n",
      "----End of step 0:01:19.075707\n",
      "train_loss 0.121 val_loss 37.323 val_auc_score 0.710\n",
      "----End of step 0:01:20.844043\n",
      "train_loss 0.250 val_loss 59.090 val_auc_score 0.481\n",
      "----End of step 0:01:22.902289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.828 val_loss 119.223 val_auc_score 0.601\n",
      "----End of step 0:01:19.393838\n",
      "train_loss 2.759 val_loss 418.204 val_auc_score 0.309\n",
      "----End of step 0:01:19.955677\n",
      "train_loss 3.320 val_loss 398.376 val_auc_score 0.380\n",
      "----End of step 0:01:18.398295\n",
      "train_loss 2.645 val_loss 323.529 val_auc_score 0.374\n",
      "----End of step 0:01:18.571757\n",
      "train_loss 1.258 val_loss 18.580 val_auc_score 0.480\n",
      "----End of step 0:01:18.540823\n",
      "train_loss 0.347 val_loss 72.087 val_auc_score 0.430\n",
      "----End of step 0:01:15.730354\n",
      "train_loss 0.704 val_loss 19.446 val_auc_score 0.727\n",
      "----End of step 0:01:15.498488\n",
      "train_loss 0.203 val_loss 10.198 val_auc_score 0.663\n",
      "----End of step 0:01:14.949137\n",
      "train_loss 0.176 val_loss 8.906 val_auc_score 0.553\n",
      "----End of step 0:01:15.063043\n",
      "train_loss 0.120 val_loss 5.425 val_auc_score 0.626\n",
      "----End of step 0:01:15.004644\n",
      "train_loss 0.127 val_loss 4.351 val_auc_score 0.640\n",
      "----End of step 0:01:16.304456\n",
      "train_loss 0.143 val_loss 3.876 val_auc_score 0.546\n",
      "----End of step 0:01:19.269040\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.022 val_loss 19.388 val_auc_score 0.551\n",
      "----End of step 0:01:18.375479\n",
      "train_loss 0.118 val_loss 54.030 val_auc_score 0.574\n",
      "----End of step 0:01:16.620251\n",
      "train_loss 0.376 val_loss 96.146 val_auc_score 0.405\n",
      "----End of step 0:01:17.068397\n",
      "train_loss 1.381 val_loss 185.405 val_auc_score 0.682\n",
      "----End of step 0:01:16.011435\n",
      "train_loss 1.883 val_loss 96.375 val_auc_score 0.690\n",
      "----End of step 0:01:17.356617\n",
      "train_loss 1.915 val_loss 342.744 val_auc_score 0.435\n",
      "----End of step 0:01:18.634543\n",
      "train_loss 4.431 val_loss 114.608 val_auc_score 0.479\n",
      "----End of step 0:01:15.807722\n",
      "train_loss 3.200 val_loss 88.642 val_auc_score 0.636\n",
      "----End of step 0:01:15.080245\n",
      "train_loss 1.896 val_loss 46.691 val_auc_score 0.471\n",
      "----End of step 0:01:17.775874\n",
      "train_loss 0.695 val_loss 8.006 val_auc_score 0.451\n",
      "----End of step 0:01:17.707848\n",
      "train_loss 0.267 val_loss 10.132 val_auc_score 0.494\n",
      "----End of step 0:01:16.525970\n",
      "train_loss 0.217 val_loss 7.568 val_auc_score 0.481\n",
      "----End of step 0:01:13.410967\n",
      "train_loss 0.230 val_loss 5.376 val_auc_score 0.606\n",
      "----End of step 0:01:12.464871\n",
      "train_loss 0.286 val_loss 3.307 val_auc_score 0.592\n",
      "----End of step 0:01:13.316998\n",
      "train_loss 0.427 val_loss 1.955 val_auc_score 0.622\n",
      "----End of step 0:01:12.981587\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.022 val_loss 22.789 val_auc_score 0.474\n",
      "----End of step 0:01:10.366563\n",
      "train_loss 0.140 val_loss 36.567 val_auc_score 0.636\n",
      "----End of step 0:01:10.946476\n",
      "train_loss 0.387 val_loss 193.253 val_auc_score 0.331\n",
      "----End of step 0:01:10.621248\n",
      "train_loss 1.687 val_loss 428.994 val_auc_score 0.460\n",
      "----End of step 0:01:09.847615\n",
      "train_loss 5.888 val_loss 238.145 val_auc_score 0.372\n",
      "----End of step 0:01:10.980555\n",
      "train_loss 4.072 val_loss 276.279 val_auc_score 0.367\n",
      "----End of step 0:01:10.930779\n",
      "train_loss 1.563 val_loss 169.742 val_auc_score 0.437\n",
      "----End of step 0:01:10.309901\n",
      "train_loss 1.729 val_loss 144.653 val_auc_score 0.444\n",
      "----End of step 0:01:10.334072\n",
      "train_loss 0.711 val_loss 40.045 val_auc_score 0.445\n",
      "----End of step 0:01:10.353480\n",
      "train_loss 0.385 val_loss 27.562 val_auc_score 0.433\n",
      "----End of step 0:01:09.690834\n",
      "train_loss 0.264 val_loss 20.422 val_auc_score 0.438\n",
      "----End of step 0:01:11.238061\n",
      "train_loss 0.233 val_loss 11.023 val_auc_score 0.442\n",
      "----End of step 0:01:09.711231\n",
      "train_loss 0.180 val_loss 5.826 val_auc_score 0.458\n",
      "----End of step 0:01:09.218484\n",
      "train_loss 0.148 val_loss 5.437 val_auc_score 0.534\n",
      "----End of step 0:01:09.240013\n",
      "train_loss 0.184 val_loss 2.830 val_auc_score 0.494\n",
      "----End of step 0:01:09.780886\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.022 val_loss 39.831 val_auc_score 0.333\n",
      "----End of step 0:01:12.048293\n",
      "train_loss 0.134 val_loss 72.137 val_auc_score 0.524\n",
      "----End of step 0:01:11.648230\n",
      "train_loss 0.450 val_loss 70.258 val_auc_score 0.687\n",
      "----End of step 0:01:11.846827\n",
      "train_loss 0.980 val_loss 172.587 val_auc_score 0.478\n",
      "----End of step 0:01:12.544561\n",
      "train_loss 2.649 val_loss 699.048 val_auc_score 0.413\n",
      "----End of step 0:01:12.272201\n",
      "train_loss 3.621 val_loss 1068.509 val_auc_score 0.458\n",
      "----End of step 0:01:13.003754\n",
      "train_loss 2.844 val_loss 168.104 val_auc_score 0.404\n",
      "----End of step 0:01:11.606435\n",
      "train_loss 0.540 val_loss 62.061 val_auc_score 0.344\n",
      "----End of step 0:01:15.920312\n",
      "train_loss 0.278 val_loss 27.589 val_auc_score 0.447\n",
      "----End of step 0:01:18.794178\n",
      "train_loss 0.126 val_loss 29.919 val_auc_score 0.497\n",
      "----End of step 0:01:14.645535\n",
      "train_loss 0.334 val_loss 14.892 val_auc_score 0.453\n",
      "----End of step 0:01:15.446095\n",
      "train_loss 0.167 val_loss 8.620 val_auc_score 0.500\n",
      "----End of step 0:01:17.181829\n",
      "train_loss 0.193 val_loss 5.780 val_auc_score 0.516\n",
      "----End of step 0:01:15.701833\n",
      "train_loss 0.141 val_loss 3.215 val_auc_score 0.475\n",
      "----End of step 0:01:14.693886\n",
      "train_loss 0.194 val_loss 2.699 val_auc_score 0.517\n",
      "----End of step 0:01:15.139682\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.022 val_loss 16.348 val_auc_score 0.651\n",
      "----End of step 0:01:12.589888\n",
      "train_loss 0.102 val_loss 55.115 val_auc_score 0.614\n",
      "----End of step 0:01:12.709008\n",
      "train_loss 0.321 val_loss 48.822 val_auc_score 0.366\n",
      "----End of step 0:01:09.585061\n",
      "train_loss 0.713 val_loss 601.157 val_auc_score 0.354\n",
      "----End of step 0:01:12.161315\n",
      "train_loss 3.911 val_loss 1047.690 val_auc_score 0.418\n",
      "----End of step 0:01:07.358625\n",
      "train_loss 4.036 val_loss 1225.540 val_auc_score 0.415\n",
      "----End of step 0:01:07.852740\n",
      "train_loss 1.897 val_loss 388.969 val_auc_score 0.394\n",
      "----End of step 0:01:05.140737\n",
      "train_loss 1.719 val_loss 260.359 val_auc_score 0.364\n",
      "----End of step 0:01:05.152903\n",
      "train_loss 1.356 val_loss 42.377 val_auc_score 0.405\n",
      "----End of step 0:01:04.944762\n",
      "train_loss 0.340 val_loss 63.826 val_auc_score 0.386\n",
      "----End of step 0:01:05.506696\n",
      "train_loss 0.293 val_loss 20.573 val_auc_score 0.504\n",
      "----End of step 0:01:04.709456\n",
      "train_loss 0.323 val_loss 9.717 val_auc_score 0.565\n",
      "----End of step 0:01:05.649297\n",
      "train_loss 0.155 val_loss 7.154 val_auc_score 0.515\n",
      "----End of step 0:01:05.712487\n",
      "train_loss 0.147 val_loss 5.717 val_auc_score 0.431\n",
      "----End of step 0:01:05.625370\n",
      "train_loss 0.156 val_loss 3.388 val_auc_score 0.559\n",
      "----End of step 0:01:05.544442\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 4.000\n",
      "train_loss 0.023 val_loss 16.768 val_auc_score 0.645\n",
      "----End of step 0:01:04.285056\n",
      "train_loss 0.121 val_loss 48.700 val_auc_score 0.626\n",
      "----End of step 0:01:03.931205\n",
      "train_loss 0.476 val_loss 394.509 val_auc_score 0.598\n",
      "----End of step 0:01:03.585273\n",
      "train_loss 3.251 val_loss 481.580 val_auc_score 0.593\n",
      "----End of step 0:01:03.496011\n",
      "train_loss 6.635 val_loss 518.690 val_auc_score 0.435\n",
      "----End of step 0:01:03.390465\n",
      "train_loss 3.999 val_loss 397.490 val_auc_score 0.434\n",
      "----End of step 0:01:03.454952\n",
      "train_loss 2.337 val_loss 701.982 val_auc_score 0.477\n",
      "----End of step 0:01:04.288315\n",
      "train_loss 1.138 val_loss 412.236 val_auc_score 0.463\n",
      "----End of step 0:01:03.961855\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for w in widths:\n",
    "    for d in depths:\n",
    "        d_s = sum(j[1] for i in d for j in i)\n",
    "        print('width multiplier - %.3f depth multiplier - %.3f' % (w, d_s))\n",
    "        model = resnet18(width_mult=w, \n",
    "                         inverted_residual_setting1=d[0], \n",
    "                         inverted_residual_setting2=d[1]).cuda()\n",
    "        \n",
    "        p = sum(p.numel() for p in model.parameters())\n",
    "        optimizer = create_optimizer(model, 0.1)\n",
    "        score, t = train_triangular_policy(model, optimizer, train_loader, valid_loader, valid_dataset,\n",
    "                                           loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                                           dataset='rsna', binary=True, max_lr=0.1, epochs=15)\n",
    "        data.append([w, d_s, score, p, t])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['width_x', 'depth_x', 'val_score', 'params', 'time_per_epoch']\n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"rsna_resnet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.read_csv('rsna_resnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
