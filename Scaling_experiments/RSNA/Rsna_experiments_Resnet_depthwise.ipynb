{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../prepare_data.py\n",
    "%run ../../architectures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, valid_dataset = rsna_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 256, 256]), torch.Size([32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(block=depthwise_block).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728193"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.9 s, sys: 5.94 s, total: 35.9 s\n",
      "Wall time: 36.2 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lrs, losses = LR_range_finder(model, train_loader, \n",
    "                              loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                              binary=True, lr_high=0.6)\n",
    "plot_lr(lrs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [1.0, 0.75, 0.5, 0.25]\n",
    "depths = [[[[64, 2], [128, 2]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 2], [128, 2]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 1], [512, 1]]],\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width multiplier - 1.000 depth multiplier - 7.000\n",
      "train_loss 0.030 val_loss 19.524 val_auc_score 0.624\n",
      "----End of step 0:01:02.513159\n",
      "train_loss 0.134 val_loss 64.853 val_auc_score 0.535\n",
      "----End of step 0:01:02.152899\n",
      "train_loss 0.387 val_loss 320.681 val_auc_score 0.526\n",
      "----End of step 0:01:01.846001\n",
      "train_loss 2.702 val_loss 1627.321 val_auc_score 0.597\n",
      "----End of step 0:01:01.450035\n",
      "train_loss 10.864 val_loss 82320.222 val_auc_score 0.497\n",
      "----End of step 0:01:02.771972\n",
      "train_loss 13.376 val_loss 731.406 val_auc_score 0.611\n",
      "----End of step 0:01:04.145403\n",
      "train_loss 11.401 val_loss 60029.963 val_auc_score 0.625\n",
      "----End of step 0:01:02.813704\n",
      "train_loss 4.655 val_loss 39733.781 val_auc_score 0.594\n",
      "----End of step 0:01:01.696390\n",
      "train_loss 1.413 val_loss 52.109 val_auc_score 0.606\n",
      "----End of step 0:01:00.540937\n",
      "train_loss 0.506 val_loss 12.216 val_auc_score 0.632\n",
      "----End of step 0:01:00.662393\n",
      "train_loss 0.274 val_loss 86.048 val_auc_score 0.609\n",
      "----End of step 0:01:01.339227\n",
      "train_loss 0.219 val_loss 28392.839 val_auc_score 0.569\n",
      "----End of step 0:01:01.630382\n",
      "train_loss 0.210 val_loss 9.037 val_auc_score 0.585\n",
      "----End of step 0:01:02.335701\n",
      "train_loss 0.335 val_loss 2.625 val_auc_score 0.640\n",
      "----End of step 0:01:02.893652\n",
      "train_loss 0.449 val_loss 21.883 val_auc_score 0.639\n",
      "----End of step 0:01:02.421098\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.029 val_loss 29.868 val_auc_score 0.662\n",
      "----End of step 0:01:00.951512\n",
      "train_loss 0.167 val_loss 81.814 val_auc_score 0.494\n",
      "----End of step 0:01:01.563675\n",
      "train_loss 0.559 val_loss 130.224 val_auc_score 0.526\n",
      "----End of step 0:01:00.543673\n",
      "train_loss 1.615 val_loss 410.824 val_auc_score 0.519\n",
      "----End of step 0:01:00.858255\n",
      "train_loss 7.534 val_loss 670.251 val_auc_score 0.705\n",
      "----End of step 0:01:00.261332\n",
      "train_loss 10.877 val_loss 1627.688 val_auc_score 0.621\n",
      "----End of step 0:01:00.774974\n",
      "train_loss 6.221 val_loss 770.865 val_auc_score 0.667\n",
      "----End of step 0:01:00.761166\n",
      "train_loss 5.460 val_loss 1551399.638 val_auc_score 0.416\n",
      "----End of step 0:01:00.846910\n",
      "train_loss 3.598 val_loss 3759.434 val_auc_score 0.532\n",
      "----End of step 0:01:02.016788\n",
      "train_loss 1.007 val_loss 4257.921 val_auc_score 0.504\n",
      "----End of step 0:01:00.956981\n",
      "train_loss 0.547 val_loss 5818.475 val_auc_score 0.495\n",
      "----End of step 0:01:00.919352\n",
      "train_loss 0.364 val_loss 612.884 val_auc_score 0.482\n",
      "----End of step 0:01:01.629554\n",
      "train_loss 0.249 val_loss 136.307 val_auc_score 0.531\n",
      "----End of step 0:01:00.230350\n",
      "train_loss 0.356 val_loss 1318.086 val_auc_score 0.474\n",
      "----End of step 0:01:00.882284\n",
      "train_loss 0.427 val_loss 23.315 val_auc_score 0.525\n",
      "----End of step 0:01:01.042728\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.030 val_loss 38.013 val_auc_score 0.581\n",
      "----End of step 0:00:59.024269\n",
      "train_loss 0.178 val_loss 51.695 val_auc_score 0.471\n",
      "----End of step 0:00:57.585712\n",
      "train_loss 0.375 val_loss 88.043 val_auc_score 0.391\n",
      "----End of step 0:00:59.197624\n",
      "train_loss 1.063 val_loss 419.593 val_auc_score 0.584\n",
      "----End of step 0:00:58.195559\n",
      "train_loss 6.694 val_loss 3277.012 val_auc_score 0.455\n",
      "----End of step 0:00:58.188698\n",
      "train_loss 11.874 val_loss 52122.762 val_auc_score 0.576\n",
      "----End of step 0:00:58.126726\n",
      "train_loss 10.778 val_loss 1218.281 val_auc_score 0.491\n",
      "----End of step 0:00:58.596893\n",
      "train_loss 7.315 val_loss 172.033 val_auc_score 0.524\n",
      "----End of step 0:00:59.233994\n",
      "train_loss 2.719 val_loss 175.245 val_auc_score 0.568\n",
      "----End of step 0:00:57.940391\n",
      "train_loss 1.158 val_loss 36.062 val_auc_score 0.610\n",
      "----End of step 0:00:58.899316\n",
      "train_loss 0.676 val_loss 15.532 val_auc_score 0.654\n",
      "----End of step 0:00:59.231445\n",
      "train_loss 0.345 val_loss 7.998 val_auc_score 0.648\n",
      "----End of step 0:00:58.462898\n",
      "train_loss 0.302 val_loss 6.301 val_auc_score 0.533\n",
      "----End of step 0:00:58.370691\n",
      "train_loss 0.380 val_loss 5.575 val_auc_score 0.487\n",
      "----End of step 0:00:58.726593\n",
      "train_loss 0.413 val_loss 2.068 val_auc_score 0.648\n",
      "----End of step 0:00:59.860995\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.028 val_loss 47.277 val_auc_score 0.315\n",
      "----End of step 0:01:01.272693\n",
      "train_loss 0.190 val_loss 73.760 val_auc_score 0.568\n",
      "----End of step 0:01:01.048185\n",
      "train_loss 0.408 val_loss 108.740 val_auc_score 0.480\n",
      "----End of step 0:01:01.355582\n",
      "train_loss 2.829 val_loss 5289.658 val_auc_score 0.439\n",
      "----End of step 0:01:00.483307\n",
      "train_loss 14.582 val_loss 2073.323 val_auc_score 0.558\n",
      "----End of step 0:01:00.496527\n",
      "train_loss 13.644 val_loss 2754.277 val_auc_score 0.505\n",
      "----End of step 0:01:02.101277\n",
      "train_loss 6.654 val_loss 195.796 val_auc_score 0.486\n",
      "----End of step 0:01:01.461335\n",
      "train_loss 2.573 val_loss 193.501 val_auc_score 0.430\n",
      "----End of step 0:01:01.197380\n",
      "train_loss 1.395 val_loss 14504.273 val_auc_score 0.362\n",
      "----End of step 0:01:01.617413\n",
      "train_loss 1.388 val_loss 53.823 val_auc_score 0.511\n",
      "----End of step 0:01:00.205519\n",
      "train_loss 0.624 val_loss 15.158 val_auc_score 0.586\n",
      "----End of step 0:01:01.979554\n",
      "train_loss 0.343 val_loss 7.298 val_auc_score 0.533\n",
      "----End of step 0:01:01.764234\n",
      "train_loss 0.215 val_loss 5.968 val_auc_score 0.551\n",
      "----End of step 0:01:01.640772\n",
      "train_loss 0.336 val_loss 4.010 val_auc_score 0.552\n",
      "----End of step 0:01:01.822535\n",
      "train_loss 0.492 val_loss 1.810 val_auc_score 0.445\n",
      "----End of step 0:01:01.160089\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.028 val_loss 28.983 val_auc_score 0.640\n",
      "----End of step 0:01:00.015072\n",
      "train_loss 0.177 val_loss 103.504 val_auc_score 0.541\n",
      "----End of step 0:00:58.968931\n",
      "train_loss 0.725 val_loss 178.437 val_auc_score 0.533\n",
      "----End of step 0:00:59.172811\n",
      "train_loss 2.211 val_loss 12231.538 val_auc_score 0.481\n",
      "----End of step 0:00:58.873265\n",
      "train_loss 9.492 val_loss 747.256 val_auc_score 0.425\n",
      "----End of step 0:00:59.232161\n",
      "train_loss 10.502 val_loss 305.015 val_auc_score 0.423\n",
      "----End of step 0:00:59.127901\n",
      "train_loss 3.526 val_loss 575.412 val_auc_score 0.473\n",
      "----End of step 0:00:58.343462\n",
      "train_loss 2.741 val_loss 71.805 val_auc_score 0.478\n",
      "----End of step 0:00:59.367643\n",
      "train_loss 1.123 val_loss 141.136 val_auc_score 0.536\n",
      "----End of step 0:00:59.519958\n",
      "train_loss 1.226 val_loss 206.179 val_auc_score 0.491\n",
      "----End of step 0:00:58.361630\n",
      "train_loss 0.669 val_loss 245.259 val_auc_score 0.478\n",
      "----End of step 0:00:58.372970\n",
      "train_loss 0.370 val_loss 1416.014 val_auc_score 0.367\n",
      "----End of step 0:00:58.841093\n",
      "train_loss 0.236 val_loss 10.655 val_auc_score 0.399\n",
      "----End of step 0:00:58.787968\n",
      "train_loss 0.238 val_loss 24.959 val_auc_score 0.476\n",
      "----End of step 0:00:58.807856\n",
      "train_loss 0.405 val_loss 7.273 val_auc_score 0.470\n",
      "----End of step 0:00:58.738582\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 4.000\n",
      "train_loss 0.031 val_loss 36.924 val_auc_score 0.652\n",
      "----End of step 0:00:56.280458\n",
      "train_loss 0.178 val_loss 68.835 val_auc_score 0.387\n",
      "----End of step 0:00:55.865849\n",
      "train_loss 0.523 val_loss 286.825 val_auc_score 0.518\n",
      "----End of step 0:00:56.032507\n",
      "train_loss 2.824 val_loss 676.421 val_auc_score 0.449\n",
      "----End of step 0:00:56.761069\n",
      "train_loss 8.490 val_loss 1313.515 val_auc_score 0.528\n",
      "----End of step 0:00:55.897056\n",
      "train_loss 6.581 val_loss 424.528 val_auc_score 0.383\n",
      "----End of step 0:00:57.222194\n",
      "train_loss 6.419 val_loss 537.589 val_auc_score 0.517\n",
      "----End of step 0:01:02.733500\n",
      "train_loss 6.248 val_loss 1380.757 val_auc_score 0.420\n",
      "----End of step 0:01:08.368877\n",
      "train_loss 3.538 val_loss 351.228 val_auc_score 0.404\n",
      "----End of step 0:00:55.443205\n",
      "train_loss 1.128 val_loss 41.369 val_auc_score 0.358\n",
      "----End of step 0:00:55.436034\n",
      "train_loss 0.606 val_loss 17.855 val_auc_score 0.377\n",
      "----End of step 0:00:55.959706\n",
      "train_loss 0.356 val_loss 14.813 val_auc_score 0.381\n",
      "----End of step 0:00:56.677680\n",
      "train_loss 0.340 val_loss 10.028 val_auc_score 0.379\n",
      "----End of step 0:01:08.645490\n",
      "train_loss 0.268 val_loss 5.346 val_auc_score 0.430\n",
      "----End of step 0:01:00.089520\n",
      "train_loss 0.465 val_loss 2.390 val_auc_score 0.372\n",
      "----End of step 0:00:57.548205\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 7.000\n",
      "train_loss 0.022 val_loss 13.470 val_auc_score 0.607\n",
      "----End of step 0:01:16.246927\n",
      "train_loss 0.104 val_loss 40.103 val_auc_score 0.465\n",
      "----End of step 0:01:14.489727\n",
      "train_loss 0.229 val_loss 171.547 val_auc_score 0.482\n",
      "----End of step 0:01:15.499803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 1.861 val_loss 1750.354 val_auc_score 0.377\n",
      "----End of step 0:01:14.673170\n",
      "train_loss 7.354 val_loss 66541.395 val_auc_score 0.409\n",
      "----End of step 0:01:10.367678\n",
      "train_loss 5.082 val_loss 659005.179 val_auc_score 0.352\n",
      "----End of step 0:01:06.011598\n",
      "train_loss 4.470 val_loss 1771.344 val_auc_score 0.357\n",
      "----End of step 0:01:06.347910\n",
      "train_loss 2.353 val_loss 273.951 val_auc_score 0.396\n",
      "----End of step 0:01:05.411926\n",
      "train_loss 1.413 val_loss 85.676 val_auc_score 0.557\n",
      "----End of step 0:01:05.657979\n",
      "train_loss 0.943 val_loss 10.480 val_auc_score 0.690\n",
      "----End of step 0:01:05.994396\n",
      "train_loss 0.387 val_loss 157.903 val_auc_score 0.637\n",
      "----End of step 0:01:05.789104\n",
      "train_loss 0.394 val_loss 11.223 val_auc_score 0.631\n",
      "----End of step 0:01:05.015309\n",
      "train_loss 0.237 val_loss 6.213 val_auc_score 0.389\n",
      "----End of step 0:01:05.513908\n",
      "train_loss 0.256 val_loss 2.912 val_auc_score 0.679\n",
      "----End of step 0:01:05.818237\n",
      "train_loss 0.525 val_loss 2.974 val_auc_score 0.654\n",
      "----End of step 0:01:10.536582\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.023 val_loss 16.845 val_auc_score 0.570\n",
      "----End of step 0:01:07.988746\n",
      "train_loss 0.109 val_loss 28.057 val_auc_score 0.565\n",
      "----End of step 0:01:07.215106\n",
      "train_loss 0.164 val_loss 121.569 val_auc_score 0.606\n",
      "----End of step 0:01:10.707307\n",
      "train_loss 1.012 val_loss 287.187 val_auc_score 0.434\n",
      "----End of step 0:01:09.572963\n",
      "train_loss 5.292 val_loss 751.699 val_auc_score 0.419\n",
      "----End of step 0:01:09.199959\n",
      "train_loss 10.577 val_loss 334.357 val_auc_score 0.452\n",
      "----End of step 0:01:08.806111\n",
      "train_loss 5.155 val_loss 141.226 val_auc_score 0.418\n",
      "----End of step 0:01:09.821488\n",
      "train_loss 1.041 val_loss 818.985 val_auc_score 0.390\n",
      "----End of step 0:01:09.391856\n",
      "train_loss 1.115 val_loss 108.701 val_auc_score 0.479\n",
      "----End of step 0:01:08.525286\n",
      "train_loss 1.327 val_loss 1492.880 val_auc_score 0.451\n",
      "----End of step 0:01:07.923197\n",
      "train_loss 0.211 val_loss 45.628 val_auc_score 0.374\n",
      "----End of step 0:01:07.682035\n",
      "train_loss 0.319 val_loss 16.129 val_auc_score 0.502\n",
      "----End of step 0:01:10.052339\n",
      "train_loss 0.294 val_loss 1787.112 val_auc_score 0.474\n",
      "----End of step 0:01:10.989136\n",
      "train_loss 0.279 val_loss 9.907 val_auc_score 0.507\n",
      "----End of step 0:01:08.421749\n",
      "train_loss 0.439 val_loss 6.948 val_auc_score 0.513\n",
      "----End of step 0:01:08.363756\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.022 val_loss 23.422 val_auc_score 0.449\n",
      "----End of step 0:01:07.500120\n",
      "train_loss 0.109 val_loss 28.385 val_auc_score 0.469\n",
      "----End of step 0:01:08.828339\n",
      "train_loss 0.235 val_loss 167.337 val_auc_score 0.519\n",
      "----End of step 0:01:07.301350\n",
      "train_loss 1.524 val_loss 439.896 val_auc_score 0.592\n",
      "----End of step 0:01:06.394694\n",
      "train_loss 7.028 val_loss 479.992 val_auc_score 0.489\n",
      "----End of step 0:01:10.408441\n",
      "train_loss 10.259 val_loss 721.134 val_auc_score 0.608\n",
      "----End of step 0:01:01.386602\n",
      "train_loss 12.557 val_loss 284.734 val_auc_score 0.420\n",
      "----End of step 0:01:00.857230\n",
      "train_loss 4.564 val_loss 207.960 val_auc_score 0.374\n",
      "----End of step 0:01:01.404565\n",
      "train_loss 2.477 val_loss 105.718 val_auc_score 0.403\n",
      "----End of step 0:01:00.230939\n",
      "train_loss 0.526 val_loss 82.722 val_auc_score 0.408\n",
      "----End of step 0:01:00.270672\n",
      "train_loss 0.332 val_loss 13.191 val_auc_score 0.536\n",
      "----End of step 0:01:00.068856\n",
      "train_loss 0.325 val_loss 10.840 val_auc_score 0.534\n",
      "----End of step 0:01:00.127070\n",
      "train_loss 0.310 val_loss 6.331 val_auc_score 0.495\n",
      "----End of step 0:00:59.668506\n",
      "train_loss 0.295 val_loss 4.962 val_auc_score 0.427\n",
      "----End of step 0:01:02.571603\n",
      "train_loss 0.570 val_loss 1.679 val_auc_score 0.407\n",
      "----End of step 0:01:06.970426\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.023 val_loss 27.612 val_auc_score 0.396\n",
      "----End of step 0:01:10.232804\n",
      "train_loss 0.106 val_loss 47.871 val_auc_score 0.621\n",
      "----End of step 0:01:06.409664\n",
      "train_loss 0.345 val_loss 187.795 val_auc_score 0.414\n",
      "----End of step 0:01:07.815416\n",
      "train_loss 1.564 val_loss 481.286 val_auc_score 0.448\n",
      "----End of step 0:01:05.482852\n",
      "train_loss 8.323 val_loss 894.042 val_auc_score 0.422\n",
      "----End of step 0:01:06.512418\n",
      "train_loss 8.441 val_loss 483.069 val_auc_score 0.579\n",
      "----End of step 0:01:08.318965\n",
      "train_loss 10.730 val_loss 244.142 val_auc_score 0.517\n",
      "----End of step 0:01:04.596489\n",
      "train_loss 5.243 val_loss 9896.872 val_auc_score 0.404\n",
      "----End of step 0:01:07.684494\n",
      "train_loss 3.515 val_loss 156.720 val_auc_score 0.388\n",
      "----End of step 0:01:08.021962\n",
      "train_loss 1.460 val_loss 71.665 val_auc_score 0.347\n",
      "----End of step 0:01:06.853714\n",
      "train_loss 0.739 val_loss 22043.310 val_auc_score 0.329\n",
      "----End of step 0:01:08.666067\n",
      "train_loss 0.349 val_loss 492.732 val_auc_score 0.378\n",
      "----End of step 0:01:05.724255\n",
      "train_loss 0.219 val_loss 126.681 val_auc_score 0.323\n",
      "----End of step 0:01:05.204080\n",
      "train_loss 0.458 val_loss 156.191 val_auc_score 0.444\n",
      "----End of step 0:01:02.402241\n",
      "train_loss 0.469 val_loss 145.679 val_auc_score 0.610\n",
      "----End of step 0:01:01.009160\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.022 val_loss 16.720 val_auc_score 0.534\n",
      "----End of step 0:01:00.733986\n",
      "train_loss 0.106 val_loss 47.352 val_auc_score 0.521\n",
      "----End of step 0:00:59.940943\n",
      "train_loss 0.680 val_loss 520.244 val_auc_score 0.518\n",
      "----End of step 0:00:59.187862\n",
      "train_loss 4.341 val_loss 872.306 val_auc_score 0.658\n",
      "----End of step 0:01:00.866033\n",
      "train_loss 9.149 val_loss 768.616 val_auc_score 0.579\n",
      "----End of step 0:00:58.899553\n",
      "train_loss 8.115 val_loss 241.694 val_auc_score 0.411\n",
      "----End of step 0:01:00.612637\n",
      "train_loss 3.152 val_loss 225.482 val_auc_score 0.527\n",
      "----End of step 0:01:00.131065\n",
      "train_loss 3.199 val_loss 150.762 val_auc_score 0.496\n",
      "----End of step 0:00:59.280321\n",
      "train_loss 2.240 val_loss 23.756 val_auc_score 0.546\n",
      "----End of step 0:01:00.597858\n",
      "train_loss 0.479 val_loss 25.238 val_auc_score 0.544\n",
      "----End of step 0:00:59.370058\n",
      "train_loss 0.485 val_loss 15.013 val_auc_score 0.503\n",
      "----End of step 0:01:00.145114\n",
      "train_loss 0.331 val_loss 21.437 val_auc_score 0.626\n",
      "----End of step 0:01:00.078582\n",
      "train_loss 0.286 val_loss 18.150 val_auc_score 0.528\n",
      "----End of step 0:01:00.478837\n",
      "train_loss 0.486 val_loss 9.224 val_auc_score 0.432\n",
      "----End of step 0:00:59.572938\n",
      "train_loss 0.404 val_loss 10.598 val_auc_score 0.601\n",
      "----End of step 0:00:59.106368\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 4.000\n",
      "train_loss 0.023 val_loss 24.427 val_auc_score 0.560\n",
      "----End of step 0:00:58.319328\n",
      "train_loss 0.124 val_loss 59.601 val_auc_score 0.443\n",
      "----End of step 0:00:58.641516\n",
      "train_loss 0.480 val_loss 311.482 val_auc_score 0.544\n",
      "----End of step 0:00:56.292936\n",
      "train_loss 2.558 val_loss 146.912 val_auc_score 0.453\n",
      "----End of step 0:00:56.575024\n",
      "train_loss 2.045 val_loss 149.891 val_auc_score 0.494\n",
      "----End of step 0:00:56.672134\n",
      "train_loss 2.513 val_loss 219.913 val_auc_score 0.355\n",
      "----End of step 0:00:57.377526\n",
      "train_loss 3.028 val_loss 72.457 val_auc_score 0.570\n",
      "----End of step 0:00:58.624618\n",
      "train_loss 1.921 val_loss 47.989 val_auc_score 0.475\n",
      "----End of step 0:00:56.856163\n",
      "train_loss 0.648 val_loss 65.574 val_auc_score 0.609\n",
      "----End of step 0:00:57.347318\n",
      "train_loss 0.666 val_loss 27.073 val_auc_score 0.601\n",
      "----End of step 0:00:56.871426\n",
      "train_loss 0.484 val_loss 7.698 val_auc_score 0.425\n",
      "----End of step 0:00:58.124546\n",
      "train_loss 0.118 val_loss 10.339 val_auc_score 0.616\n",
      "----End of step 0:00:58.334907\n",
      "train_loss 0.322 val_loss 6.952 val_auc_score 0.279\n",
      "----End of step 0:00:58.247918\n",
      "train_loss 0.334 val_loss 2.603 val_auc_score 0.685\n",
      "----End of step 0:01:03.553647\n",
      "train_loss 0.430 val_loss 1.536 val_auc_score 0.626\n",
      "----End of step 0:01:03.118162\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 7.000\n",
      "train_loss 0.022 val_loss 15.177 val_auc_score 0.604\n",
      "----End of step 0:01:09.851275\n",
      "train_loss 0.120 val_loss 52.303 val_auc_score 0.628\n",
      "----End of step 0:01:10.311617\n",
      "train_loss 0.523 val_loss 383.477 val_auc_score 0.458\n",
      "----End of step 0:01:10.429228\n",
      "train_loss 2.763 val_loss 670.993 val_auc_score 0.516\n",
      "----End of step 0:01:08.555930\n",
      "train_loss 5.441 val_loss 713.521 val_auc_score 0.392\n",
      "----End of step 0:01:09.147148\n",
      "train_loss 7.173 val_loss 194411.589 val_auc_score 0.377\n",
      "----End of step 0:01:07.305624\n",
      "train_loss 7.094 val_loss 11066.411 val_auc_score 0.436\n",
      "----End of step 0:01:06.774933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 5.414 val_loss 284305.040 val_auc_score 0.548\n",
      "----End of step 0:01:10.122049\n",
      "train_loss 3.177 val_loss 87.732 val_auc_score 0.413\n",
      "----End of step 0:01:08.373130\n",
      "train_loss 1.111 val_loss 19.896 val_auc_score 0.537\n",
      "----End of step 0:01:08.239447\n",
      "train_loss 0.364 val_loss 8.877 val_auc_score 0.651\n",
      "----End of step 0:01:07.785468\n",
      "train_loss 0.360 val_loss 7.255 val_auc_score 0.638\n",
      "----End of step 0:01:06.445525\n",
      "train_loss 0.372 val_loss 3.855 val_auc_score 0.606\n",
      "----End of step 0:01:04.841240\n",
      "train_loss 0.304 val_loss 2.559 val_auc_score 0.518\n",
      "----End of step 0:01:05.477044\n",
      "train_loss 0.569 val_loss 1.638 val_auc_score 0.485\n",
      "----End of step 0:01:05.231030\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 6.000\n",
      "train_loss 0.021 val_loss 16.275 val_auc_score 0.593\n",
      "----End of step 0:01:03.047483\n",
      "train_loss 0.130 val_loss 37.902 val_auc_score 0.622\n",
      "----End of step 0:01:03.193555\n",
      "train_loss 0.329 val_loss 258.272 val_auc_score 0.481\n",
      "----End of step 0:01:03.206116\n",
      "train_loss 3.113 val_loss 475.614 val_auc_score 0.641\n",
      "----End of step 0:01:02.154135\n",
      "train_loss 6.363 val_loss 614.451 val_auc_score 0.567\n",
      "----End of step 0:01:01.461323\n",
      "train_loss 9.482 val_loss 450.782 val_auc_score 0.362\n",
      "----End of step 0:01:02.332839\n",
      "train_loss 7.939 val_loss 280.377 val_auc_score 0.501\n",
      "----End of step 0:01:03.588252\n",
      "train_loss 4.378 val_loss 183.922 val_auc_score 0.559\n",
      "----End of step 0:01:01.739403\n",
      "train_loss 3.516 val_loss 53.510 val_auc_score 0.446\n",
      "----End of step 0:01:02.733482\n",
      "train_loss 0.689 val_loss 25.536 val_auc_score 0.616\n",
      "----End of step 0:01:02.374692\n",
      "train_loss 0.519 val_loss 14181.386 val_auc_score 0.567\n",
      "----End of step 0:01:02.702433\n",
      "train_loss 0.258 val_loss 7.039 val_auc_score 0.611\n",
      "----End of step 0:01:02.443183\n",
      "train_loss 0.319 val_loss 5.829 val_auc_score 0.556\n",
      "----End of step 0:01:01.855578\n",
      "train_loss 0.444 val_loss 2.461 val_auc_score 0.549\n",
      "----End of step 0:01:03.507240\n",
      "train_loss 0.685 val_loss 1.041 val_auc_score 0.555\n",
      "----End of step 0:01:02.944151\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 5.000\n",
      "train_loss 0.023 val_loss 20.478 val_auc_score 0.533\n",
      "----End of step 0:01:00.031543\n",
      "train_loss 0.118 val_loss 30.610 val_auc_score 0.601\n",
      "----End of step 0:00:59.783347\n",
      "train_loss 0.281 val_loss 201.758 val_auc_score 0.465\n",
      "----End of step 0:01:00.124416\n",
      "train_loss 9.145 val_loss 291.199 val_auc_score 0.354\n",
      "----End of step 0:01:00.428087\n",
      "train_loss 6.289 val_loss 644.312 val_auc_score 0.517\n",
      "----End of step 0:00:59.244649\n",
      "train_loss 8.453 val_loss 306.937 val_auc_score 0.663\n",
      "----End of step 0:01:00.940954\n",
      "train_loss 4.017 val_loss 156.152 val_auc_score 0.402\n",
      "----End of step 0:01:01.536369\n",
      "train_loss 1.501 val_loss 55.577 val_auc_score 0.411\n",
      "----End of step 0:01:00.625907\n",
      "train_loss 0.667 val_loss 19.461 val_auc_score 0.442\n",
      "----End of step 0:01:01.225417\n",
      "train_loss 0.450 val_loss 21.653 val_auc_score 0.522\n",
      "----End of step 0:01:00.485400\n",
      "train_loss 0.332 val_loss 8.793 val_auc_score 0.546\n",
      "----End of step 0:01:00.329707\n",
      "train_loss 0.235 val_loss 5.946 val_auc_score 0.598\n",
      "----End of step 0:01:00.400161\n",
      "train_loss 0.275 val_loss 3.989 val_auc_score 0.583\n",
      "----End of step 0:01:00.635921\n",
      "train_loss 0.555 val_loss 1.997 val_auc_score 0.379\n",
      "----End of step 0:01:00.226080\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 6.000\n",
      "train_loss 0.022 val_loss 13.819 val_auc_score 0.590\n",
      "----End of step 0:01:02.693309\n",
      "train_loss 0.104 val_loss 67.190 val_auc_score 0.518\n",
      "----End of step 0:01:03.191411\n",
      "train_loss 0.347 val_loss 126.901 val_auc_score 0.526\n",
      "----End of step 0:01:02.318451\n",
      "train_loss 2.628 val_loss 665.221 val_auc_score 0.432\n",
      "----End of step 0:01:02.418976\n",
      "train_loss 8.000 val_loss 295.400 val_auc_score 0.383\n",
      "----End of step 0:01:03.330865\n",
      "train_loss 8.156 val_loss 276.129 val_auc_score 0.432\n",
      "----End of step 0:01:02.654129\n",
      "train_loss 4.476 val_loss 177.485 val_auc_score 0.499\n",
      "----End of step 0:01:03.591626\n",
      "train_loss 2.390 val_loss 3060.785 val_auc_score 0.598\n",
      "----End of step 0:01:03.437032\n",
      "train_loss 2.045 val_loss 50.619 val_auc_score 0.559\n",
      "----End of step 0:01:03.124397\n",
      "train_loss 0.697 val_loss 29.844 val_auc_score 0.564\n",
      "----End of step 0:01:03.381475\n",
      "train_loss 0.589 val_loss 241.446 val_auc_score 0.685\n",
      "----End of step 0:01:03.506278\n",
      "train_loss 0.574 val_loss 423.754 val_auc_score 0.477\n",
      "----End of step 0:01:03.243163\n",
      "train_loss 0.319 val_loss 4.867 val_auc_score 0.563\n",
      "----End of step 0:01:03.362434\n",
      "train_loss 0.421 val_loss 3.285 val_auc_score 0.554\n",
      "----End of step 0:01:03.520425\n",
      "train_loss 0.533 val_loss 2.737 val_auc_score 0.414\n",
      "----End of step 0:01:03.884256\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 5.000\n",
      "train_loss 0.023 val_loss 19.292 val_auc_score 0.488\n",
      "----End of step 0:01:00.547004\n",
      "train_loss 0.102 val_loss 70.302 val_auc_score 0.525\n",
      "----End of step 0:01:00.877081\n",
      "train_loss 0.427 val_loss 127.070 val_auc_score 0.381\n",
      "----End of step 0:00:59.483997\n",
      "train_loss 2.036 val_loss 932.953 val_auc_score 0.631\n",
      "----End of step 0:01:00.799250\n",
      "train_loss 9.341 val_loss 9063.259 val_auc_score 0.358\n",
      "----End of step 0:01:00.578253\n",
      "train_loss 7.793 val_loss 486.637 val_auc_score 0.533\n",
      "----End of step 0:01:01.563550\n",
      "train_loss 3.879 val_loss 172.314 val_auc_score 0.329\n",
      "----End of step 0:01:01.202991\n",
      "train_loss 3.257 val_loss 29.055 val_auc_score 0.577\n",
      "----End of step 0:01:00.893173\n",
      "train_loss 1.146 val_loss 90.370 val_auc_score 0.385\n",
      "----End of step 0:01:00.759407\n",
      "train_loss 1.243 val_loss 59.469 val_auc_score 0.485\n",
      "----End of step 0:01:01.148891\n",
      "train_loss 0.845 val_loss 53.839 val_auc_score 0.374\n",
      "----End of step 0:01:01.621668\n",
      "train_loss 0.514 val_loss 12.218 val_auc_score 0.445\n",
      "----End of step 0:01:02.931767\n",
      "train_loss 0.288 val_loss 10.324 val_auc_score 0.321\n",
      "----End of step 0:01:01.034292\n",
      "train_loss 0.378 val_loss 7.083 val_auc_score 0.530\n",
      "----End of step 0:01:02.780734\n",
      "train_loss 0.587 val_loss 3.935 val_auc_score 0.522\n",
      "----End of step 0:01:02.182720\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 4.000\n",
      "train_loss 0.023 val_loss 17.012 val_auc_score 0.552\n",
      "----End of step 0:00:59.864807\n",
      "train_loss 0.089 val_loss 51.313 val_auc_score 0.522\n",
      "----End of step 0:01:00.697873\n",
      "train_loss 0.325 val_loss 135.524 val_auc_score 0.399\n",
      "----End of step 0:00:59.336766\n",
      "train_loss 1.824 val_loss 497.847 val_auc_score 0.613\n",
      "----End of step 0:00:59.507270\n",
      "train_loss 6.720 val_loss 700.158 val_auc_score 0.511\n",
      "----End of step 0:00:58.048515\n",
      "train_loss 10.400 val_loss 140.471 val_auc_score 0.461\n",
      "----End of step 0:00:58.586345\n",
      "train_loss 3.233 val_loss 116.931 val_auc_score 0.439\n",
      "----End of step 0:00:58.606805\n",
      "train_loss 1.447 val_loss 40.271 val_auc_score 0.527\n",
      "----End of step 0:00:58.635806\n",
      "train_loss 0.985 val_loss 36.695 val_auc_score 0.510\n",
      "----End of step 0:01:00.675537\n",
      "train_loss 0.896 val_loss 16.107 val_auc_score 0.456\n",
      "----End of step 0:01:01.590926\n",
      "train_loss 0.426 val_loss 26.684 val_auc_score 0.461\n",
      "----End of step 0:01:04.841250\n",
      "train_loss 0.375 val_loss 7.818 val_auc_score 0.509\n",
      "----End of step 0:01:03.216690\n",
      "train_loss 0.297 val_loss 7.284 val_auc_score 0.370\n",
      "----End of step 0:01:03.488959\n",
      "train_loss 0.443 val_loss 3.591 val_auc_score 0.609\n",
      "----End of step 0:01:01.570631\n",
      "train_loss 0.660 val_loss 0.898 val_auc_score 0.425\n",
      "----End of step 0:01:01.453666\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 7.000\n",
      "train_loss 0.022 val_loss 16.710 val_auc_score 0.598\n",
      "----End of step 0:01:10.061815\n",
      "train_loss 0.116 val_loss 44.996 val_auc_score 0.582\n",
      "----End of step 0:01:06.927341\n",
      "train_loss 0.436 val_loss 231.160 val_auc_score 0.467\n",
      "----End of step 0:01:08.084493\n",
      "train_loss 2.238 val_loss 698.966 val_auc_score 0.429\n",
      "----End of step 0:01:10.396073\n",
      "train_loss 8.491 val_loss 1309.082 val_auc_score 0.474\n",
      "----End of step 0:01:07.658553\n",
      "train_loss 14.954 val_loss 743.397 val_auc_score 0.606\n",
      "----End of step 0:01:10.973478\n",
      "train_loss 8.546 val_loss 235.177 val_auc_score 0.608\n",
      "----End of step 0:01:10.714743\n",
      "train_loss 4.306 val_loss 294.306 val_auc_score 0.608\n",
      "----End of step 0:01:08.910435\n",
      "train_loss 3.877 val_loss 132.795 val_auc_score 0.332\n",
      "----End of step 0:01:11.100551\n",
      "train_loss 1.119 val_loss 2644.324 val_auc_score 0.560\n",
      "----End of step 0:01:09.201292\n",
      "train_loss 0.300 val_loss 1257.301 val_auc_score 0.554\n",
      "----End of step 0:01:09.378642\n",
      "train_loss 0.263 val_loss 83.800 val_auc_score 0.545\n",
      "----End of step 0:01:08.990605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.255 val_loss 506.379 val_auc_score 0.454\n",
      "----End of step 0:01:09.236086\n",
      "train_loss 0.362 val_loss 13.555 val_auc_score 0.474\n",
      "----End of step 0:01:08.794977\n",
      "train_loss 0.583 val_loss 6.323 val_auc_score 0.526\n",
      "----End of step 0:01:11.485987\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 6.000\n",
      "train_loss 0.022 val_loss 15.115 val_auc_score 0.614\n",
      "----End of step 0:01:09.596515\n",
      "train_loss 0.103 val_loss 36.255 val_auc_score 0.584\n",
      "----End of step 0:01:07.137267\n",
      "train_loss 0.320 val_loss 221.733 val_auc_score 0.653\n",
      "----End of step 0:01:10.185551\n",
      "train_loss 2.337 val_loss 685.161 val_auc_score 0.442\n",
      "----End of step 0:01:05.108256\n",
      "train_loss 10.327 val_loss 3080.284 val_auc_score 0.393\n",
      "----End of step 0:01:05.592828\n",
      "train_loss 17.455 val_loss 16501.370 val_auc_score 0.575\n",
      "----End of step 0:01:05.935766\n",
      "train_loss 9.909 val_loss 238.973 val_auc_score 0.623\n",
      "----End of step 0:01:04.850426\n",
      "train_loss 3.152 val_loss 33.621 val_auc_score 0.637\n",
      "----End of step 0:01:05.257636\n",
      "train_loss 0.638 val_loss 19.345 val_auc_score 0.476\n",
      "----End of step 0:01:04.779192\n",
      "train_loss 0.715 val_loss 41.765 val_auc_score 0.640\n",
      "----End of step 0:01:05.282795\n",
      "train_loss 0.484 val_loss 13.841 val_auc_score 0.632\n",
      "----End of step 0:01:06.366661\n",
      "train_loss 0.318 val_loss 5.702 val_auc_score 0.661\n",
      "----End of step 0:01:07.343831\n",
      "train_loss 0.285 val_loss 5.279 val_auc_score 0.640\n",
      "----End of step 0:01:05.871921\n",
      "train_loss 0.448 val_loss 4.304 val_auc_score 0.598\n",
      "----End of step 0:01:04.064146\n",
      "train_loss 0.576 val_loss 1.599 val_auc_score 0.626\n",
      "----End of step 0:01:03.250693\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 5.000\n",
      "train_loss 0.021 val_loss 20.691 val_auc_score 0.563\n",
      "----End of step 0:01:00.148855\n",
      "train_loss 0.119 val_loss 47.615 val_auc_score 0.728\n",
      "----End of step 0:01:00.823931\n",
      "train_loss 0.382 val_loss 87.569 val_auc_score 0.537\n",
      "----End of step 0:01:01.603896\n",
      "train_loss 1.508 val_loss 250.587 val_auc_score 0.502\n",
      "----End of step 0:01:00.619661\n",
      "train_loss 3.655 val_loss 1032.007 val_auc_score 0.578\n",
      "----End of step 0:01:00.693629\n",
      "train_loss 9.506 val_loss 431.385 val_auc_score 0.731\n",
      "----End of step 0:01:00.071101\n",
      "train_loss 6.459 val_loss 103.334 val_auc_score 0.584\n",
      "----End of step 0:01:00.368407\n",
      "train_loss 1.503 val_loss 30.546 val_auc_score 0.588\n",
      "----End of step 0:01:01.385790\n",
      "train_loss 0.495 val_loss 23.495 val_auc_score 0.468\n",
      "----End of step 0:01:00.760274\n",
      "train_loss 0.363 val_loss 11.497 val_auc_score 0.629\n",
      "----End of step 0:01:01.298510\n",
      "train_loss 0.255 val_loss 7.968 val_auc_score 0.596\n",
      "----End of step 0:01:00.942568\n",
      "train_loss 0.281 val_loss 7.747 val_auc_score 0.601\n",
      "----End of step 0:01:00.800307\n",
      "train_loss 0.326 val_loss 7.807 val_auc_score 0.373\n",
      "----End of step 0:01:00.052085\n",
      "train_loss 0.502 val_loss 3.053 val_auc_score 0.588\n",
      "----End of step 0:01:00.513054\n",
      "train_loss 0.402 val_loss 1.687 val_auc_score 0.644\n",
      "----End of step 0:01:00.795193\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 6.000\n",
      "train_loss 0.022 val_loss 20.751 val_auc_score 0.506\n",
      "----End of step 0:01:02.364641\n",
      "train_loss 0.108 val_loss 30.975 val_auc_score 0.572\n",
      "----End of step 0:01:02.894062\n",
      "train_loss 0.317 val_loss 196.664 val_auc_score 0.538\n",
      "----End of step 0:01:02.624087\n",
      "train_loss 2.552 val_loss 502.906 val_auc_score 0.651\n",
      "----End of step 0:01:02.441120\n",
      "train_loss 7.101 val_loss 1080.409 val_auc_score 0.579\n",
      "----End of step 0:01:01.958264\n",
      "train_loss 9.784 val_loss 351.798 val_auc_score 0.460\n",
      "----End of step 0:01:02.774949\n",
      "train_loss 5.848 val_loss 259.870 val_auc_score 0.615\n",
      "----End of step 0:01:01.822311\n",
      "train_loss 4.323 val_loss 81.847 val_auc_score 0.461\n",
      "----End of step 0:01:02.665821\n",
      "train_loss 1.678 val_loss 137.623 val_auc_score 0.450\n",
      "----End of step 0:01:03.307630\n",
      "train_loss 0.937 val_loss 10.673 val_auc_score 0.516\n",
      "----End of step 0:01:02.284397\n",
      "train_loss 0.252 val_loss 13.304 val_auc_score 0.554\n",
      "----End of step 0:01:02.359852\n",
      "train_loss 0.305 val_loss 7.880 val_auc_score 0.487\n",
      "----End of step 0:01:02.542489\n",
      "train_loss 0.330 val_loss 5.678 val_auc_score 0.605\n",
      "----End of step 0:01:02.805405\n",
      "train_loss 0.479 val_loss 2.580 val_auc_score 0.616\n",
      "----End of step 0:01:02.858973\n",
      "train_loss 0.488 val_loss 1.336 val_auc_score 0.614\n",
      "----End of step 0:01:03.099120\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 5.000\n",
      "train_loss 0.022 val_loss 20.874 val_auc_score 0.483\n",
      "----End of step 0:01:00.659378\n",
      "train_loss 0.112 val_loss 63.414 val_auc_score 0.582\n",
      "----End of step 0:01:00.439279\n",
      "train_loss 0.386 val_loss 170.595 val_auc_score 0.537\n",
      "----End of step 0:00:59.492772\n",
      "train_loss 3.004 val_loss 349.587 val_auc_score 0.494\n",
      "----End of step 0:00:59.097518\n",
      "train_loss 7.173 val_loss 500.840 val_auc_score 0.449\n",
      "----End of step 0:00:59.862873\n",
      "train_loss 10.742 val_loss 567.983 val_auc_score 0.608\n",
      "----End of step 0:01:00.211769\n",
      "train_loss 8.385 val_loss 239.911 val_auc_score 0.575\n",
      "----End of step 0:01:00.072741\n",
      "train_loss 3.109 val_loss 134.197 val_auc_score 0.510\n",
      "----End of step 0:01:01.066415\n",
      "train_loss 1.478 val_loss 83.543 val_auc_score 0.492\n",
      "----End of step 0:00:59.044224\n",
      "train_loss 0.978 val_loss 46.658 val_auc_score 0.366\n",
      "----End of step 0:00:59.047850\n",
      "train_loss 0.729 val_loss 14.674 val_auc_score 0.586\n",
      "----End of step 0:00:59.513958\n",
      "train_loss 0.356 val_loss 7.821 val_auc_score 0.428\n",
      "----End of step 0:00:57.855393\n",
      "train_loss 0.305 val_loss 3.918 val_auc_score 0.494\n",
      "----End of step 0:00:58.862045\n",
      "train_loss 0.293 val_loss 11.270 val_auc_score 0.615\n",
      "----End of step 0:01:00.039780\n",
      "train_loss 0.714 val_loss 6.483 val_auc_score 0.489\n",
      "----End of step 0:00:59.156510\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 4.000\n",
      "train_loss 0.024 val_loss 28.289 val_auc_score 0.478\n",
      "----End of step 0:00:56.489845\n",
      "train_loss 0.153 val_loss 27.007 val_auc_score 0.632\n",
      "----End of step 0:00:57.093730\n",
      "train_loss 0.278 val_loss 124.813 val_auc_score 0.557\n",
      "----End of step 0:00:57.263139\n",
      "train_loss 1.299 val_loss 558.650 val_auc_score 0.457\n",
      "----End of step 0:00:56.780975\n",
      "train_loss 6.422 val_loss 1177.057 val_auc_score 0.386\n",
      "----End of step 0:00:57.691868\n",
      "train_loss 11.244 val_loss 412.869 val_auc_score 0.409\n",
      "----End of step 0:01:00.411517\n",
      "train_loss 7.448 val_loss 249.159 val_auc_score 0.401\n",
      "----End of step 0:00:59.440089\n",
      "train_loss 3.800 val_loss 91.822 val_auc_score 0.369\n",
      "----End of step 0:01:00.778038\n",
      "train_loss 1.560 val_loss 78.543 val_auc_score 0.480\n",
      "----End of step 0:00:59.705325\n",
      "train_loss 1.178 val_loss 24.387 val_auc_score 0.497\n",
      "----End of step 0:00:58.887717\n",
      "train_loss 0.516 val_loss 14.615 val_auc_score 0.361\n",
      "----End of step 0:00:58.648231\n",
      "train_loss 0.336 val_loss 5.071 val_auc_score 0.315\n",
      "----End of step 0:01:00.377838\n",
      "train_loss 0.309 val_loss 5.301 val_auc_score 0.407\n",
      "----End of step 0:00:58.665699\n",
      "train_loss 0.253 val_loss 4.147 val_auc_score 0.575\n",
      "----End of step 0:00:59.399749\n",
      "train_loss 0.385 val_loss 1.997 val_auc_score 0.551\n",
      "----End of step 0:00:59.411506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for w in widths:\n",
    "    for d in depths:\n",
    "        d_s = sum(j[1] for i in d for j in i)\n",
    "        print('width multiplier - %.3f depth multiplier - %.3f' % (w, d_s))\n",
    "        model = resnet18(block=depthwise_block, width_mult=w, \n",
    "                         inverted_residual_setting1=d[0], \n",
    "                         inverted_residual_setting2=d[1]).cuda()\n",
    "        \n",
    "        p = sum(p.numel() for p in model.parameters())\n",
    "        optimizer = create_optimizer(model, 0.1)\n",
    "        score, t = train_triangular_policy(model, optimizer, train_loader, valid_loader, valid_dataset,\n",
    "                                           loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                                           dataset='rsna', binary=True, max_lr=0.1, epochs=15)\n",
    "        data.append([w, d_s, score, p, t])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['width_x', 'depth_x', 'val_score', 'params', 'time_per_epoch']\n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"rsna_resnet_depthwise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.read_csv('rsna_resnet_depthwise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
