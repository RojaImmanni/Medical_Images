{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rimmanni/Medical_Images/Scaling_experiments/Chexpert'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(4)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../prepare_data.py\n",
    "%run ../../architectures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, valid_dataset = get_chexpert_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 256, 256]), torch.Size([32, 5]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(num_classes=5, block=depthwise_block).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730245"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.4 s, sys: 7.87 s, total: 48.3 s\n",
      "Wall time: 52 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lrs, losses = LR_range_finder(model, train_loader, \n",
    "                              loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                              binary=False, lr_high=0.05)\n",
    "plot_lr(lrs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [1.0, 0.75, 0.5, 0.25]\n",
    "depths = [[[[64, 2], [128, 2]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 2], [128, 2]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 1], [512, 1]]],\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width multiplier - 1.000 depth multiplier - 7.000\n",
      "train_loss 0.422 val_loss 0.425 val_auc_score 0.610\n",
      "----End of step 0:01:30.813074\n",
      "train_loss 0.412 val_loss 0.423 val_auc_score 0.631\n",
      "----End of step 0:01:29.537312\n",
      "train_loss 0.407 val_loss 0.425 val_auc_score 0.646\n",
      "----End of step 0:01:32.693696\n",
      "train_loss 0.400 val_loss 0.409 val_auc_score 0.662\n",
      "----End of step 0:01:33.322792\n",
      "train_loss 0.393 val_loss 0.422 val_auc_score 0.668\n",
      "----End of step 0:01:31.208804\n",
      "train_loss 0.389 val_loss 0.409 val_auc_score 0.671\n",
      "----End of step 0:01:30.201969\n",
      "train_loss 0.385 val_loss 0.406 val_auc_score 0.685\n",
      "----End of step 0:01:29.203531\n",
      "train_loss 0.382 val_loss 0.412 val_auc_score 0.682\n",
      "----End of step 0:01:27.408126\n",
      "train_loss 0.380 val_loss 0.402 val_auc_score 0.685\n",
      "----End of step 0:01:27.855715\n",
      "train_loss 0.377 val_loss 0.396 val_auc_score 0.690\n",
      "----End of step 0:01:30.009036\n",
      "train_loss 0.375 val_loss 0.403 val_auc_score 0.692\n",
      "----End of step 0:01:29.241566\n",
      "train_loss 0.373 val_loss 0.405 val_auc_score 0.694\n",
      "----End of step 0:01:29.406355\n",
      "train_loss 0.371 val_loss 0.404 val_auc_score 0.693\n",
      "----End of step 0:01:29.556693\n",
      "train_loss 0.370 val_loss 0.406 val_auc_score 0.693\n",
      "----End of step 0:01:30.004455\n",
      "train_loss 0.370 val_loss 0.407 val_auc_score 0.694\n",
      "----End of step 0:01:31.524146\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.423 val_loss 0.424 val_auc_score 0.612\n",
      "----End of step 0:01:25.600250\n",
      "train_loss 0.412 val_loss 0.416 val_auc_score 0.634\n",
      "----End of step 0:01:26.531167\n",
      "train_loss 0.408 val_loss 0.423 val_auc_score 0.649\n",
      "----End of step 0:01:26.424190\n",
      "train_loss 0.401 val_loss 0.424 val_auc_score 0.663\n",
      "----End of step 0:01:28.524818\n",
      "train_loss 0.395 val_loss 0.405 val_auc_score 0.664\n",
      "----End of step 0:01:27.201936\n",
      "train_loss 0.390 val_loss 0.406 val_auc_score 0.674\n",
      "----End of step 0:01:28.038115\n",
      "train_loss 0.386 val_loss 0.401 val_auc_score 0.680\n",
      "----End of step 0:01:27.562604\n",
      "train_loss 0.384 val_loss 0.404 val_auc_score 0.684\n",
      "----End of step 0:01:27.898988\n",
      "train_loss 0.381 val_loss 0.408 val_auc_score 0.683\n",
      "----End of step 0:01:28.020025\n",
      "train_loss 0.378 val_loss 0.407 val_auc_score 0.683\n",
      "----End of step 0:01:25.433319\n",
      "train_loss 0.377 val_loss 0.402 val_auc_score 0.688\n",
      "----End of step 0:01:26.225288\n",
      "train_loss 0.375 val_loss 0.397 val_auc_score 0.689\n",
      "----End of step 0:01:27.123016\n",
      "train_loss 0.373 val_loss 0.397 val_auc_score 0.690\n",
      "----End of step 0:01:28.742698\n",
      "train_loss 0.372 val_loss 0.397 val_auc_score 0.691\n",
      "----End of step 0:01:26.986650\n",
      "train_loss 0.372 val_loss 0.401 val_auc_score 0.690\n",
      "----End of step 0:01:27.315541\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.423 val_loss 0.432 val_auc_score 0.611\n",
      "----End of step 0:01:24.013415\n",
      "train_loss 0.412 val_loss 0.422 val_auc_score 0.631\n",
      "----End of step 0:01:23.827422\n",
      "train_loss 0.409 val_loss 0.416 val_auc_score 0.650\n",
      "----End of step 0:01:20.996342\n",
      "train_loss 0.402 val_loss 0.444 val_auc_score 0.663\n",
      "----End of step 0:01:21.638022\n",
      "train_loss 0.396 val_loss 0.417 val_auc_score 0.665\n",
      "----End of step 0:01:23.074487\n",
      "train_loss 0.391 val_loss 0.406 val_auc_score 0.672\n",
      "----End of step 0:01:22.733967\n",
      "train_loss 0.388 val_loss 0.413 val_auc_score 0.673\n",
      "----End of step 0:01:23.167170\n",
      "train_loss 0.386 val_loss 0.401 val_auc_score 0.680\n",
      "----End of step 0:01:19.706933\n",
      "train_loss 0.383 val_loss 0.407 val_auc_score 0.680\n",
      "----End of step 0:01:21.620213\n",
      "train_loss 0.380 val_loss 0.402 val_auc_score 0.685\n",
      "----End of step 0:01:22.770480\n",
      "train_loss 0.378 val_loss 0.398 val_auc_score 0.684\n",
      "----End of step 0:01:22.897536\n",
      "train_loss 0.377 val_loss 0.404 val_auc_score 0.686\n",
      "----End of step 0:01:23.265989\n",
      "train_loss 0.375 val_loss 0.401 val_auc_score 0.687\n",
      "----End of step 0:01:21.235301\n",
      "train_loss 0.374 val_loss 0.403 val_auc_score 0.687\n",
      "----End of step 0:01:23.308209\n",
      "train_loss 0.374 val_loss 0.403 val_auc_score 0.688\n",
      "----End of step 0:01:24.255817\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.420 val_loss 0.425 val_auc_score 0.612\n",
      "----End of step 0:01:27.978966\n",
      "train_loss 0.411 val_loss 0.421 val_auc_score 0.633\n",
      "----End of step 0:01:29.018893\n",
      "train_loss 0.406 val_loss 0.409 val_auc_score 0.656\n",
      "----End of step 0:01:26.833482\n",
      "train_loss 0.398 val_loss 0.411 val_auc_score 0.660\n",
      "----End of step 0:01:27.802467\n",
      "train_loss 0.392 val_loss 0.413 val_auc_score 0.664\n",
      "----End of step 0:01:27.121517\n",
      "train_loss 0.388 val_loss 0.415 val_auc_score 0.674\n",
      "----End of step 0:01:27.272636\n",
      "train_loss 0.384 val_loss 0.415 val_auc_score 0.682\n",
      "----End of step 0:01:28.794812\n",
      "train_loss 0.382 val_loss 0.396 val_auc_score 0.684\n",
      "----End of step 0:01:28.463584\n",
      "train_loss 0.379 val_loss 0.396 val_auc_score 0.690\n",
      "----End of step 0:01:27.283541\n",
      "train_loss 0.377 val_loss 0.404 val_auc_score 0.692\n",
      "----End of step 0:01:28.573085\n",
      "train_loss 0.374 val_loss 0.392 val_auc_score 0.692\n",
      "----End of step 0:01:28.798871\n",
      "train_loss 0.373 val_loss 0.393 val_auc_score 0.694\n",
      "----End of step 0:01:28.672705\n",
      "train_loss 0.372 val_loss 0.395 val_auc_score 0.695\n",
      "----End of step 0:01:27.796522\n",
      "train_loss 0.371 val_loss 0.390 val_auc_score 0.696\n",
      "----End of step 0:01:27.381079\n",
      "train_loss 0.370 val_loss 0.390 val_auc_score 0.696\n",
      "----End of step 0:01:26.069689\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.421 val_loss 0.425 val_auc_score 0.616\n",
      "----End of step 0:01:17.364550\n",
      "train_loss 0.411 val_loss 0.443 val_auc_score 0.636\n",
      "----End of step 0:01:18.153234\n",
      "train_loss 0.406 val_loss 0.443 val_auc_score 0.654\n",
      "----End of step 0:01:17.901234\n",
      "train_loss 0.392 val_loss 0.412 val_auc_score 0.675\n",
      "----End of step 0:01:15.556320\n",
      "train_loss 0.388 val_loss 0.404 val_auc_score 0.682\n",
      "----End of step 0:01:15.527204\n",
      "train_loss 0.385 val_loss 0.404 val_auc_score 0.680\n",
      "----End of step 0:01:14.861696\n",
      "train_loss 0.382 val_loss 0.397 val_auc_score 0.690\n",
      "----End of step 0:01:14.071574\n",
      "train_loss 0.379 val_loss 0.399 val_auc_score 0.691\n",
      "----End of step 0:01:12.655264\n",
      "train_loss 0.376 val_loss 0.402 val_auc_score 0.690\n",
      "----End of step 0:01:13.994750\n",
      "train_loss 0.374 val_loss 0.399 val_auc_score 0.695\n",
      "----End of step 0:01:12.595992\n",
      "train_loss 0.373 val_loss 0.410 val_auc_score 0.696\n",
      "----End of step 0:01:11.579042\n",
      "train_loss 0.371 val_loss 0.398 val_auc_score 0.697\n",
      "----End of step 0:01:13.784713\n",
      "train_loss 0.370 val_loss 0.398 val_auc_score 0.698\n",
      "----End of step 0:01:14.158018\n",
      "train_loss 0.370 val_loss 0.398 val_auc_score 0.697\n",
      "----End of step 0:01:14.153903\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 4.000\n",
      "train_loss 0.421 val_loss 0.428 val_auc_score 0.611\n",
      "----End of step 0:01:13.375146\n",
      "train_loss 0.412 val_loss 0.419 val_auc_score 0.633\n",
      "----End of step 0:01:12.860437\n",
      "train_loss 0.408 val_loss 0.414 val_auc_score 0.650\n",
      "----End of step 0:01:14.050488\n",
      "train_loss 0.401 val_loss 0.429 val_auc_score 0.656\n",
      "----End of step 0:01:12.618059\n",
      "train_loss 0.395 val_loss 0.410 val_auc_score 0.660\n",
      "----End of step 0:01:15.006626\n",
      "train_loss 0.391 val_loss 0.406 val_auc_score 0.669\n",
      "----End of step 0:01:12.192903\n",
      "train_loss 0.388 val_loss 0.417 val_auc_score 0.680\n",
      "----End of step 0:01:12.018845\n",
      "train_loss 0.385 val_loss 0.404 val_auc_score 0.678\n",
      "----End of step 0:01:12.325844\n",
      "train_loss 0.383 val_loss 0.412 val_auc_score 0.682\n",
      "----End of step 0:01:12.176459\n",
      "train_loss 0.381 val_loss 0.399 val_auc_score 0.685\n",
      "----End of step 0:01:12.187123\n",
      "train_loss 0.378 val_loss 0.402 val_auc_score 0.686\n",
      "----End of step 0:01:12.328155\n",
      "train_loss 0.374 val_loss 0.396 val_auc_score 0.690\n",
      "----End of step 0:01:12.059300\n",
      "train_loss 0.374 val_loss 0.396 val_auc_score 0.690\n",
      "----End of step 0:01:12.383046\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 7.000\n",
      "train_loss 0.424 val_loss 0.428 val_auc_score 0.585\n",
      "----End of step 0:01:18.219500\n",
      "train_loss 0.414 val_loss 0.438 val_auc_score 0.607\n",
      "----End of step 0:01:18.263703\n",
      "train_loss 0.409 val_loss 0.425 val_auc_score 0.629\n",
      "----End of step 0:01:18.241806\n",
      "train_loss 0.403 val_loss 0.415 val_auc_score 0.642\n",
      "----End of step 0:01:18.116658\n",
      "train_loss 0.398 val_loss 0.412 val_auc_score 0.659\n",
      "----End of step 0:01:18.222648\n",
      "train_loss 0.394 val_loss 0.412 val_auc_score 0.660\n",
      "----End of step 0:01:18.226630\n",
      "train_loss 0.390 val_loss 0.404 val_auc_score 0.662\n",
      "----End of step 0:01:18.250721\n",
      "train_loss 0.386 val_loss 0.401 val_auc_score 0.678\n",
      "----End of step 0:01:18.292427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.384 val_loss 0.399 val_auc_score 0.682\n",
      "----End of step 0:01:18.491887\n",
      "train_loss 0.382 val_loss 0.399 val_auc_score 0.683\n",
      "----End of step 0:01:19.008253\n",
      "train_loss 0.380 val_loss 0.398 val_auc_score 0.683\n",
      "----End of step 0:01:19.022534\n",
      "train_loss 0.377 val_loss 0.396 val_auc_score 0.686\n",
      "----End of step 0:01:19.436302\n",
      "train_loss 0.377 val_loss 0.396 val_auc_score 0.686\n",
      "----End of step 0:01:19.346126\n",
      "train_loss 0.375 val_loss 0.395 val_auc_score 0.687\n",
      "----End of step 0:01:18.269738\n",
      "train_loss 0.375 val_loss 0.395 val_auc_score 0.687\n",
      "----End of step 0:01:18.596128\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.425 val_loss 0.433 val_auc_score 0.591\n",
      "----End of step 0:01:15.288949\n",
      "train_loss 0.414 val_loss 0.432 val_auc_score 0.609\n",
      "----End of step 0:01:15.444502\n",
      "train_loss 0.411 val_loss 0.422 val_auc_score 0.615\n",
      "----End of step 0:01:16.299149\n",
      "train_loss 0.405 val_loss 0.413 val_auc_score 0.637\n",
      "----End of step 0:01:15.897261\n",
      "train_loss 0.401 val_loss 0.411 val_auc_score 0.647\n",
      "----End of step 0:01:16.164048\n",
      "train_loss 0.397 val_loss 0.412 val_auc_score 0.652\n",
      "----End of step 0:01:16.332888\n",
      "train_loss 0.393 val_loss 0.409 val_auc_score 0.656\n",
      "----End of step 0:01:16.143506\n",
      "train_loss 0.390 val_loss 0.411 val_auc_score 0.662\n",
      "----End of step 0:01:16.114162\n",
      "train_loss 0.388 val_loss 0.414 val_auc_score 0.667\n",
      "----End of step 0:01:15.890623\n",
      "train_loss 0.386 val_loss 0.406 val_auc_score 0.668\n",
      "----End of step 0:01:15.702782\n",
      "train_loss 0.384 val_loss 0.409 val_auc_score 0.671\n",
      "----End of step 0:01:15.823961\n",
      "train_loss 0.383 val_loss 0.406 val_auc_score 0.672\n",
      "----End of step 0:01:15.885206\n",
      "train_loss 0.381 val_loss 0.403 val_auc_score 0.674\n",
      "----End of step 0:01:16.249518\n",
      "train_loss 0.380 val_loss 0.405 val_auc_score 0.674\n",
      "----End of step 0:01:15.819411\n",
      "train_loss 0.380 val_loss 0.405 val_auc_score 0.674\n",
      "----End of step 0:01:15.946059\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.426 val_loss 0.427 val_auc_score 0.596\n",
      "----End of step 0:01:13.419454\n",
      "train_loss 0.414 val_loss 0.439 val_auc_score 0.596\n",
      "----End of step 0:01:13.176161\n",
      "train_loss 0.411 val_loss 0.436 val_auc_score 0.618\n",
      "----End of step 0:01:13.421155\n",
      "train_loss 0.406 val_loss 0.422 val_auc_score 0.633\n",
      "----End of step 0:01:13.305594\n",
      "train_loss 0.402 val_loss 0.416 val_auc_score 0.641\n",
      "----End of step 0:01:13.510913\n",
      "train_loss 0.398 val_loss 0.413 val_auc_score 0.648\n",
      "----End of step 0:01:13.856586\n",
      "train_loss 0.395 val_loss 0.415 val_auc_score 0.643\n",
      "----End of step 0:01:13.375851\n",
      "train_loss 0.392 val_loss 0.404 val_auc_score 0.662\n",
      "----End of step 0:01:13.601854\n",
      "train_loss 0.390 val_loss 0.404 val_auc_score 0.665\n",
      "----End of step 0:01:13.368405\n",
      "train_loss 0.387 val_loss 0.402 val_auc_score 0.668\n",
      "----End of step 0:01:13.492846\n",
      "train_loss 0.386 val_loss 0.402 val_auc_score 0.669\n",
      "----End of step 0:01:13.587145\n",
      "train_loss 0.385 val_loss 0.402 val_auc_score 0.671\n",
      "----End of step 0:01:13.295778\n",
      "train_loss 0.383 val_loss 0.400 val_auc_score 0.674\n",
      "----End of step 0:01:13.924449\n",
      "train_loss 0.383 val_loss 0.399 val_auc_score 0.675\n",
      "----End of step 0:01:13.671808\n",
      "train_loss 0.382 val_loss 0.399 val_auc_score 0.675\n",
      "----End of step 0:01:13.654802\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.424 val_loss 0.429 val_auc_score 0.598\n",
      "----End of step 0:01:14.746472\n",
      "train_loss 0.414 val_loss 0.429 val_auc_score 0.609\n",
      "----End of step 0:01:14.790232\n",
      "train_loss 0.410 val_loss 0.420 val_auc_score 0.624\n",
      "----End of step 0:01:14.965032\n",
      "train_loss 0.404 val_loss 0.414 val_auc_score 0.642\n",
      "----End of step 0:01:14.316990\n",
      "train_loss 0.398 val_loss 0.414 val_auc_score 0.656\n",
      "----End of step 0:01:14.239323\n",
      "train_loss 0.394 val_loss 0.408 val_auc_score 0.667\n",
      "----End of step 0:01:15.144423\n",
      "train_loss 0.390 val_loss 0.404 val_auc_score 0.672\n",
      "----End of step 0:01:14.997035\n",
      "train_loss 0.387 val_loss 0.404 val_auc_score 0.672\n",
      "----End of step 0:01:15.164170\n",
      "train_loss 0.384 val_loss 0.403 val_auc_score 0.677\n",
      "----End of step 0:01:14.804149\n",
      "train_loss 0.382 val_loss 0.406 val_auc_score 0.681\n",
      "----End of step 0:01:14.750330\n",
      "train_loss 0.380 val_loss 0.398 val_auc_score 0.685\n",
      "----End of step 0:01:14.491967\n",
      "train_loss 0.378 val_loss 0.399 val_auc_score 0.684\n",
      "----End of step 0:01:15.062093\n",
      "train_loss 0.377 val_loss 0.398 val_auc_score 0.686\n",
      "----End of step 0:01:15.088921\n",
      "train_loss 0.376 val_loss 0.399 val_auc_score 0.686\n",
      "----End of step 0:01:14.798495\n",
      "train_loss 0.375 val_loss 0.400 val_auc_score 0.685\n",
      "----End of step 0:01:14.663636\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.426 val_loss 0.428 val_auc_score 0.602\n",
      "----End of step 0:01:08.556107\n",
      "train_loss 0.414 val_loss 0.427 val_auc_score 0.618\n",
      "----End of step 0:01:08.243149\n",
      "train_loss 0.409 val_loss 0.419 val_auc_score 0.632\n",
      "----End of step 0:01:08.573769\n",
      "train_loss 0.403 val_loss 0.412 val_auc_score 0.647\n",
      "----End of step 0:01:08.489701\n",
      "train_loss 0.397 val_loss 0.408 val_auc_score 0.657\n",
      "----End of step 0:01:08.692653\n",
      "train_loss 0.393 val_loss 0.409 val_auc_score 0.662\n",
      "----End of step 0:01:08.747011\n",
      "train_loss 0.389 val_loss 0.404 val_auc_score 0.673\n",
      "----End of step 0:01:08.494235\n",
      "train_loss 0.386 val_loss 0.402 val_auc_score 0.672\n",
      "----End of step 0:01:08.803943\n",
      "train_loss 0.383 val_loss 0.402 val_auc_score 0.679\n",
      "----End of step 0:01:08.249829\n",
      "train_loss 0.382 val_loss 0.398 val_auc_score 0.678\n",
      "----End of step 0:01:08.482162\n",
      "train_loss 0.379 val_loss 0.402 val_auc_score 0.678\n",
      "----End of step 0:01:08.768794\n",
      "train_loss 0.378 val_loss 0.400 val_auc_score 0.684\n",
      "----End of step 0:01:08.557583\n",
      "train_loss 0.377 val_loss 0.400 val_auc_score 0.683\n",
      "----End of step 0:01:08.374444\n",
      "train_loss 0.376 val_loss 0.399 val_auc_score 0.684\n",
      "----End of step 0:01:08.378007\n",
      "train_loss 0.375 val_loss 0.399 val_auc_score 0.684\n",
      "----End of step 0:01:08.623296\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 4.000\n",
      "train_loss 0.424 val_loss 0.441 val_auc_score 0.596\n",
      "----End of step 0:01:04.491665\n",
      "train_loss 0.414 val_loss 0.443 val_auc_score 0.612\n",
      "----End of step 0:01:05.052925\n",
      "train_loss 0.410 val_loss 0.419 val_auc_score 0.624\n",
      "----End of step 0:01:04.710287\n",
      "train_loss 0.404 val_loss 0.421 val_auc_score 0.640\n",
      "----End of step 0:01:05.146601\n",
      "train_loss 0.400 val_loss 0.417 val_auc_score 0.654\n",
      "----End of step 0:01:04.712594\n",
      "train_loss 0.396 val_loss 0.407 val_auc_score 0.657\n",
      "----End of step 0:01:04.676776\n",
      "train_loss 0.393 val_loss 0.408 val_auc_score 0.665\n",
      "----End of step 0:01:04.880890\n",
      "train_loss 0.390 val_loss 0.409 val_auc_score 0.664\n",
      "----End of step 0:01:04.703259\n",
      "train_loss 0.389 val_loss 0.405 val_auc_score 0.671\n",
      "----End of step 0:01:04.515595\n",
      "train_loss 0.386 val_loss 0.406 val_auc_score 0.671\n",
      "----End of step 0:01:06.299570\n",
      "train_loss 0.385 val_loss 0.403 val_auc_score 0.671\n",
      "----End of step 0:01:07.110225\n",
      "train_loss 0.383 val_loss 0.406 val_auc_score 0.673\n",
      "----End of step 0:01:06.754339\n",
      "train_loss 0.382 val_loss 0.407 val_auc_score 0.672\n",
      "----End of step 0:01:07.567373\n",
      "train_loss 0.381 val_loss 0.406 val_auc_score 0.672\n",
      "----End of step 0:01:05.951007\n",
      "train_loss 0.381 val_loss 0.406 val_auc_score 0.672\n",
      "----End of step 0:01:04.927865\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 7.000\n",
      "train_loss 0.425 val_loss 0.431 val_auc_score 0.582\n",
      "----End of step 0:01:05.301941\n",
      "train_loss 0.416 val_loss 0.431 val_auc_score 0.598\n",
      "----End of step 0:01:08.161657\n",
      "train_loss 0.413 val_loss 0.426 val_auc_score 0.617\n",
      "----End of step 0:01:05.793058\n",
      "train_loss 0.406 val_loss 0.421 val_auc_score 0.623\n",
      "----End of step 0:01:06.414591\n",
      "train_loss 0.401 val_loss 0.418 val_auc_score 0.639\n",
      "----End of step 0:01:07.353646\n",
      "train_loss 0.397 val_loss 0.411 val_auc_score 0.643\n",
      "----End of step 0:01:07.931243\n",
      "train_loss 0.394 val_loss 0.419 val_auc_score 0.649\n",
      "----End of step 0:01:08.253039\n",
      "train_loss 0.392 val_loss 0.406 val_auc_score 0.665\n",
      "----End of step 0:01:09.569059\n",
      "train_loss 0.388 val_loss 0.406 val_auc_score 0.670\n",
      "----End of step 0:01:09.206021\n",
      "train_loss 0.386 val_loss 0.404 val_auc_score 0.667\n",
      "----End of step 0:01:07.126609\n",
      "train_loss 0.384 val_loss 0.403 val_auc_score 0.670\n",
      "----End of step 0:01:07.204172\n",
      "train_loss 0.383 val_loss 0.403 val_auc_score 0.674\n",
      "----End of step 0:01:07.077897\n",
      "train_loss 0.382 val_loss 0.400 val_auc_score 0.676\n",
      "----End of step 0:01:07.610637\n",
      "train_loss 0.381 val_loss 0.401 val_auc_score 0.674\n",
      "----End of step 0:01:06.999752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.381 val_loss 0.401 val_auc_score 0.674\n",
      "----End of step 0:01:07.062598\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 6.000\n",
      "train_loss 0.427 val_loss 0.434 val_auc_score 0.582\n",
      "----End of step 0:01:05.373491\n",
      "train_loss 0.416 val_loss 0.428 val_auc_score 0.604\n",
      "----End of step 0:01:06.171330\n",
      "train_loss 0.414 val_loss 0.431 val_auc_score 0.611\n",
      "----End of step 0:01:04.167744\n",
      "train_loss 0.409 val_loss 0.419 val_auc_score 0.622\n",
      "----End of step 0:01:04.472322\n",
      "train_loss 0.405 val_loss 0.422 val_auc_score 0.627\n",
      "----End of step 0:01:05.650919\n",
      "train_loss 0.400 val_loss 0.414 val_auc_score 0.643\n",
      "----End of step 0:01:04.360229\n",
      "train_loss 0.397 val_loss 0.436 val_auc_score 0.646\n",
      "----End of step 0:01:04.682219\n",
      "train_loss 0.394 val_loss 0.412 val_auc_score 0.649\n",
      "----End of step 0:01:05.306403\n",
      "train_loss 0.392 val_loss 0.418 val_auc_score 0.654\n",
      "----End of step 0:01:06.985488\n",
      "train_loss 0.390 val_loss 0.407 val_auc_score 0.660\n",
      "----End of step 0:01:07.326324\n",
      "train_loss 0.388 val_loss 0.412 val_auc_score 0.660\n",
      "----End of step 0:01:09.578168\n",
      "train_loss 0.387 val_loss 0.411 val_auc_score 0.664\n",
      "----End of step 0:01:09.977061\n",
      "train_loss 0.386 val_loss 0.410 val_auc_score 0.665\n",
      "----End of step 0:01:07.315807\n",
      "train_loss 0.385 val_loss 0.410 val_auc_score 0.665\n",
      "----End of step 0:01:04.887303\n",
      "train_loss 0.384 val_loss 0.409 val_auc_score 0.665\n",
      "----End of step 0:01:04.452855\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 5.000\n",
      "train_loss 0.427 val_loss 0.435 val_auc_score 0.593\n",
      "----End of step 0:01:06.909399\n",
      "train_loss 0.415 val_loss 0.432 val_auc_score 0.603\n",
      "----End of step 0:01:04.676486\n",
      "train_loss 0.412 val_loss 0.420 val_auc_score 0.624\n",
      "----End of step 0:01:05.265251\n",
      "train_loss 0.407 val_loss 0.420 val_auc_score 0.633\n",
      "----End of step 0:01:04.784061\n",
      "train_loss 0.403 val_loss 0.416 val_auc_score 0.642\n",
      "----End of step 0:01:02.802742\n",
      "train_loss 0.400 val_loss 0.417 val_auc_score 0.644\n",
      "----End of step 0:01:03.210961\n",
      "train_loss 0.397 val_loss 0.412 val_auc_score 0.653\n",
      "----End of step 0:01:03.097041\n",
      "train_loss 0.394 val_loss 0.409 val_auc_score 0.659\n",
      "----End of step 0:01:03.229112\n",
      "train_loss 0.392 val_loss 0.406 val_auc_score 0.663\n",
      "----End of step 0:01:03.455617\n",
      "train_loss 0.390 val_loss 0.404 val_auc_score 0.666\n",
      "----End of step 0:01:02.972995\n",
      "train_loss 0.388 val_loss 0.406 val_auc_score 0.667\n",
      "----End of step 0:01:02.483183\n",
      "train_loss 0.387 val_loss 0.406 val_auc_score 0.670\n",
      "----End of step 0:01:03.001111\n",
      "train_loss 0.386 val_loss 0.406 val_auc_score 0.670\n",
      "----End of step 0:01:03.254624\n",
      "train_loss 0.385 val_loss 0.405 val_auc_score 0.669\n",
      "----End of step 0:01:03.138931\n",
      "train_loss 0.385 val_loss 0.405 val_auc_score 0.670\n",
      "----End of step 0:01:03.172546\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 6.000\n",
      "train_loss 0.426 val_loss 0.428 val_auc_score 0.589\n",
      "----End of step 0:01:03.817787\n",
      "train_loss 0.415 val_loss 0.434 val_auc_score 0.597\n",
      "----End of step 0:01:03.746042\n",
      "train_loss 0.412 val_loss 0.425 val_auc_score 0.615\n",
      "----End of step 0:01:03.757502\n",
      "train_loss 0.406 val_loss 0.418 val_auc_score 0.628\n",
      "----End of step 0:01:03.343254\n",
      "train_loss 0.402 val_loss 0.417 val_auc_score 0.639\n",
      "----End of step 0:01:03.646998\n",
      "train_loss 0.398 val_loss 0.417 val_auc_score 0.651\n",
      "----End of step 0:01:03.435422\n",
      "train_loss 0.394 val_loss 0.411 val_auc_score 0.656\n",
      "----End of step 0:01:03.132991\n",
      "train_loss 0.391 val_loss 0.412 val_auc_score 0.661\n",
      "----End of step 0:01:03.314708\n",
      "train_loss 0.389 val_loss 0.406 val_auc_score 0.664\n",
      "----End of step 0:01:03.428334\n",
      "train_loss 0.386 val_loss 0.405 val_auc_score 0.666\n",
      "----End of step 0:01:04.018212\n",
      "train_loss 0.384 val_loss 0.406 val_auc_score 0.669\n",
      "----End of step 0:01:04.063316\n",
      "train_loss 0.383 val_loss 0.405 val_auc_score 0.671\n",
      "----End of step 0:01:03.428719\n",
      "train_loss 0.381 val_loss 0.406 val_auc_score 0.673\n",
      "----End of step 0:01:03.558595\n",
      "train_loss 0.381 val_loss 0.404 val_auc_score 0.672\n",
      "----End of step 0:01:03.777054\n",
      "train_loss 0.381 val_loss 0.405 val_auc_score 0.672\n",
      "----End of step 0:01:03.265808\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 5.000\n",
      "train_loss 0.424 val_loss 0.430 val_auc_score 0.589\n",
      "----End of step 0:01:03.405911\n",
      "train_loss 0.415 val_loss 0.429 val_auc_score 0.607\n",
      "----End of step 0:01:03.295649\n",
      "train_loss 0.411 val_loss 0.442 val_auc_score 0.619\n",
      "----End of step 0:01:03.292288\n",
      "train_loss 0.405 val_loss 0.415 val_auc_score 0.640\n",
      "----End of step 0:01:02.773629\n",
      "train_loss 0.400 val_loss 0.413 val_auc_score 0.646\n",
      "----End of step 0:01:02.933227\n",
      "train_loss 0.396 val_loss 0.408 val_auc_score 0.662\n",
      "----End of step 0:01:03.107041\n",
      "train_loss 0.392 val_loss 0.407 val_auc_score 0.665\n",
      "----End of step 0:01:03.624158\n",
      "train_loss 0.389 val_loss 0.402 val_auc_score 0.671\n",
      "----End of step 0:01:04.368535\n",
      "train_loss 0.387 val_loss 0.403 val_auc_score 0.671\n",
      "----End of step 0:01:04.850353\n",
      "train_loss 0.385 val_loss 0.402 val_auc_score 0.673\n",
      "----End of step 0:01:04.095212\n",
      "train_loss 0.384 val_loss 0.403 val_auc_score 0.675\n",
      "----End of step 0:01:04.241530\n",
      "train_loss 0.382 val_loss 0.402 val_auc_score 0.676\n",
      "----End of step 0:01:02.760574\n",
      "train_loss 0.382 val_loss 0.402 val_auc_score 0.676\n",
      "----End of step 0:01:02.773418\n",
      "train_loss 0.381 val_loss 0.403 val_auc_score 0.676\n",
      "----End of step 0:01:02.586261\n",
      "train_loss 0.380 val_loss 0.402 val_auc_score 0.677\n",
      "----End of step 0:01:03.006633\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 4.000\n",
      "train_loss 0.424 val_loss 0.425 val_auc_score 0.594\n",
      "----End of step 0:01:03.877211\n",
      "train_loss 0.414 val_loss 0.428 val_auc_score 0.604\n",
      "----End of step 0:01:02.896640\n",
      "train_loss 0.411 val_loss 0.423 val_auc_score 0.619\n",
      "----End of step 0:01:03.004630\n",
      "train_loss 0.406 val_loss 0.418 val_auc_score 0.629\n",
      "----End of step 0:01:02.837743\n",
      "train_loss 0.403 val_loss 0.422 val_auc_score 0.641\n",
      "----End of step 0:01:02.840887\n",
      "train_loss 0.399 val_loss 0.427 val_auc_score 0.642\n",
      "----End of step 0:01:03.279172\n",
      "train_loss 0.396 val_loss 0.411 val_auc_score 0.649\n",
      "----End of step 0:01:05.450815\n",
      "train_loss 0.394 val_loss 0.410 val_auc_score 0.658\n",
      "----End of step 0:01:06.349558\n",
      "train_loss 0.392 val_loss 0.420 val_auc_score 0.654\n",
      "----End of step 0:01:05.092809\n",
      "train_loss 0.390 val_loss 0.416 val_auc_score 0.655\n",
      "----End of step 0:01:05.028215\n",
      "train_loss 0.388 val_loss 0.411 val_auc_score 0.660\n",
      "----End of step 0:01:05.879773\n",
      "train_loss 0.387 val_loss 0.415 val_auc_score 0.662\n",
      "----End of step 0:01:05.595956\n",
      "train_loss 0.385 val_loss 0.412 val_auc_score 0.661\n",
      "----End of step 0:01:05.504262\n",
      "train_loss 0.386 val_loss 0.414 val_auc_score 0.661\n",
      "----End of step 0:01:07.643651\n",
      "train_loss 0.385 val_loss 0.413 val_auc_score 0.662\n",
      "----End of step 0:01:08.835994\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 7.000\n",
      "train_loss 0.426 val_loss 0.431 val_auc_score 0.576\n",
      "----End of step 0:01:13.631911\n",
      "train_loss 0.416 val_loss 0.431 val_auc_score 0.600\n",
      "----End of step 0:01:14.622324\n",
      "train_loss 0.415 val_loss 0.427 val_auc_score 0.615\n",
      "----End of step 0:01:13.030790\n",
      "train_loss 0.411 val_loss 0.425 val_auc_score 0.620\n",
      "----End of step 0:01:13.333340\n",
      "train_loss 0.407 val_loss 0.427 val_auc_score 0.623\n",
      "----End of step 0:01:13.734768\n",
      "train_loss 0.403 val_loss 0.420 val_auc_score 0.633\n",
      "----End of step 0:01:12.379080\n",
      "train_loss 0.401 val_loss 0.422 val_auc_score 0.637\n",
      "----End of step 0:01:13.833341\n",
      "train_loss 0.398 val_loss 0.421 val_auc_score 0.641\n",
      "----End of step 0:01:16.926009\n",
      "train_loss 0.396 val_loss 0.427 val_auc_score 0.644\n",
      "----End of step 0:01:13.488277\n",
      "train_loss 0.394 val_loss 0.419 val_auc_score 0.645\n",
      "----End of step 0:01:20.314298\n",
      "train_loss 0.393 val_loss 0.423 val_auc_score 0.649\n",
      "----End of step 0:01:16.669748\n",
      "train_loss 0.392 val_loss 0.419 val_auc_score 0.650\n",
      "----End of step 0:01:20.116410\n",
      "train_loss 0.391 val_loss 0.419 val_auc_score 0.651\n",
      "----End of step 0:01:17.649807\n",
      "train_loss 0.390 val_loss 0.419 val_auc_score 0.652\n",
      "----End of step 0:01:16.525485\n",
      "train_loss 0.390 val_loss 0.418 val_auc_score 0.652\n",
      "----End of step 0:01:17.406075\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 6.000\n",
      "train_loss 0.426 val_loss 0.435 val_auc_score 0.578\n",
      "----End of step 0:01:12.437394\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for w in widths:\n",
    "    for d in depths:\n",
    "        d_s = sum(j[1] for i in d for j in i)\n",
    "        print('width multiplier - %.3f depth multiplier - %.3f' % (w, d_s))\n",
    "        model = resnet18(num_classes=5, block=depthwise_block, width_mult=w, \n",
    "                         inverted_residual_setting1=d[0], \n",
    "                         inverted_residual_setting2=d[1]).cuda()\n",
    "        \n",
    "        p = sum(p.numel() for p in model.parameters())\n",
    "        optimizer = create_optimizer(model, 0.001)\n",
    "        score, t = train_triangular_policy(model, optimizer, train_loader, valid_loader, valid_dataset,\n",
    "                                       loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                                       dataset='chexpert', binary=False, max_lr=0.001, epochs=15)\n",
    "        \n",
    "        p = \"/home/rimmanni/Medical_Images/Scaling_experiments/Chexpert/ResDepth_\" + str(w) + '_' + str(depths.index(d))\n",
    "        save_model(model, p)\n",
    "        data.append([w, d_s, score, p, t])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['width_x', 'depth_x', 'val_score', 'params', 'time_per_epoch']\n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"chexpert_resnet_depthwise_13.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.read_csv('chexpert_resnet_depthwise_13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
