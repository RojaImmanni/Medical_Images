{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../prepare_data.py\n",
    "%run ../../architectures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, valid_dataset = mura_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 250, 200]), torch.Size([32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(block=depthwise_block).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728193"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.8 s, sys: 6.26 s, total: 45.1 s\n",
      "Wall time: 45.4 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "lrs, losses = LR_range_finder(model, train_loader, \n",
    "                              loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                              binary=True, lr_high=0.05)\n",
    "plot_lr(lrs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [1.0, 0.75, 0.5, 0.25]\n",
    "depths = [[[[64, 2], [128, 2]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 2], [128, 2]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 1], [512, 1]]],\n",
    "          [[[64, 2], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 2], [512, 1]]],\n",
    "          [[[64, 1], [128, 1]], [[256, 1], [512, 1]]],\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width multiplier - 1.000 depth multiplier - 7.000\n",
      "train_loss 0.663 val_loss 0.743 val_auc_score 0.652\n",
      "----End of step 0:00:48.942714\n",
      "train_loss 0.625 val_loss 0.641 val_auc_score 0.719\n",
      "----End of step 0:00:49.950085\n",
      "train_loss 0.607 val_loss 0.625 val_auc_score 0.725\n",
      "----End of step 0:00:50.449320\n",
      "train_loss 0.588 val_loss 0.632 val_auc_score 0.753\n",
      "----End of step 0:00:48.432212\n",
      "train_loss 0.572 val_loss 0.590 val_auc_score 0.771\n",
      "----End of step 0:00:48.221793\n",
      "train_loss 0.563 val_loss 0.685 val_auc_score 0.776\n",
      "----End of step 0:00:49.519692\n",
      "train_loss 0.550 val_loss 0.597 val_auc_score 0.803\n",
      "----End of step 0:00:50.123596\n",
      "train_loss 0.538 val_loss 0.576 val_auc_score 0.778\n",
      "----End of step 0:00:49.862369\n",
      "train_loss 0.526 val_loss 0.544 val_auc_score 0.820\n",
      "----End of step 0:00:50.812632\n",
      "train_loss 0.518 val_loss 0.553 val_auc_score 0.827\n",
      "----End of step 0:00:49.890737\n",
      "train_loss 0.508 val_loss 0.524 val_auc_score 0.830\n",
      "----End of step 0:00:50.530207\n",
      "train_loss 0.497 val_loss 0.521 val_auc_score 0.834\n",
      "----End of step 0:00:51.046413\n",
      "train_loss 0.489 val_loss 0.514 val_auc_score 0.842\n",
      "----End of step 0:00:48.782252\n",
      "train_loss 0.482 val_loss 0.514 val_auc_score 0.843\n",
      "----End of step 0:00:49.451624\n",
      "train_loss 0.478 val_loss 0.512 val_auc_score 0.843\n",
      "----End of step 0:00:51.636009\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.667 val_loss 0.712 val_auc_score 0.659\n",
      "----End of step 0:00:48.936312\n",
      "train_loss 0.632 val_loss 0.667 val_auc_score 0.681\n",
      "----End of step 0:00:47.840438\n",
      "train_loss 0.608 val_loss 0.641 val_auc_score 0.728\n",
      "----End of step 0:00:47.795406\n",
      "train_loss 0.588 val_loss 0.618 val_auc_score 0.745\n",
      "----End of step 0:00:45.603467\n",
      "train_loss 0.571 val_loss 0.592 val_auc_score 0.775\n",
      "----End of step 0:00:44.710628\n",
      "train_loss 0.560 val_loss 0.614 val_auc_score 0.794\n",
      "----End of step 0:00:47.775374\n",
      "train_loss 0.548 val_loss 0.567 val_auc_score 0.804\n",
      "----End of step 0:00:45.975977\n",
      "train_loss 0.539 val_loss 0.548 val_auc_score 0.813\n",
      "----End of step 0:00:45.615367\n",
      "train_loss 0.529 val_loss 0.537 val_auc_score 0.824\n",
      "----End of step 0:00:45.090681\n",
      "train_loss 0.520 val_loss 0.528 val_auc_score 0.836\n",
      "----End of step 0:00:44.400825\n",
      "train_loss 0.510 val_loss 0.536 val_auc_score 0.833\n",
      "----End of step 0:00:43.667376\n",
      "train_loss 0.499 val_loss 0.524 val_auc_score 0.844\n",
      "----End of step 0:00:43.595871\n",
      "train_loss 0.489 val_loss 0.513 val_auc_score 0.845\n",
      "----End of step 0:00:43.981808\n",
      "train_loss 0.485 val_loss 0.512 val_auc_score 0.844\n",
      "----End of step 0:00:42.648799\n",
      "train_loss 0.481 val_loss 0.512 val_auc_score 0.847\n",
      "----End of step 0:00:45.084963\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.666 val_loss 0.674 val_auc_score 0.667\n",
      "----End of step 0:00:42.287432\n",
      "train_loss 0.632 val_loss 0.710 val_auc_score 0.683\n",
      "----End of step 0:00:41.369986\n",
      "train_loss 0.611 val_loss 0.621 val_auc_score 0.727\n",
      "----End of step 0:00:40.905326\n",
      "train_loss 0.591 val_loss 0.663 val_auc_score 0.763\n",
      "----End of step 0:00:40.813776\n",
      "train_loss 0.576 val_loss 0.576 val_auc_score 0.784\n",
      "----End of step 0:00:42.479745\n",
      "train_loss 0.562 val_loss 0.580 val_auc_score 0.788\n",
      "----End of step 0:00:40.556387\n",
      "train_loss 0.555 val_loss 0.598 val_auc_score 0.806\n",
      "----End of step 0:00:42.467099\n",
      "train_loss 0.541 val_loss 0.566 val_auc_score 0.804\n",
      "----End of step 0:00:41.259680\n",
      "train_loss 0.532 val_loss 0.541 val_auc_score 0.810\n",
      "----End of step 0:00:41.030056\n",
      "train_loss 0.524 val_loss 0.529 val_auc_score 0.820\n",
      "----End of step 0:00:42.144885\n",
      "train_loss 0.514 val_loss 0.536 val_auc_score 0.824\n",
      "----End of step 0:00:41.138162\n",
      "train_loss 0.505 val_loss 0.522 val_auc_score 0.834\n",
      "----End of step 0:00:40.832911\n",
      "train_loss 0.495 val_loss 0.524 val_auc_score 0.838\n",
      "----End of step 0:00:40.072679\n",
      "train_loss 0.492 val_loss 0.523 val_auc_score 0.837\n",
      "----End of step 0:00:41.915422\n",
      "train_loss 0.488 val_loss 0.525 val_auc_score 0.837\n",
      "----End of step 0:00:41.040878\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 6.000\n",
      "train_loss 0.664 val_loss 0.689 val_auc_score 0.625\n",
      "----End of step 0:00:43.168213\n",
      "train_loss 0.630 val_loss 0.715 val_auc_score 0.704\n",
      "----End of step 0:00:43.727781\n",
      "train_loss 0.613 val_loss 0.670 val_auc_score 0.709\n",
      "----End of step 0:00:44.485718\n",
      "train_loss 0.589 val_loss 0.618 val_auc_score 0.747\n",
      "----End of step 0:00:42.878169\n",
      "train_loss 0.574 val_loss 0.617 val_auc_score 0.785\n",
      "----End of step 0:00:43.670352\n",
      "train_loss 0.565 val_loss 0.579 val_auc_score 0.764\n",
      "----End of step 0:00:44.588379\n",
      "train_loss 0.557 val_loss 0.589 val_auc_score 0.798\n",
      "----End of step 0:00:45.134564\n",
      "train_loss 0.545 val_loss 0.574 val_auc_score 0.784\n",
      "----End of step 0:00:44.352192\n",
      "train_loss 0.533 val_loss 0.545 val_auc_score 0.812\n",
      "----End of step 0:00:45.097478\n",
      "train_loss 0.526 val_loss 0.551 val_auc_score 0.817\n",
      "----End of step 0:00:45.378432\n",
      "train_loss 0.516 val_loss 0.525 val_auc_score 0.827\n",
      "----End of step 0:00:44.465751\n",
      "train_loss 0.506 val_loss 0.523 val_auc_score 0.830\n",
      "----End of step 0:00:43.494986\n",
      "train_loss 0.497 val_loss 0.516 val_auc_score 0.836\n",
      "----End of step 0:00:44.964567\n",
      "train_loss 0.491 val_loss 0.518 val_auc_score 0.838\n",
      "----End of step 0:00:45.571943\n",
      "train_loss 0.487 val_loss 0.520 val_auc_score 0.840\n",
      "----End of step 0:00:45.032497\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 5.000\n",
      "train_loss 0.665 val_loss 0.864 val_auc_score 0.626\n",
      "----End of step 0:00:41.655735\n",
      "train_loss 0.628 val_loss 0.654 val_auc_score 0.693\n",
      "----End of step 0:00:41.642992\n",
      "train_loss 0.607 val_loss 0.630 val_auc_score 0.719\n",
      "----End of step 0:00:41.884915\n",
      "train_loss 0.588 val_loss 0.616 val_auc_score 0.756\n",
      "----End of step 0:00:42.355804\n",
      "train_loss 0.577 val_loss 0.625 val_auc_score 0.762\n",
      "----End of step 0:00:42.263697\n",
      "train_loss 0.568 val_loss 0.583 val_auc_score 0.788\n",
      "----End of step 0:00:42.701169\n",
      "train_loss 0.555 val_loss 0.586 val_auc_score 0.778\n",
      "----End of step 0:00:41.938088\n",
      "train_loss 0.549 val_loss 0.572 val_auc_score 0.795\n",
      "----End of step 0:00:42.257838\n",
      "train_loss 0.539 val_loss 0.572 val_auc_score 0.822\n",
      "----End of step 0:00:41.550759\n",
      "train_loss 0.529 val_loss 0.569 val_auc_score 0.824\n",
      "----End of step 0:00:40.731980\n",
      "train_loss 0.519 val_loss 0.538 val_auc_score 0.828\n",
      "----End of step 0:00:42.907212\n",
      "train_loss 0.509 val_loss 0.526 val_auc_score 0.834\n",
      "----End of step 0:00:43.201895\n",
      "train_loss 0.503 val_loss 0.523 val_auc_score 0.832\n",
      "----End of step 0:00:43.673949\n",
      "train_loss 0.496 val_loss 0.526 val_auc_score 0.830\n",
      "----End of step 0:00:43.735135\n",
      "train_loss 0.491 val_loss 0.525 val_auc_score 0.831\n",
      "----End of step 0:00:42.495181\n",
      "\n",
      "width multiplier - 1.000 depth multiplier - 4.000\n",
      "train_loss 0.669 val_loss 0.709 val_auc_score 0.663\n",
      "----End of step 0:00:39.514126\n",
      "train_loss 0.642 val_loss 0.644 val_auc_score 0.696\n",
      "----End of step 0:00:39.890594\n",
      "train_loss 0.621 val_loss 0.703 val_auc_score 0.693\n",
      "----End of step 0:00:40.557991\n",
      "train_loss 0.604 val_loss 0.653 val_auc_score 0.708\n",
      "----End of step 0:00:40.356754\n",
      "train_loss 0.586 val_loss 0.644 val_auc_score 0.740\n",
      "----End of step 0:00:40.762215\n",
      "train_loss 0.575 val_loss 0.682 val_auc_score 0.736\n",
      "----End of step 0:00:37.814270\n",
      "train_loss 0.566 val_loss 0.582 val_auc_score 0.783\n",
      "----End of step 0:00:39.887962\n",
      "train_loss 0.554 val_loss 0.590 val_auc_score 0.761\n",
      "----End of step 0:00:39.684031\n",
      "train_loss 0.543 val_loss 0.564 val_auc_score 0.800\n",
      "----End of step 0:00:39.705689\n",
      "train_loss 0.535 val_loss 0.577 val_auc_score 0.808\n",
      "----End of step 0:00:39.621736\n",
      "train_loss 0.526 val_loss 0.553 val_auc_score 0.805\n",
      "----End of step 0:00:39.040065\n",
      "train_loss 0.518 val_loss 0.534 val_auc_score 0.823\n",
      "----End of step 0:00:40.874728\n",
      "train_loss 0.510 val_loss 0.535 val_auc_score 0.823\n",
      "----End of step 0:00:39.344316\n",
      "train_loss 0.504 val_loss 0.534 val_auc_score 0.826\n",
      "----End of step 0:00:39.683299\n",
      "train_loss 0.503 val_loss 0.537 val_auc_score 0.829\n",
      "----End of step 0:00:39.889220\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 7.000\n",
      "train_loss 0.663 val_loss 0.741 val_auc_score 0.608\n",
      "----End of step 0:00:49.510155\n",
      "train_loss 0.642 val_loss 0.675 val_auc_score 0.668\n",
      "----End of step 0:00:49.501886\n",
      "train_loss 0.616 val_loss 0.622 val_auc_score 0.732\n",
      "----End of step 0:00:48.731200\n",
      "train_loss 0.595 val_loss 0.616 val_auc_score 0.738\n",
      "----End of step 0:00:50.667066\n",
      "train_loss 0.578 val_loss 0.590 val_auc_score 0.770\n",
      "----End of step 0:00:48.622779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.563 val_loss 0.630 val_auc_score 0.775\n",
      "----End of step 0:00:47.795310\n",
      "train_loss 0.551 val_loss 0.559 val_auc_score 0.799\n",
      "----End of step 0:00:49.287246\n",
      "train_loss 0.539 val_loss 0.550 val_auc_score 0.803\n",
      "----End of step 0:00:48.867443\n",
      "train_loss 0.531 val_loss 0.574 val_auc_score 0.804\n",
      "----End of step 0:00:50.728537\n",
      "train_loss 0.520 val_loss 0.551 val_auc_score 0.822\n",
      "----End of step 0:00:49.684449\n",
      "train_loss 0.510 val_loss 0.534 val_auc_score 0.830\n",
      "----End of step 0:00:48.377378\n",
      "train_loss 0.498 val_loss 0.521 val_auc_score 0.838\n",
      "----End of step 0:00:48.649482\n",
      "train_loss 0.491 val_loss 0.507 val_auc_score 0.841\n",
      "----End of step 0:00:50.384732\n",
      "train_loss 0.487 val_loss 0.516 val_auc_score 0.846\n",
      "----End of step 0:00:49.489202\n",
      "train_loss 0.485 val_loss 0.515 val_auc_score 0.845\n",
      "----End of step 0:00:48.804583\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.666 val_loss 0.685 val_auc_score 0.651\n",
      "----End of step 0:00:45.411260\n",
      "train_loss 0.647 val_loss 0.719 val_auc_score 0.679\n",
      "----End of step 0:00:45.433433\n",
      "train_loss 0.623 val_loss 0.669 val_auc_score 0.678\n",
      "----End of step 0:00:44.618347\n",
      "train_loss 0.605 val_loss 0.613 val_auc_score 0.740\n",
      "----End of step 0:00:45.789916\n",
      "train_loss 0.590 val_loss 0.686 val_auc_score 0.716\n",
      "----End of step 0:00:45.083885\n",
      "train_loss 0.580 val_loss 0.595 val_auc_score 0.763\n",
      "----End of step 0:00:46.242970\n",
      "train_loss 0.566 val_loss 0.578 val_auc_score 0.781\n",
      "----End of step 0:00:44.679244\n",
      "train_loss 0.555 val_loss 0.593 val_auc_score 0.771\n",
      "----End of step 0:00:45.100100\n",
      "train_loss 0.544 val_loss 0.582 val_auc_score 0.793\n",
      "----End of step 0:00:46.381670\n",
      "train_loss 0.535 val_loss 0.570 val_auc_score 0.787\n",
      "----End of step 0:00:45.603186\n",
      "train_loss 0.524 val_loss 0.555 val_auc_score 0.798\n",
      "----End of step 0:00:45.770138\n",
      "train_loss 0.517 val_loss 0.539 val_auc_score 0.815\n",
      "----End of step 0:00:45.603494\n",
      "train_loss 0.505 val_loss 0.546 val_auc_score 0.819\n",
      "----End of step 0:00:45.109542\n",
      "train_loss 0.503 val_loss 0.539 val_auc_score 0.825\n",
      "----End of step 0:00:45.517685\n",
      "train_loss 0.502 val_loss 0.543 val_auc_score 0.823\n",
      "----End of step 0:00:45.342947\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.666 val_loss 0.674 val_auc_score 0.654\n",
      "----End of step 0:00:43.638710\n",
      "train_loss 0.642 val_loss 0.703 val_auc_score 0.668\n",
      "----End of step 0:00:41.919245\n",
      "train_loss 0.619 val_loss 0.638 val_auc_score 0.694\n",
      "----End of step 0:00:43.530521\n",
      "train_loss 0.602 val_loss 0.658 val_auc_score 0.727\n",
      "----End of step 0:00:42.690261\n",
      "train_loss 0.588 val_loss 0.657 val_auc_score 0.747\n",
      "----End of step 0:00:43.658913\n",
      "train_loss 0.580 val_loss 0.585 val_auc_score 0.766\n",
      "----End of step 0:00:42.571360\n",
      "train_loss 0.569 val_loss 0.584 val_auc_score 0.768\n",
      "----End of step 0:00:42.160506\n",
      "train_loss 0.561 val_loss 0.564 val_auc_score 0.795\n",
      "----End of step 0:00:41.596813\n",
      "train_loss 0.551 val_loss 0.557 val_auc_score 0.790\n",
      "----End of step 0:00:43.277445\n",
      "train_loss 0.541 val_loss 0.563 val_auc_score 0.796\n",
      "----End of step 0:00:41.880134\n",
      "train_loss 0.530 val_loss 0.545 val_auc_score 0.807\n",
      "----End of step 0:00:41.666114\n",
      "train_loss 0.524 val_loss 0.546 val_auc_score 0.818\n",
      "----End of step 0:00:41.593859\n",
      "train_loss 0.517 val_loss 0.543 val_auc_score 0.822\n",
      "----End of step 0:00:41.654554\n",
      "train_loss 0.510 val_loss 0.536 val_auc_score 0.823\n",
      "----End of step 0:00:42.062921\n",
      "train_loss 0.506 val_loss 0.540 val_auc_score 0.827\n",
      "----End of step 0:00:42.173493\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 6.000\n",
      "train_loss 0.664 val_loss 0.773 val_auc_score 0.652\n",
      "----End of step 0:00:45.926474\n",
      "train_loss 0.636 val_loss 0.660 val_auc_score 0.665\n",
      "----End of step 0:00:46.273843\n",
      "train_loss 0.615 val_loss 0.637 val_auc_score 0.709\n",
      "----End of step 0:00:45.198606\n",
      "train_loss 0.596 val_loss 0.618 val_auc_score 0.737\n",
      "----End of step 0:00:46.264523\n",
      "train_loss 0.583 val_loss 0.593 val_auc_score 0.765\n",
      "----End of step 0:00:45.713526\n",
      "train_loss 0.575 val_loss 0.612 val_auc_score 0.767\n",
      "----End of step 0:00:44.368948\n",
      "train_loss 0.562 val_loss 0.575 val_auc_score 0.787\n",
      "----End of step 0:00:45.112158\n",
      "train_loss 0.553 val_loss 0.576 val_auc_score 0.787\n",
      "----End of step 0:00:45.732556\n",
      "train_loss 0.541 val_loss 0.546 val_auc_score 0.807\n",
      "----End of step 0:00:45.705484\n",
      "train_loss 0.535 val_loss 0.558 val_auc_score 0.805\n",
      "----End of step 0:00:46.570840\n",
      "train_loss 0.526 val_loss 0.553 val_auc_score 0.797\n",
      "----End of step 0:00:46.577302\n",
      "train_loss 0.516 val_loss 0.559 val_auc_score 0.825\n",
      "----End of step 0:00:46.142618\n",
      "train_loss 0.509 val_loss 0.540 val_auc_score 0.818\n",
      "----End of step 0:00:45.933729\n",
      "train_loss 0.504 val_loss 0.527 val_auc_score 0.828\n",
      "----End of step 0:00:46.627164\n",
      "train_loss 0.499 val_loss 0.532 val_auc_score 0.826\n",
      "----End of step 0:00:45.882268\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 5.000\n",
      "train_loss 0.662 val_loss 0.702 val_auc_score 0.677\n",
      "----End of step 0:00:43.123109\n",
      "train_loss 0.636 val_loss 0.667 val_auc_score 0.670\n",
      "----End of step 0:00:44.295953\n",
      "train_loss 0.619 val_loss 0.647 val_auc_score 0.706\n",
      "----End of step 0:00:43.872736\n",
      "train_loss 0.602 val_loss 0.626 val_auc_score 0.713\n",
      "----End of step 0:00:43.005850\n",
      "train_loss 0.586 val_loss 0.606 val_auc_score 0.741\n",
      "----End of step 0:00:43.004325\n",
      "train_loss 0.575 val_loss 0.596 val_auc_score 0.772\n",
      "----End of step 0:00:42.286784\n",
      "train_loss 0.567 val_loss 0.601 val_auc_score 0.750\n",
      "----End of step 0:00:43.491131\n",
      "train_loss 0.559 val_loss 0.610 val_auc_score 0.789\n",
      "----End of step 0:00:41.344968\n",
      "train_loss 0.547 val_loss 0.564 val_auc_score 0.785\n",
      "----End of step 0:00:42.535191\n",
      "train_loss 0.538 val_loss 0.577 val_auc_score 0.774\n",
      "----End of step 0:00:42.685465\n",
      "train_loss 0.529 val_loss 0.567 val_auc_score 0.786\n",
      "----End of step 0:00:42.361334\n",
      "train_loss 0.520 val_loss 0.544 val_auc_score 0.820\n",
      "----End of step 0:00:41.965261\n",
      "train_loss 0.514 val_loss 0.548 val_auc_score 0.815\n",
      "----End of step 0:00:42.404287\n",
      "train_loss 0.506 val_loss 0.539 val_auc_score 0.820\n",
      "----End of step 0:00:42.813599\n",
      "train_loss 0.503 val_loss 0.540 val_auc_score 0.821\n",
      "----End of step 0:00:42.683851\n",
      "\n",
      "width multiplier - 0.750 depth multiplier - 4.000\n",
      "train_loss 0.666 val_loss 0.712 val_auc_score 0.623\n",
      "----End of step 0:00:41.253250\n",
      "train_loss 0.650 val_loss 0.683 val_auc_score 0.663\n",
      "----End of step 0:00:39.688186\n",
      "train_loss 0.633 val_loss 0.643 val_auc_score 0.710\n",
      "----End of step 0:00:39.239355\n",
      "train_loss 0.613 val_loss 0.684 val_auc_score 0.695\n",
      "----End of step 0:00:40.518291\n",
      "train_loss 0.598 val_loss 0.639 val_auc_score 0.744\n",
      "----End of step 0:00:40.121243\n",
      "train_loss 0.586 val_loss 0.608 val_auc_score 0.760\n",
      "----End of step 0:00:41.290549\n",
      "train_loss 0.575 val_loss 0.584 val_auc_score 0.775\n",
      "----End of step 0:00:40.244966\n",
      "train_loss 0.567 val_loss 0.592 val_auc_score 0.781\n",
      "----End of step 0:00:40.525066\n",
      "train_loss 0.555 val_loss 0.601 val_auc_score 0.794\n",
      "----End of step 0:00:40.174727\n",
      "train_loss 0.546 val_loss 0.567 val_auc_score 0.803\n",
      "----End of step 0:00:41.198325\n",
      "train_loss 0.539 val_loss 0.578 val_auc_score 0.810\n",
      "----End of step 0:00:41.323612\n",
      "train_loss 0.529 val_loss 0.567 val_auc_score 0.810\n",
      "----End of step 0:00:40.677641\n",
      "train_loss 0.523 val_loss 0.569 val_auc_score 0.811\n",
      "----End of step 0:00:41.166878\n",
      "train_loss 0.519 val_loss 0.569 val_auc_score 0.808\n",
      "----End of step 0:00:40.543520\n",
      "train_loss 0.515 val_loss 0.561 val_auc_score 0.812\n",
      "----End of step 0:00:40.700129\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 7.000\n",
      "train_loss 0.666 val_loss 0.732 val_auc_score 0.631\n",
      "----End of step 0:00:48.767559\n",
      "train_loss 0.642 val_loss 0.761 val_auc_score 0.666\n",
      "----End of step 0:00:49.026192\n",
      "train_loss 0.616 val_loss 0.659 val_auc_score 0.676\n",
      "----End of step 0:00:47.907287\n",
      "train_loss 0.598 val_loss 0.662 val_auc_score 0.709\n",
      "----End of step 0:00:49.579212\n",
      "train_loss 0.582 val_loss 0.607 val_auc_score 0.755\n",
      "----End of step 0:00:49.520067\n",
      "train_loss 0.570 val_loss 0.579 val_auc_score 0.775\n",
      "----End of step 0:00:48.837574\n",
      "train_loss 0.562 val_loss 0.588 val_auc_score 0.799\n",
      "----End of step 0:00:48.933627\n",
      "train_loss 0.552 val_loss 0.569 val_auc_score 0.778\n",
      "----End of step 0:00:50.136990\n",
      "train_loss 0.544 val_loss 0.573 val_auc_score 0.813\n",
      "----End of step 0:00:49.996990\n",
      "train_loss 0.535 val_loss 0.546 val_auc_score 0.806\n",
      "----End of step 0:00:48.318009\n",
      "train_loss 0.526 val_loss 0.561 val_auc_score 0.820\n",
      "----End of step 0:00:48.812804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.516 val_loss 0.539 val_auc_score 0.820\n",
      "----End of step 0:00:47.518147\n",
      "train_loss 0.510 val_loss 0.528 val_auc_score 0.832\n",
      "----End of step 0:00:48.638329\n",
      "train_loss 0.502 val_loss 0.532 val_auc_score 0.832\n",
      "----End of step 0:00:49.843111\n",
      "train_loss 0.500 val_loss 0.526 val_auc_score 0.832\n",
      "----End of step 0:00:49.373051\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 6.000\n",
      "train_loss 0.667 val_loss 0.705 val_auc_score 0.619\n",
      "----End of step 0:00:46.955128\n",
      "train_loss 0.651 val_loss 0.703 val_auc_score 0.670\n",
      "----End of step 0:00:46.589786\n",
      "train_loss 0.625 val_loss 0.660 val_auc_score 0.679\n",
      "----End of step 0:00:47.817056\n",
      "train_loss 0.600 val_loss 0.611 val_auc_score 0.741\n",
      "----End of step 0:00:48.394271\n",
      "train_loss 0.585 val_loss 0.611 val_auc_score 0.781\n",
      "----End of step 0:00:47.133645\n",
      "train_loss 0.575 val_loss 0.600 val_auc_score 0.755\n",
      "----End of step 0:00:48.381218\n",
      "train_loss 0.563 val_loss 0.574 val_auc_score 0.787\n",
      "----End of step 0:00:47.322674\n",
      "train_loss 0.554 val_loss 0.564 val_auc_score 0.799\n",
      "----End of step 0:00:46.299301\n",
      "train_loss 0.543 val_loss 0.564 val_auc_score 0.818\n",
      "----End of step 0:00:47.400474\n",
      "train_loss 0.532 val_loss 0.545 val_auc_score 0.821\n",
      "----End of step 0:00:48.440683\n",
      "train_loss 0.523 val_loss 0.544 val_auc_score 0.815\n",
      "----End of step 0:00:46.688482\n",
      "train_loss 0.515 val_loss 0.540 val_auc_score 0.829\n",
      "----End of step 0:00:45.942522\n",
      "train_loss 0.507 val_loss 0.537 val_auc_score 0.828\n",
      "----End of step 0:00:46.996109\n",
      "train_loss 0.502 val_loss 0.540 val_auc_score 0.828\n",
      "----End of step 0:00:46.075138\n",
      "train_loss 0.501 val_loss 0.531 val_auc_score 0.829\n",
      "----End of step 0:00:46.320168\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 5.000\n",
      "train_loss 0.668 val_loss 0.707 val_auc_score 0.648\n",
      "----End of step 0:00:44.491451\n",
      "train_loss 0.648 val_loss 0.659 val_auc_score 0.690\n",
      "----End of step 0:00:42.264285\n",
      "train_loss 0.625 val_loss 0.697 val_auc_score 0.704\n",
      "----End of step 0:00:43.289183\n",
      "train_loss 0.609 val_loss 0.630 val_auc_score 0.731\n",
      "----End of step 0:00:42.383707\n",
      "train_loss 0.593 val_loss 0.625 val_auc_score 0.718\n",
      "----End of step 0:00:43.773813\n",
      "train_loss 0.582 val_loss 0.595 val_auc_score 0.750\n",
      "----End of step 0:00:43.651808\n",
      "train_loss 0.572 val_loss 0.597 val_auc_score 0.769\n",
      "----End of step 0:00:43.446656\n",
      "train_loss 0.564 val_loss 0.601 val_auc_score 0.781\n",
      "----End of step 0:00:44.503511\n",
      "train_loss 0.554 val_loss 0.562 val_auc_score 0.792\n",
      "----End of step 0:00:43.733996\n",
      "train_loss 0.544 val_loss 0.558 val_auc_score 0.797\n",
      "----End of step 0:00:44.700666\n",
      "train_loss 0.536 val_loss 0.554 val_auc_score 0.806\n",
      "----End of step 0:00:44.611619\n",
      "train_loss 0.531 val_loss 0.553 val_auc_score 0.810\n",
      "----End of step 0:00:44.050454\n",
      "train_loss 0.522 val_loss 0.549 val_auc_score 0.810\n",
      "----End of step 0:00:43.943058\n",
      "train_loss 0.517 val_loss 0.547 val_auc_score 0.814\n",
      "----End of step 0:00:44.229118\n",
      "train_loss 0.515 val_loss 0.541 val_auc_score 0.815\n",
      "----End of step 0:00:44.666182\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 6.000\n",
      "train_loss 0.668 val_loss 0.686 val_auc_score 0.626\n",
      "----End of step 0:00:48.108566\n",
      "train_loss 0.649 val_loss 0.665 val_auc_score 0.698\n",
      "----End of step 0:00:47.215987\n",
      "train_loss 0.630 val_loss 0.638 val_auc_score 0.702\n",
      "----End of step 0:00:48.339806\n",
      "train_loss 0.610 val_loss 0.615 val_auc_score 0.736\n",
      "----End of step 0:00:48.442454\n",
      "train_loss 0.591 val_loss 0.604 val_auc_score 0.765\n",
      "----End of step 0:00:48.068838\n",
      "train_loss 0.576 val_loss 0.617 val_auc_score 0.759\n",
      "----End of step 0:00:46.493464\n",
      "train_loss 0.561 val_loss 0.578 val_auc_score 0.797\n",
      "----End of step 0:00:47.313799\n",
      "train_loss 0.550 val_loss 0.573 val_auc_score 0.806\n",
      "----End of step 0:00:48.232527\n",
      "train_loss 0.541 val_loss 0.560 val_auc_score 0.809\n",
      "----End of step 0:00:48.133811\n",
      "train_loss 0.528 val_loss 0.569 val_auc_score 0.824\n",
      "----End of step 0:00:47.739051\n",
      "train_loss 0.519 val_loss 0.554 val_auc_score 0.826\n",
      "----End of step 0:00:47.547051\n",
      "train_loss 0.510 val_loss 0.549 val_auc_score 0.833\n",
      "----End of step 0:00:47.589521\n",
      "train_loss 0.500 val_loss 0.537 val_auc_score 0.835\n",
      "----End of step 0:00:47.439826\n",
      "train_loss 0.495 val_loss 0.523 val_auc_score 0.837\n",
      "----End of step 0:00:46.913979\n",
      "train_loss 0.493 val_loss 0.531 val_auc_score 0.837\n",
      "----End of step 0:00:47.653359\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 5.000\n",
      "train_loss 0.667 val_loss 0.680 val_auc_score 0.637\n",
      "----End of step 0:00:44.985336\n",
      "train_loss 0.646 val_loss 0.734 val_auc_score 0.637\n",
      "----End of step 0:00:45.127093\n",
      "train_loss 0.624 val_loss 0.703 val_auc_score 0.711\n",
      "----End of step 0:00:44.775642\n",
      "train_loss 0.602 val_loss 0.639 val_auc_score 0.705\n",
      "----End of step 0:00:45.237386\n",
      "train_loss 0.588 val_loss 0.606 val_auc_score 0.745\n",
      "----End of step 0:00:45.808305\n",
      "train_loss 0.579 val_loss 0.639 val_auc_score 0.754\n",
      "----End of step 0:00:46.272278\n",
      "train_loss 0.568 val_loss 0.602 val_auc_score 0.755\n",
      "----End of step 0:00:45.130880\n",
      "train_loss 0.560 val_loss 0.571 val_auc_score 0.788\n",
      "----End of step 0:00:44.752794\n",
      "train_loss 0.550 val_loss 0.591 val_auc_score 0.792\n",
      "----End of step 0:00:43.811025\n",
      "train_loss 0.543 val_loss 0.575 val_auc_score 0.797\n",
      "----End of step 0:00:41.598728\n",
      "train_loss 0.534 val_loss 0.567 val_auc_score 0.804\n",
      "----End of step 0:00:42.204158\n",
      "train_loss 0.526 val_loss 0.562 val_auc_score 0.813\n",
      "----End of step 0:00:41.262214\n",
      "train_loss 0.518 val_loss 0.552 val_auc_score 0.810\n",
      "----End of step 0:00:41.691208\n",
      "train_loss 0.513 val_loss 0.549 val_auc_score 0.815\n",
      "----End of step 0:00:40.765185\n",
      "train_loss 0.511 val_loss 0.543 val_auc_score 0.815\n",
      "----End of step 0:00:41.721670\n",
      "\n",
      "width multiplier - 0.500 depth multiplier - 4.000\n",
      "train_loss 0.667 val_loss 0.704 val_auc_score 0.615\n",
      "----End of step 0:00:38.496671\n",
      "train_loss 0.648 val_loss 0.644 val_auc_score 0.713\n",
      "----End of step 0:00:38.675941\n",
      "train_loss 0.632 val_loss 0.693 val_auc_score 0.700\n",
      "----End of step 0:00:37.923038\n",
      "train_loss 0.613 val_loss 0.691 val_auc_score 0.732\n",
      "----End of step 0:00:38.817318\n",
      "train_loss 0.597 val_loss 0.608 val_auc_score 0.758\n",
      "----End of step 0:00:38.087805\n",
      "train_loss 0.582 val_loss 0.606 val_auc_score 0.774\n",
      "----End of step 0:00:37.367734\n",
      "train_loss 0.573 val_loss 0.620 val_auc_score 0.773\n",
      "----End of step 0:00:38.536210\n",
      "train_loss 0.563 val_loss 0.613 val_auc_score 0.781\n",
      "----End of step 0:00:38.522275\n",
      "train_loss 0.553 val_loss 0.625 val_auc_score 0.800\n",
      "----End of step 0:00:38.977628\n",
      "train_loss 0.542 val_loss 0.570 val_auc_score 0.811\n",
      "----End of step 0:00:38.607534\n",
      "train_loss 0.532 val_loss 0.552 val_auc_score 0.809\n",
      "----End of step 0:00:38.944000\n",
      "train_loss 0.524 val_loss 0.558 val_auc_score 0.812\n",
      "----End of step 0:00:37.822015\n",
      "train_loss 0.515 val_loss 0.542 val_auc_score 0.820\n",
      "----End of step 0:00:38.820140\n",
      "train_loss 0.512 val_loss 0.545 val_auc_score 0.825\n",
      "----End of step 0:00:38.380201\n",
      "train_loss 0.508 val_loss 0.543 val_auc_score 0.824\n",
      "----End of step 0:00:38.999984\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 7.000\n",
      "train_loss 0.669 val_loss 0.678 val_auc_score 0.642\n",
      "----End of step 0:00:47.313375\n",
      "train_loss 0.651 val_loss 0.719 val_auc_score 0.637\n",
      "----End of step 0:00:47.078118\n",
      "train_loss 0.635 val_loss 0.627 val_auc_score 0.716\n",
      "----End of step 0:00:46.635482\n",
      "train_loss 0.612 val_loss 0.684 val_auc_score 0.735\n",
      "----End of step 0:00:46.550282\n",
      "train_loss 0.592 val_loss 0.627 val_auc_score 0.734\n",
      "----End of step 0:00:47.254349\n",
      "train_loss 0.582 val_loss 0.612 val_auc_score 0.763\n",
      "----End of step 0:00:47.559961\n",
      "train_loss 0.570 val_loss 0.600 val_auc_score 0.781\n",
      "----End of step 0:00:46.790405\n",
      "train_loss 0.560 val_loss 0.604 val_auc_score 0.789\n",
      "----End of step 0:00:48.001946\n",
      "train_loss 0.549 val_loss 0.555 val_auc_score 0.796\n",
      "----End of step 0:00:47.442382\n",
      "train_loss 0.539 val_loss 0.613 val_auc_score 0.789\n",
      "----End of step 0:00:47.144121\n",
      "train_loss 0.531 val_loss 0.551 val_auc_score 0.824\n",
      "----End of step 0:00:46.729467\n",
      "train_loss 0.522 val_loss 0.555 val_auc_score 0.821\n",
      "----End of step 0:00:47.015812\n",
      "train_loss 0.517 val_loss 0.551 val_auc_score 0.825\n",
      "----End of step 0:00:46.979603\n",
      "train_loss 0.511 val_loss 0.537 val_auc_score 0.831\n",
      "----End of step 0:00:46.663517\n",
      "train_loss 0.510 val_loss 0.540 val_auc_score 0.832\n",
      "----End of step 0:00:47.475651\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 6.000\n",
      "train_loss 0.672 val_loss 0.736 val_auc_score 0.622\n",
      "----End of step 0:00:44.249823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.655 val_loss 0.680 val_auc_score 0.660\n",
      "----End of step 0:00:44.749791\n",
      "train_loss 0.644 val_loss 0.665 val_auc_score 0.649\n",
      "----End of step 0:00:43.757387\n",
      "train_loss 0.622 val_loss 0.655 val_auc_score 0.702\n",
      "----End of step 0:00:44.311524\n",
      "train_loss 0.605 val_loss 0.616 val_auc_score 0.743\n",
      "----End of step 0:00:43.675701\n",
      "train_loss 0.593 val_loss 0.608 val_auc_score 0.738\n",
      "----End of step 0:00:44.287543\n",
      "train_loss 0.579 val_loss 0.616 val_auc_score 0.762\n",
      "----End of step 0:00:44.271733\n",
      "train_loss 0.569 val_loss 0.629 val_auc_score 0.775\n",
      "----End of step 0:00:44.539705\n",
      "train_loss 0.561 val_loss 0.628 val_auc_score 0.771\n",
      "----End of step 0:00:44.345908\n",
      "train_loss 0.550 val_loss 0.590 val_auc_score 0.785\n",
      "----End of step 0:00:43.656012\n",
      "train_loss 0.541 val_loss 0.571 val_auc_score 0.802\n",
      "----End of step 0:00:44.388544\n",
      "train_loss 0.535 val_loss 0.566 val_auc_score 0.801\n",
      "----End of step 0:00:44.779488\n",
      "train_loss 0.529 val_loss 0.587 val_auc_score 0.807\n",
      "----End of step 0:00:44.854702\n",
      "train_loss 0.526 val_loss 0.568 val_auc_score 0.803\n",
      "----End of step 0:00:43.764066\n",
      "train_loss 0.524 val_loss 0.562 val_auc_score 0.806\n",
      "----End of step 0:00:43.811550\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 5.000\n",
      "train_loss 0.670 val_loss 0.707 val_auc_score 0.567\n",
      "----End of step 0:00:40.494057\n",
      "train_loss 0.651 val_loss 0.658 val_auc_score 0.689\n",
      "----End of step 0:00:40.565024\n",
      "train_loss 0.642 val_loss 0.759 val_auc_score 0.647\n",
      "----End of step 0:00:41.501391\n",
      "train_loss 0.627 val_loss 0.721 val_auc_score 0.691\n",
      "----End of step 0:00:41.903782\n",
      "train_loss 0.605 val_loss 0.667 val_auc_score 0.721\n",
      "----End of step 0:00:40.226617\n",
      "train_loss 0.596 val_loss 0.979 val_auc_score 0.618\n",
      "----End of step 0:00:41.270093\n",
      "train_loss 0.589 val_loss 0.621 val_auc_score 0.758\n",
      "----End of step 0:00:42.097208\n",
      "train_loss 0.580 val_loss 0.600 val_auc_score 0.776\n",
      "----End of step 0:00:41.415804\n",
      "train_loss 0.570 val_loss 0.588 val_auc_score 0.773\n",
      "----End of step 0:00:42.177369\n",
      "train_loss 0.562 val_loss 0.592 val_auc_score 0.777\n",
      "----End of step 0:00:40.598931\n",
      "train_loss 0.556 val_loss 0.596 val_auc_score 0.793\n",
      "----End of step 0:00:41.398631\n",
      "train_loss 0.548 val_loss 0.572 val_auc_score 0.796\n",
      "----End of step 0:00:41.397634\n",
      "train_loss 0.541 val_loss 0.555 val_auc_score 0.803\n",
      "----End of step 0:00:41.271231\n",
      "train_loss 0.539 val_loss 0.564 val_auc_score 0.806\n",
      "----End of step 0:00:42.222924\n",
      "train_loss 0.534 val_loss 0.566 val_auc_score 0.805\n",
      "----End of step 0:00:41.361706\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 6.000\n",
      "train_loss 0.668 val_loss 0.672 val_auc_score 0.641\n",
      "----End of step 0:00:44.229461\n",
      "train_loss 0.649 val_loss 0.687 val_auc_score 0.641\n",
      "----End of step 0:00:44.220743\n",
      "train_loss 0.628 val_loss 0.642 val_auc_score 0.691\n",
      "----End of step 0:00:44.796970\n",
      "train_loss 0.607 val_loss 0.638 val_auc_score 0.728\n",
      "----End of step 0:00:44.095533\n",
      "train_loss 0.596 val_loss 0.624 val_auc_score 0.724\n",
      "----End of step 0:00:44.184050\n",
      "train_loss 0.586 val_loss 0.601 val_auc_score 0.752\n",
      "----End of step 0:00:43.891938\n",
      "train_loss 0.578 val_loss 0.591 val_auc_score 0.761\n",
      "----End of step 0:00:44.637518\n",
      "train_loss 0.566 val_loss 0.588 val_auc_score 0.784\n",
      "----End of step 0:00:44.047342\n",
      "train_loss 0.556 val_loss 0.574 val_auc_score 0.798\n",
      "----End of step 0:00:44.033446\n",
      "train_loss 0.550 val_loss 0.579 val_auc_score 0.790\n",
      "----End of step 0:00:43.962301\n",
      "train_loss 0.543 val_loss 0.566 val_auc_score 0.802\n",
      "----End of step 0:00:44.505774\n",
      "train_loss 0.534 val_loss 0.576 val_auc_score 0.811\n",
      "----End of step 0:00:44.894711\n",
      "train_loss 0.525 val_loss 0.561 val_auc_score 0.815\n",
      "----End of step 0:00:45.692878\n",
      "train_loss 0.519 val_loss 0.543 val_auc_score 0.816\n",
      "----End of step 0:00:45.218972\n",
      "train_loss 0.516 val_loss 0.552 val_auc_score 0.815\n",
      "----End of step 0:00:44.365476\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 5.000\n",
      "train_loss 0.670 val_loss 0.670 val_auc_score 0.661\n",
      "----End of step 0:00:42.587216\n",
      "train_loss 0.654 val_loss 0.675 val_auc_score 0.623\n",
      "----End of step 0:00:42.151979\n",
      "train_loss 0.642 val_loss 0.670 val_auc_score 0.632\n",
      "----End of step 0:00:40.850819\n",
      "train_loss 0.623 val_loss 0.730 val_auc_score 0.691\n",
      "----End of step 0:00:42.386240\n",
      "train_loss 0.605 val_loss 0.628 val_auc_score 0.742\n",
      "----End of step 0:00:41.508690\n",
      "train_loss 0.591 val_loss 0.620 val_auc_score 0.720\n",
      "----End of step 0:00:40.880681\n",
      "train_loss 0.583 val_loss 0.616 val_auc_score 0.734\n",
      "----End of step 0:00:42.338961\n",
      "train_loss 0.570 val_loss 0.592 val_auc_score 0.761\n",
      "----End of step 0:00:41.504280\n",
      "train_loss 0.561 val_loss 0.583 val_auc_score 0.762\n",
      "----End of step 0:00:42.468990\n",
      "train_loss 0.553 val_loss 0.612 val_auc_score 0.774\n",
      "----End of step 0:00:40.957424\n",
      "train_loss 0.546 val_loss 0.557 val_auc_score 0.802\n",
      "----End of step 0:00:41.158550\n",
      "train_loss 0.539 val_loss 0.554 val_auc_score 0.797\n",
      "----End of step 0:00:41.183929\n",
      "train_loss 0.532 val_loss 0.550 val_auc_score 0.801\n",
      "----End of step 0:00:41.332686\n",
      "train_loss 0.528 val_loss 0.553 val_auc_score 0.809\n",
      "----End of step 0:00:42.376513\n",
      "train_loss 0.524 val_loss 0.558 val_auc_score 0.805\n",
      "----End of step 0:00:41.887145\n",
      "\n",
      "width multiplier - 0.250 depth multiplier - 4.000\n",
      "train_loss 0.674 val_loss 0.682 val_auc_score 0.615\n",
      "----End of step 0:00:39.196835\n",
      "train_loss 0.657 val_loss 0.680 val_auc_score 0.643\n",
      "----End of step 0:00:39.001365\n",
      "train_loss 0.651 val_loss 0.660 val_auc_score 0.662\n",
      "----End of step 0:00:38.703252\n",
      "train_loss 0.641 val_loss 0.680 val_auc_score 0.688\n",
      "----End of step 0:00:38.830155\n",
      "train_loss 0.620 val_loss 0.641 val_auc_score 0.699\n",
      "----End of step 0:00:38.776188\n",
      "train_loss 0.607 val_loss 0.640 val_auc_score 0.698\n",
      "----End of step 0:00:38.383871\n",
      "train_loss 0.594 val_loss 0.596 val_auc_score 0.762\n",
      "----End of step 0:00:38.727121\n",
      "train_loss 0.585 val_loss 0.614 val_auc_score 0.763\n",
      "----End of step 0:00:39.815408\n",
      "train_loss 0.577 val_loss 0.598 val_auc_score 0.775\n",
      "----End of step 0:00:40.251464\n",
      "train_loss 0.567 val_loss 0.585 val_auc_score 0.785\n",
      "----End of step 0:00:40.184688\n",
      "train_loss 0.558 val_loss 0.576 val_auc_score 0.784\n",
      "----End of step 0:00:40.442919\n",
      "train_loss 0.550 val_loss 0.574 val_auc_score 0.792\n",
      "----End of step 0:00:40.070055\n",
      "train_loss 0.544 val_loss 0.567 val_auc_score 0.798\n",
      "----End of step 0:00:40.359270\n",
      "train_loss 0.539 val_loss 0.569 val_auc_score 0.804\n",
      "----End of step 0:00:40.464652\n",
      "train_loss 0.537 val_loss 0.568 val_auc_score 0.808\n",
      "----End of step 0:00:39.105204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for w in widths:\n",
    "    for d in depths:\n",
    "        d_s = sum(j[1] for i in d for j in i)\n",
    "        print('width multiplier - %.3f depth multiplier - %.3f' % (w, d_s))\n",
    "        model = resnet18(block=depthwise_block, width_mult=w, \n",
    "                         inverted_residual_setting1=d[0], \n",
    "                         inverted_residual_setting2=d[1]).cuda()\n",
    "        \n",
    "        p = sum(p.numel() for p in model.parameters())\n",
    "        optimizer = create_optimizer(model, 0.02)\n",
    "        score, t = train_triangular_policy(model, optimizer, train_loader, valid_loader, valid_dataset,\n",
    "                                           loss_fn=F.binary_cross_entropy_with_logits, \n",
    "                                           dataset='mura', binary=True, max_lr=0.02, epochs=15)\n",
    "        data.append([w, d_s, score, p, t])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['width_x', 'depth_x', 'val_score', 'params', 'time_per_epoch']\n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mura_resnet_depthwise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_re = pd.read_csv('mura_resnet_depthwise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>width_x</th>\n",
       "      <th>depth_x</th>\n",
       "      <th>val_score</th>\n",
       "      <th>params</th>\n",
       "      <th>time_per_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.842868</td>\n",
       "      <td>728193</td>\n",
       "      <td>0 days 00:00:51.636009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.847418</td>\n",
       "      <td>659329</td>\n",
       "      <td>0 days 00:00:45.084963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.837213</td>\n",
       "      <td>641281</td>\n",
       "      <td>0 days 00:00:41.040878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.840171</td>\n",
       "      <td>710145</td>\n",
       "      <td>0 days 00:00:45.032497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.831386</td>\n",
       "      <td>705217</td>\n",
       "      <td>0 days 00:00:42.495181000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   width_x  depth_x  val_score  params             time_per_epoch\n",
       "0      1.0        7   0.842868  728193  0 days 00:00:51.636009000\n",
       "1      1.0        6   0.847418  659329  0 days 00:00:45.084963000\n",
       "2      1.0        5   0.837213  641281  0 days 00:00:41.040878000\n",
       "3      1.0        6   0.840171  710145  0 days 00:00:45.032497000\n",
       "4      1.0        5   0.831386  705217  0 days 00:00:42.495181000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_re.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
